{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8921598,"sourceType":"datasetVersion","datasetId":5366031},{"sourceId":8936349,"sourceType":"datasetVersion","datasetId":5376452}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[],"collapsed_sections":["tpYel0PeGb7O","iNELPnjTGb7Q"]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","project_dir = '/content/drive/My Drive/Colab Notebooks/event_camera_repo'\n","%cd {project_dir}"],"metadata":{"id":"QTx90vTeGl3_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721098852670,"user_tz":-540,"elapsed":16976,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}},"outputId":"d7bcf5a7-1781-4855-e7ba-200ec397c582"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","[Errno 2] No such file or directory: '/content/drive/My Drive/Colab Notebooks/event_camera_repo'\n","/content\n"]}]},{"cell_type":"code","source":["!pip install hydra-core omegaconf hdf5plugin h5py numba imageio imageio-ffmpeg tqdm torchvision --quiet"],"metadata":{"trusted":true,"id":"fdfquW_iGb7K","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7f0a1fa5-1f9b-440f-f1ae-5ab153eb3ec2","executionInfo":{"status":"ok","timestamp":1721123868921,"user_tz":-540,"elapsed":89367,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["import torch\n","import hydra\n","from omegaconf import DictConfig\n","from torch.utils.data import DataLoader\n","import random\n","import numpy as np\n","from enum import Enum, auto\n","from tqdm import tqdm\n","from pathlib import Path\n","from typing import Dict, Any\n","import os\n","import time\n","\n","import math\n","from pathlib import PurePath\n","from typing import Tuple\n","import cv2\n","import hdf5plugin\n","import h5py\n","from numba import jit\n","import imageio\n","imageio.plugins.freeimage.download()\n","import imageio.v3 as iio\n","from torchvision.transforms import RandomCrop\n","from torchvision import transforms as tf\n","from torch.utils.checkpoint import checkpoint\n","from torch.utils.data import Dataset\n","import torchvision.transforms.functional as F\n","\n","from torch import nn"],"metadata":{"trusted":true,"id":"tAW9ff9RGb7M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721123877558,"user_tz":-540,"elapsed":8643,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}},"outputId":"2d2cfe52-a620-426d-c01d-f764c296efa4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Imageio: 'libfreeimage-3.16.0-linux64.so' was not found on your computer; downloading it now.\n","Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/freeimage/libfreeimage-3.16.0-linux64.so (4.6 MB)\n","Downloading: 8192/4830080 bytes (0.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b90112/4830080 bytes (1.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b647168/4830080 bytes (13.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b4603904/4830080 bytes (95.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b4830080/4830080 bytes (100.0%)\n","  Done\n","File saved as /root/.imageio/freeimage/libfreeimage-3.16.0-linux64.so.\n"]}]},{"cell_type":"code","source":["!pip install einops\n","from einops.layers.torch import Rearrange\n","from einops import rearrange"],"metadata":{"trusted":true,"id":"EjOf_ZjXGb7N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from omegaconf import OmegaConf\n","from hydra import compose, initialize\n","\n","cfg_string = '''\n","dataset_path: data\n","seed: 42\n","num_epoch: 100\n","data_loader:\n","    common:\n","        num_voxel_bins: 15\n","    train:\n","        batch_size: 16\n","        shuffle: false\n","    test:\n","        batch_size: 1\n","        shuffle: false\n","train:\n","    no_batch_norm: false\n","    initial_learning_rate: 0.01\n","    learning_rate_decay: 0.9\n","    weight_decay: 0.0001\n","    epochs: 10\n","'''\n","cfg = OmegaConf.create(cfg_string)"],"metadata":{"trusted":true,"id":"yfNzEvaIGb7O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## `utils.py`"],"metadata":{"id":"tpYel0PeGb7O"}},{"cell_type":"code","source":["def set_seed(seed: int = 0) -> None:\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","class RepresentationType(Enum):\n","    VOXEL = auto()\n","    STEPAN = auto()\n","\n","\n","class EventRepresentation:\n","    def __init__(self):\n","        pass\n","\n","    def convert(self, events):\n","        raise NotImplementedError\n","\n","\n","class VoxelGrid(EventRepresentation):\n","    def __init__(self, input_size: tuple, normalize: bool):\n","        assert len(input_size) == 3\n","        self.voxel_grid = torch.zeros(\n","            (input_size), dtype=torch.float, requires_grad=False)\n","        self.nb_channels = input_size[0]\n","        self.normalize = normalize\n","\n","    def convert(self, events):\n","        C, H, W = self.voxel_grid.shape\n","        with torch.no_grad():\n","            self.voxel_grid = self.voxel_grid.to(events['p'].device)\n","            voxel_grid = self.voxel_grid.clone()\n","\n","            t_norm = events['t']\n","            t_norm = (C - 1) * (t_norm-t_norm[0]) / (t_norm[-1]-t_norm[0])\n","\n","            x0 = events['x'].int()\n","            y0 = events['y'].int()\n","            t0 = t_norm.int()\n","\n","            value = 2*events['p']-1\n","            #start_t = time()\n","            for xlim in [x0, x0+1]:\n","                for ylim in [y0, y0+1]:\n","                    for tlim in [t0, t0+1]:\n","\n","                        mask = (xlim < W) & (xlim >= 0) & (ylim < H) & (\n","                            ylim >= 0) & (tlim >= 0) & (tlim < self.nb_channels)\n","                        interp_weights = value * (1 - (xlim-events['x']).abs()) * (\n","                            1 - (ylim-events['y']).abs()) * (1 - (tlim - t_norm).abs())\n","                        index = H * W * tlim.long() + \\\n","                            W * ylim.long() + \\\n","                            xlim.long()\n","\n","                        voxel_grid.put_(\n","                            index[mask], interp_weights[mask], accumulate=True)\n","\n","            if self.normalize:\n","                mask = torch.nonzero(voxel_grid, as_tuple=True)\n","                if mask[0].size()[0] > 0:\n","                    mean = voxel_grid[mask].mean()\n","                    std = voxel_grid[mask].std()\n","                    if std > 0:\n","                        voxel_grid[mask] = (voxel_grid[mask] - mean) / std\n","                    else:\n","                        voxel_grid[mask] = voxel_grid[mask] - mean\n","\n","        return voxel_grid\n","\n","\n","class PolarityCount(EventRepresentation):\n","    def __init__(self, input_size: tuple):\n","        assert len(input_size) == 3\n","        self.voxel_grid = torch.zeros(\n","            (input_size), dtype=torch.float, requires_grad=False)\n","        self.nb_channels = input_size[0]\n","\n","    def convert(self, events):\n","        C, H, W = self.voxel_grid.shape\n","        with torch.no_grad():\n","            self.voxel_grid = self.voxel_grid.to(events['p'].device)\n","            voxel_grid = self.voxel_grid.clone()\n","\n","            x0 = events['x'].int()\n","            y0 = events['y'].int()\n","\n","            #start_t = time()\n","            for xlim in [x0, x0+1]:\n","                for ylim in [y0, y0+1]:\n","                    mask = (xlim < W) & (xlim >= 0) & (ylim < H) & (\n","                        ylim >= 0)\n","                    interp_weights = (1 - (xlim-events['x']).abs()) * (\n","                        1 - (ylim-events['y']).abs())\n","                    index = H * W * events['p'].long() + \\\n","                        W * ylim.long() + \\\n","                        xlim.long()\n","\n","                    voxel_grid.put_(\n","                        index[mask], interp_weights[mask], accumulate=True)\n","\n","        return voxel_grid\n","\n","\n","def flow_16bit_to_float(flow_16bit: np.ndarray):\n","    assert flow_16bit.dtype == np.uint16\n","    assert flow_16bit.ndim == 3\n","    h, w, c = flow_16bit.shape\n","    assert c == 3\n","\n","    valid2D = flow_16bit[..., 2] == 1\n","    assert valid2D.shape == (h, w)\n","    assert np.all(flow_16bit[~valid2D, -1] == 0)\n","    valid_map = np.where(valid2D)\n","\n","    # to actually compute something useful:\n","    flow_16bit = flow_16bit.astype('float')\n","\n","    flow_map = np.zeros((h, w, 2))\n","    flow_map[valid_map[0], valid_map[1], 0] = (\n","        flow_16bit[valid_map[0], valid_map[1], 0] - 2 ** 15) / 128\n","    flow_map[valid_map[0], valid_map[1], 1] = (\n","        flow_16bit[valid_map[0], valid_map[1], 1] - 2 ** 15) / 128\n","    return flow_map, valid2D"],"metadata":{"trusted":true,"id":"8z3ZgnkfGb7P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def warp_images_with_flow(images, flow):\n","    dim3 = 0\n","    if images.dim() == 3:\n","        dim3 = 1\n","        images = images.unsqueeze(0)\n","        flow = flow.unsqueeze(0)\n","    height = images.shape[2]\n","    width = images.shape[3]\n","    flow_x,flow_y = flow[:,0,...],flow[:,1,...]\n","    coord_x, coord_y = torch.meshgrid(torch.arange(height), torch.arange(width))\n","\n","    if torch.cuda.is_available():\n","        pos_x = coord_x.reshape(height,width).type(torch.float32).cuda() + flow_x\n","        pos_y = coord_y.reshape(height,width).type(torch.float32).cuda() + flow_y\n","    else: # Troubleshoot without cuda\n","        pos_x = coord_x.reshape(height,width).type(torch.float32) + flow_x\n","        pos_y = coord_y.reshape(height,width).type(torch.float32) + flow_y\n","    pos_x = (pos_x-(height-1)/2)/((height-1)/2)\n","    pos_y = (pos_y-(width-1)/2)/((width-1)/2)\n","\n","    pos = torch.stack((pos_y,pos_x),3).type(torch.float32)\n","    result = torch.nn.functional.grid_sample(images, pos, mode='bilinear', padding_mode='zeros')\n","    if dim3 == 1:\n","        result = result.squeeze()\n","\n","    return result\n","\n","def charbonnier_loss(delta, alpha=0.45, epsilon=1e-3):\n","        loss = torch.mean(torch.pow((delta ** 2 + epsilon ** 2), alpha))\n","        return loss\n","\n","def compute_smoothness_loss(flow):\n","\n","    flow_ucrop = flow[..., 1:]\n","    flow_dcrop = flow[..., :-1]\n","    flow_lcrop = flow[..., 1:, :]\n","    flow_rcrop = flow[..., :-1, :]\n","\n","    flow_ulcrop = flow[..., 1:, 1:]\n","    flow_drcrop = flow[..., :-1, :-1]\n","    flow_dlcrop = flow[..., :-1, 1:]\n","    flow_urcrop = flow[..., 1:, :-1]\n","\n","    smoothness_loss = charbonnier_loss(flow_lcrop - flow_rcrop) +\\\n","                      charbonnier_loss(flow_ucrop - flow_dcrop) +\\\n","                      charbonnier_loss(flow_ulcrop - flow_drcrop) +\\\n","                      charbonnier_loss(flow_dlcrop - flow_urcrop)\n","    smoothness_loss /= 4.\n","\n","    return smoothness_loss\n","\n","def compute_photometric_loss(prev_images, next_images, flow_dict):\n","    total_photometric_loss = 0.\n","    loss_weight_sum = 0.\n","    for i in range(len(flow_dict)):\n","        for image_num in range(prev_images.shape[0]):\n","            flow = flow_dict[\"flow{}\".format(i)][image_num]\n","            height = flow.shape[1]\n","            width = flow.shape[2]\n","\n","            prev_images_resize = F.to_tensor(F.resize(F.to_pil_image(prev_images[image_num].cpu()),\n","                                                    [height, width]))\n","            next_images_resize = F.to_tensor(F.resize(F.to_pil_image(next_images[image_num].cpu()),\n","                                                    [height, width]))\n","\n","            if torch.cuda.is_available():\n","                prev_images_resize = prev_images_resize.cuda()\n","                next_images_resize = next_images_resize.cuda()\n","\n","            next_images_warped = warp_images_with_flow(next_images_resize, flow)\n","\n","            distance = next_images_warped - prev_images_resize\n","            photometric_loss = charbonnier_loss(distance)\n","            total_photometric_loss += photometric_loss\n","        loss_weight_sum += 1.\n","    total_photometric_loss /= loss_weight_sum\n","\n","    return total_photometric_loss\n","\n","\n","class TotalLoss(torch.nn.Module):\n","    def __init__(self, smoothness_weight, weight_decay_weight=1e-4):\n","        super(TotalLoss, self).__init__()\n","        self._smoothness_weight = smoothness_weight\n","        self._weight_decay_weight = weight_decay_weight\n","\n","    def forward(self, flow_dict, prev_image, next_image, EVFlowNet_model):\n","        # weight decay loss\n","        weight_decay_loss = 0\n","        for i in EVFlowNet_model.parameters():\n","            weight_decay_loss += torch.sum(i ** 2) / 2 * self._weight_decay_weight\n","\n","        # smoothness loss\n","        smoothness_loss = 0\n","        for i in range(len(flow_dict)):\n","            smoothness_loss += compute_smoothness_loss(flow_dict[\"flow{}\".format(i)])\n","        smoothness_loss *= self._smoothness_weight / 4.\n","\n","        # Photometric loss.\n","        photometric_loss = compute_photometric_loss(prev_image,\n","                                                    next_image,\n","                                                    flow_dict)\n","\n","        # Warped next image for debugging.\n","        #next_image_warped = warp_images_with_flow(next_image,\n","        #                                          flow_dict['flow3'])\n","\n","        loss = weight_decay_loss + photometric_loss + smoothness_loss\n","\n","        return loss"],"metadata":{"trusted":true,"id":"72oMFIZCGb7P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## `datasets.py`"],"metadata":{"id":"iNELPnjTGb7Q"}},{"cell_type":"code","source":["VISU_INDEX = 1\n","\n","\n","class EventSlicer:\n","    def __init__(self, h5f: h5py.File):\n","        self.h5f = h5f\n","\n","        self.events = dict()\n","        for dset_str in ['p', 'x', 'y', 't']:\n","            self.events[dset_str] = self.h5f['events/{}'.format(dset_str)]\n","\n","        # This is the mapping from milliseconds to event index:\n","        # It is defined such that\n","        # (1) t[ms_to_idx[ms]] >= ms*1000\n","        # (2) t[ms_to_idx[ms] - 1] < ms*1000\n","        # ,where 'ms' is the time in milliseconds and 't' the event timestamps in microseconds.\n","        #\n","        # As an example, given 't' and 'ms':\n","        # t:    0     500    2100    5000    5000    7100    7200    7200    8100    9000\n","        # ms:   0       1       2       3       4       5       6       7       8       9\n","        #\n","        # we get\n","        #\n","        # ms_to_idx:\n","        #       0       2       2       3       3       3       5       5       8       9\n","        self.ms_to_idx = np.asarray(self.h5f['ms_to_idx'], dtype='int64')\n","\n","        self.t_offset = int(h5f['t_offset'][()])\n","        self.t_final = int(self.events['t'][-1]) + self.t_offset\n","\n","    def get_final_time_us(self):\n","        return self.t_final\n","\n","    def get_events(self, t_start_us: int, t_end_us: int) -> Dict[str, np.ndarray]:\n","        \"\"\"Get events (p, x, y, t) within the specified time window\n","        Parameters\n","        ----------\n","        t_start_us: start time in microseconds\n","        t_end_us: end time in microseconds\n","        Returns\n","        -------\n","        events: dictionary of (p, x, y, t) or None if the time window cannot be retrieved\n","        \"\"\"\n","        assert t_start_us < t_end_us\n","\n","        # We assume that the times are top-off-day, hence subtract offset:\n","        t_start_us -= self.t_offset\n","        t_end_us -= self.t_offset\n","\n","        t_start_ms, t_end_ms = self.get_conservative_window_ms(\n","            t_start_us, t_end_us)\n","        t_start_ms_idx = self.ms2idx(t_start_ms)\n","        t_end_ms_idx = self.ms2idx(t_end_ms)\n","        if t_start_ms_idx is None or t_end_ms_idx is None:\n","            print('Error', 'start', t_start_us, 'end', t_end_us)\n","            # Cannot guarantee window size anymore\n","            return None\n","\n","        events = dict()\n","        time_array_conservative = np.asarray(\n","            self.events['t'][t_start_ms_idx:t_end_ms_idx])\n","        idx_start_offset, idx_end_offset = self.get_time_indices_offsets(\n","            time_array_conservative, t_start_us, t_end_us)\n","        t_start_us_idx = t_start_ms_idx + idx_start_offset\n","        t_end_us_idx = t_start_ms_idx + idx_end_offset\n","        # Again add t_offset to get gps time\n","        events['t'] = time_array_conservative[idx_start_offset:idx_end_offset] + self.t_offset\n","        for dset_str in ['p', 'x', 'y']:\n","            events[dset_str] = np.asarray(\n","                self.events[dset_str][t_start_us_idx:t_end_us_idx])\n","            assert events[dset_str].size == events['t'].size\n","        return events\n","\n","    @staticmethod\n","    def get_conservative_window_ms(ts_start_us: int, ts_end_us) -> Tuple[int, int]:\n","        \"\"\"Compute a conservative time window of time with millisecond resolution.\n","        We have a time to index mapping for each millisecond. Hence, we need\n","        to compute the lower and upper millisecond to retrieve events.\n","        Parameters\n","        ----------\n","        ts_start_us:    start time in microseconds\n","        ts_end_us:      end time in microseconds\n","        Returns\n","        -------\n","        window_start_ms:    conservative start time in milliseconds\n","        window_end_ms:      conservative end time in milliseconds\n","        \"\"\"\n","        assert ts_end_us > ts_start_us\n","        window_start_ms = math.floor(ts_start_us/1000)\n","        window_end_ms = math.ceil(ts_end_us/1000)\n","        return window_start_ms, window_end_ms\n","\n","    @staticmethod\n","    @jit(nopython=True)\n","    def get_time_indices_offsets(\n","            time_array: np.ndarray,\n","            time_start_us: int,\n","            time_end_us: int) -> Tuple[int, int]:\n","        \"\"\"Compute index offset of start and end timestamps in microseconds\n","        Parameters\n","        ----------\n","        time_array:     timestamps (in us) of the events\n","        time_start_us:  start timestamp (in us)\n","        time_end_us:    end timestamp (in us)\n","        Returns\n","        -------\n","        idx_start:  Index within this array corresponding to time_start_us\n","        idx_end:    Index within this array corresponding to time_end_us\n","        such that (in non-edge cases)\n","        time_array[idx_start] >= time_start_us\n","        time_array[idx_end] >= time_end_us\n","        time_array[idx_start - 1] < time_start_us\n","        time_array[idx_end - 1] < time_end_us\n","        this means that\n","        time_start_us <= time_array[idx_start:idx_end] < time_end_us\n","        \"\"\"\n","\n","        assert time_array.ndim == 1\n","\n","        idx_start = -1\n","        if time_array[-1] < time_start_us:\n","\n","            # Return same index twice: array[x:x] is empty.\n","            return time_array.size, time_array.size\n","        else:\n","            for idx_from_start in range(0, time_array.size, 1):\n","                if time_array[idx_from_start] >= time_start_us:\n","                    idx_start = idx_from_start\n","                    break\n","        assert idx_start >= 0\n","\n","        idx_end = time_array.size\n","        for idx_from_end in range(time_array.size - 1, -1, -1):\n","            if time_array[idx_from_end] >= time_end_us:\n","                idx_end = idx_from_end\n","            else:\n","                break\n","\n","        assert time_array[idx_start] >= time_start_us\n","        if idx_end < time_array.size:\n","            assert time_array[idx_end] >= time_end_us\n","        if idx_start > 0:\n","            assert time_array[idx_start - 1] < time_start_us\n","        if idx_end > 0:\n","            assert time_array[idx_end - 1] < time_end_us\n","        return idx_start, idx_end\n","\n","    def ms2idx(self, time_ms: int) -> int:\n","        assert time_ms >= 0\n","        if time_ms >= self.ms_to_idx.size:\n","            return None\n","        return self.ms_to_idx[time_ms]\n","\n","\n","class Sequence(Dataset):\n","    def __init__(self, seq_path: Path, representation_type: RepresentationType, mode: str = 'test', delta_t_ms: int = 100,\n","                 num_bins: int = 4, transforms=[], name_idx=0, visualize=False, load_gt=False):\n","        assert num_bins >= 1\n","        assert delta_t_ms == 100\n","        assert seq_path.is_dir(), seq_path\n","        assert mode in {'train', 'test'}\n","        assert representation_type is not None\n","        '''\n","        ディレクトリ構造:\n","\n","        data\n","        ├─test\n","        |  ├─seq_1\n","        |  |    ├─events_left\n","        |  |    |   ├─events.h5\n","        |  |    |   └─rectify_map.h5\n","        |  |    └─forward_timestamps.txt\n","        └─train\n","            ├─seq_1\n","            |    ├─events_left\n","            |    |       ├─ events.h5\n","            |    |       └─ rectify_map.h5\n","            |    ├─flow_forward\n","            |    |       ├─ 000134.png\n","            |    |       |.....\n","            |    └─forward_timestamps.txt\n","            ├─seq_2\n","            └─seq_3\n","        '''\n","        self.seq_name = PurePath(seq_path).name\n","        self.mode = mode\n","        self.name_idx = name_idx\n","        self.visualize_samples = visualize\n","        self.load_gt = load_gt\n","        self.transforms = transforms\n","        if self.mode == \"test\":\n","            assert load_gt == False\n","            # Get Test Timestamp File\n","            ev_dir_location = seq_path / 'events_left'\n","            timestamp_file = seq_path / 'forward_timestamps.txt'\n","            flow_path = seq_path / 'flow_forward'\n","            timestamps_flow = np.loadtxt(\n","                seq_path / 'forward_timestamps.txt', delimiter=',', dtype='int64')\n","            self.indices = np.arange(len(timestamps_flow))\n","            self.timestamps_flow = timestamps_flow[:, 0]\n","\n","        elif self.mode == \"train\":\n","            ev_dir_location = seq_path / 'events_left'\n","            flow_path = seq_path / 'flow_forward'\n","            timestamp_file = seq_path / 'forward_timestamps.txt'\n","            self.flow_png = [Path(os.path.join(flow_path, img)) for img in sorted(\n","                os.listdir(flow_path))]\n","            timestamps_flow = np.loadtxt(\n","                seq_path / 'forward_timestamps.txt', delimiter=',', dtype='int64')\n","            self.indices = np.arange(len(timestamps_flow))\n","            self.timestamps_flow = timestamps_flow[:, 0]\n","        else:\n","            pass\n","        assert timestamp_file.is_file()\n","\n","        file = np.genfromtxt(\n","            timestamp_file,\n","            delimiter=','\n","        )\n","\n","        self.idx_to_visualize = file[:, 2] if file.shape[1] == 3 else []\n","\n","        # Save output dimensions\n","        self.height = 480\n","        self.width = 640\n","        self.num_bins = num_bins\n","\n","\n","        # Set event representation\n","        self.voxel_grid = VoxelGrid(\n","                (self.num_bins, self.height, self.width), normalize=True)\n","        self.delta_t_us = delta_t_ms * 1000\n","\n","        # Left events only\n","        ev_data_file = ev_dir_location / 'events.h5'\n","        ev_rect_file = ev_dir_location / 'rectify_map.h5'\n","\n","        h5f_location = h5py.File(str(ev_data_file), 'r')\n","        self.h5f = h5f_location\n","        self.event_slicer = EventSlicer(h5f_location)\n","\n","        self.h5rect = h5py.File(str(ev_rect_file), 'r')\n","        self.rectify_ev_map = self.h5rect['rectify_map'][()]\n","\n","\n","    def events_to_voxel_grid(self, p, t, x, y, device: str = 'cpu'):\n","        t = (t - t[0]).astype('float32')\n","        t = (t/t[-1])\n","        x = x.astype('float32')\n","        y = y.astype('float32')\n","        pol = p.astype('float32')\n","        event_data_torch = {\n","            'p': torch.from_numpy(pol),\n","            't': torch.from_numpy(t),\n","            'x': torch.from_numpy(x),\n","            'y': torch.from_numpy(y),\n","        }\n","        return self.voxel_grid.convert(event_data_torch)\n","\n","    def getHeightAndWidth(self):\n","        return self.height, self.width\n","\n","    @staticmethod\n","    def get_disparity_map(filepath: Path):\n","        assert filepath.is_file()\n","        disp_16bit = cv2.imread(str(filepath), cv2.IMREAD_ANYDEPTH)\n","        return disp_16bit.astype('float32')/256\n","\n","    @staticmethod\n","    def load_flow(flowfile: Path):\n","        assert flowfile.exists()\n","        assert flowfile.suffix == '.png'\n","        flow_16bit = iio.imread(str(flowfile), plugin='PNG-FI')\n","        flow, valid2D = flow_16bit_to_float(flow_16bit)\n","        return flow, valid2D\n","\n","    @staticmethod\n","    def close_callback(h5f):\n","        h5f.close()\n","\n","    def get_image_width_height(self):\n","        return self.height, self.width\n","\n","    def __len__(self):\n","        # Ignore the first and last images as their own\n","        return len(self.timestamps_flow) # - 2\n","\n","    def rectify_events(self, x: np.ndarray, y: np.ndarray):\n","        # assert location in self.locations\n","        # From distorted to undistorted\n","        rectify_map = self.rectify_ev_map\n","        assert rectify_map.shape == (\n","            self.height, self.width, 2), rectify_map.shape\n","        assert x.max() < self.width\n","        assert y.max() < self.height\n","        return rectify_map[y, x]\n","\n","    def get_data(self, index) -> Dict[str, any]:\n","        # Adjust index to skip the first element\n","#         index += 1\n","\n","        ts_start: int = self.timestamps_flow[index] - self.delta_t_us\n","        ts_end: int = self.timestamps_flow[index]\n","\n","        file_index = self.indices[index]\n","\n","        output = {\n","            'file_index': file_index,\n","            'timestamp': self.timestamps_flow[index],\n","            'seq_name': self.seq_name\n","        }\n","        # Save sample for benchmark submission\n","        output['save_submission'] = file_index in self.idx_to_visualize\n","        output['visualize'] = self.visualize_samples\n","        event_data = self.event_slicer.get_events(\n","            ts_start, ts_end)\n","        p = event_data['p']\n","        t = event_data['t']\n","        x = event_data['x']\n","        y = event_data['y']\n","\n","        xy_rect = self.rectify_events(x, y)\n","        x_rect = xy_rect[:, 0]\n","        y_rect = xy_rect[:, 1]\n","\n","        if self.voxel_grid is None:\n","            raise NotImplementedError\n","        else:\n","            event_representation = self.events_to_voxel_grid(\n","                p, t, x_rect, y_rect)\n","            output['event_volume'] = event_representation\n","        output['name_map'] = self.name_idx\n","\n","        if self.load_gt:\n","            output['flow_gt'] = [torch.tensor(x) for x in self.load_flow(self.flow_png[index])]\n","            output['flow_gt'][0] = torch.moveaxis(output['flow_gt'][0], -1, 0)\n","            output['flow_gt'][1] = torch.unsqueeze(output['flow_gt'][1], 0)\n","\n","            flow_gt_shape = [tensor.shape for tensor in output['flow_gt']]\n","            zero_flow_gt = [torch.zeros_like(tensor) for tensor in output['flow_gt']]\n","\n","            # Load previous image\n","            if index > 0:\n","                output['prev_flow_gt'] = [torch.tensor(x) for x in self.load_flow(self.flow_png[index - 1])]\n","                output['prev_flow_gt'][0] = torch.moveaxis(output['prev_flow_gt'][0], -1, 0)\n","                output['prev_flow_gt'][1] = torch.unsqueeze(output['prev_flow_gt'][1], 0)\n","            else:\n","                output['prev_flow_gt'] = zero_flow_gt\n","\n","            # Load next image\n","            if index < len(self.timestamps_flow) - 1:\n","                output['next_flow_gt'] = [torch.tensor(x) for x in self.load_flow(self.flow_png[index + 1])]\n","                output['next_flow_gt'][0] = torch.moveaxis(output['next_flow_gt'][0], -1, 0)\n","                output['next_flow_gt'][1] = torch.unsqueeze(output['next_flow_gt'][1], 0)\n","            else:\n","                output['next_flow_gt'] = zero_flow_gt\n","\n","        return output\n","\n","    def __getitem__(self, idx):\n","        # Adjust index to skip the first element\n","        sample = self.get_data(idx) # idx + 1\n","\n","        if self.transforms:\n","            sample = self.transforms(sample)\n","\n","        return sample\n","\n","    def get_voxel_grid(self, idx):\n","\n","        if idx == 0:\n","            event_data = self.event_slicer.get_events(\n","                self.timestamps_flow[0] - self.delta_t_us, self.timestamps_flow[0])\n","        elif idx > 0 and idx <= self.__len__():\n","            event_data = self.event_slicer.get_events(\n","                self.timestamps_flow[idx-1], self.timestamps_flow[idx-1] + self.delta_t_us)\n","        else:\n","            raise IndexError\n","\n","        p = event_data['p']\n","        t = event_data['t']\n","        x = event_data['x']\n","        y = event_data['y']\n","\n","        xy_rect = self.rectify_events(x, y)\n","        x_rect = xy_rect[:, 0]\n","        y_rect = xy_rect[:, 1]\n","        return self.events_to_voxel_grid(p, t, x_rect, y_rect)\n","\n","    def get_event_count_image(self, ts_start, ts_end, num_bins, normalize=True):\n","        assert ts_end > ts_start\n","        delta_t_bin = (ts_end - ts_start) / num_bins\n","        ts_start_bin = np.linspace(\n","            ts_start, ts_end, num=num_bins, endpoint=False)\n","        ts_end_bin = ts_start_bin + delta_t_bin\n","        assert abs(ts_end_bin[-1] - ts_end) < 10.\n","        ts_end_bin[-1] = ts_end\n","\n","        event_count = torch.zeros(\n","            (num_bins, self.height, self.width), dtype=torch.float, requires_grad=False)\n","\n","        for i in range(num_bins):\n","            event_data = self.event_slicer.get_events(\n","                ts_start_bin[i], ts_end_bin[i])\n","            p = event_data['p']\n","            t = event_data['t']\n","            x = event_data['x']\n","            y = event_data['y']\n","\n","            t = (t - t[0]).astype('float32')\n","            t = (t/t[-1])\n","            x = x.astype('float32')\n","            y = y.astype('float32')\n","            pol = p.astype('float32')\n","            event_data_torch = {\n","                'p': torch.from_numpy(pol),\n","                't': torch.from_numpy(t),\n","                'x': torch.from_numpy(x),\n","                'y': torch.from_numpy(y),\n","            }\n","            x = event_data_torch['x']\n","            y = event_data_torch['y']\n","            xy_rect = self.rectify_events(x.int(), y.int())\n","            x_rect = torch.from_numpy(xy_rect[:, 0]).long()\n","            y_rect = torch.from_numpy(xy_rect[:, 1]).long()\n","            value = 2*event_data_torch['p']-1\n","            index = self.width*y_rect + x_rect\n","            mask = (x_rect < self.width) & (y_rect < self.height)\n","            event_count[i].put_(index[mask], value[mask], accumulate=True)\n","\n","        return event_count\n","\n","    @staticmethod\n","    def normalize_tensor(event_count):\n","        mask = torch.nonzero(event_count, as_tuple=True)\n","        if mask[0].size()[0] > 0:\n","            mean = event_count[mask].mean()\n","            std = event_count[mask].std()\n","            if std > 0:\n","                event_count[mask] = (event_count[mask] - mean) / std\n","            else:\n","                event_count[mask] = event_count[mask] - mean\n","        return event_count\n","\n","\n","class SequenceRecurrent(Sequence):\n","    def __init__(self, seq_path: Path, representation_type: RepresentationType, mode: str = 'test', delta_t_ms: int = 100,\n","                 num_bins: int = 15, transforms=None, sequence_length=1, name_idx=0, visualize=False, load_gt=False):\n","        super(SequenceRecurrent, self).__init__(seq_path, representation_type, mode, delta_t_ms, transforms=transforms,\n","                                                name_idx=name_idx, visualize=visualize, load_gt=load_gt)\n","        self.crop_size = self.transforms['randomcrop'] if 'randomcrop' in self.transforms else None\n","        self.sequence_length = sequence_length\n","        self.valid_indices = self.get_continuous_sequences()\n","\n","    def get_continuous_sequences(self):\n","        continuous_seq_idcs = []\n","        if self.sequence_length > 1:\n","            for i in range(len(self.timestamps_flow)-self.sequence_length+1):\n","                diff = self.timestamps_flow[i + self.sequence_length-1] - self.timestamps_flow[i]\n","                if diff < np.max([100000 * (self.sequence_length-1) + 1000, 101000]):\n","                    continuous_seq_idcs.append(i)\n","        else:\n","            for i in range(len(self.timestamps_flow)-1):\n","                diff = self.timestamps_flow[i+1] - self.timestamps_flow[i]\n","                if diff < np.max([100000 * (self.sequence_length-1) + 1000, 101000]):\n","                    continuous_seq_idcs.append(i)\n","        return continuous_seq_idcs\n","\n","    def __len__(self):\n","        return len(self.valid_indices)\n","\n","    def __getitem__(self, idx):\n","        assert idx >= 0\n","        assert idx < len(self)\n","\n","        # Valid index is the actual index we want to load, which guarantees a continuous sequence length\n","        valid_idx = self.valid_indices[idx]\n","\n","        sequence = []\n","        j = valid_idx\n","\n","        ts_cur = self.timestamps_flow[j]\n","        # Add first sample\n","        sample = self.get_data_sample(j)\n","        sequence.append(sample)\n","\n","        # Data augmentation according to first sample\n","        crop_window = None\n","        flip = None\n","        if 'crop_window' in sample.keys():\n","            crop_window = sample['crop_window']\n","        if 'flipped' in sample.keys():\n","            flip = sample['flipped']\n","\n","        for i in range(self.sequence_length-1):\n","            j += 1\n","            ts_old = ts_cur\n","            ts_cur = self.timestamps_flow[j]\n","            assert(ts_cur-ts_old < 100000 + 1000)\n","            sample = self.get_data_sample(\n","                j, crop_window=crop_window, flip=flip)\n","            sequence.append(sample)\n","\n","        # Check if the current sample is the first sample of a continuous sequence\n","        if idx == 0 or self.valid_indices[idx]-self.valid_indices[idx-1] != 1:\n","            sequence[0]['new_sequence'] = 1\n","            print(\"Timestamp {} is the first one of the next seq!\".format(\n","                self.timestamps_flow[self.valid_indices[idx]]))\n","        else:\n","            sequence[0]['new_sequence'] = 0\n","\n","        # random crop\n","        if self.crop_size is not None:\n","            i, j, h, w = RandomCrop.get_params(\n","                sample[\"event_volume_old\"], output_size=self.crop_size)\n","            keys_to_crop = [\"event_volume_old\", \"event_volume_new\",\n","                            \"flow_gt_event_volume_old\", \"flow_gt_event_volume_new\",\n","                            \"flow_gt_next\",]\n","\n","            for sample in sequence:\n","                for key, value in sample.items():\n","                    if key in keys_to_crop:\n","                        if isinstance(value, torch.Tensor):\n","                            sample[key] = tf.functional.crop(value, i, j, h, w)\n","                        elif isinstance(value, list) or isinstance(value, tuple):\n","                            sample[key] = [tf.functional.crop(v, i, j, h, w) for v in value]\n","        return sequence\n","\n","\n","class DatasetProvider:\n","    def __init__(self, dataset_path: Path, representation_type: RepresentationType, delta_t_ms: int = 100, num_bins=4, config=None, visualize=False, transforms=None):\n","        test_path = Path(os.path.join(dataset_path, 'test'))\n","        train_path = Path(os.path.join(dataset_path, 'train'))\n","        assert dataset_path.is_dir(), str(dataset_path)\n","        assert test_path.is_dir(), str(test_path)\n","        assert delta_t_ms == 100\n","        self.config = config\n","        self.name_mapper_test = []\n","\n","        if transforms:\n","            self.transforms = transforms\n","        else:\n","            self.transforms = tf.Compose([\n","                transforms.ToTensor(),  # Convert image to PyTorch tensor\n","            ])\n","\n","        # Assemble test sequences\n","        test_sequences = list()\n","        for child in test_path.iterdir():\n","            self.name_mapper_test.append(str(child).split(\"/\")[-1])\n","            test_sequences.append(Sequence(child, representation_type, 'test', delta_t_ms, num_bins,\n","                                               name_idx=len(self.name_mapper_test) - 1,\n","                                               visualize=visualize,\n","                                               transforms=self.transforms))\n","\n","        self.test_dataset = torch.utils.data.ConcatDataset(test_sequences)\n","\n","        # Assemble train sequences\n","        available_seqs = os.listdir(train_path)\n","\n","        seqs = available_seqs\n","\n","        train_sequences: list[Sequence] = []\n","        for seq in seqs:\n","            extra_arg = dict()\n","            train_sequences.append(Sequence(Path(train_path) / seq,\n","                                   representation_type=representation_type, mode=\"train\",\n","                                   load_gt=True, **extra_arg, transforms=self.transforms))\n","            self.train_dataset: torch.utils.data.ConcatDataset[Sequence] = torch.utils.data.ConcatDataset(train_sequences)\n","\n","    def get_test_dataset(self):\n","        return self.test_dataset\n","\n","    def get_train_dataset(self):\n","        return self.train_dataset\n","\n","    def get_name_mapping_test(self):\n","        return self.name_mapper_test\n","\n","    def summary(self, logger):\n","        logger.write_line(\n","            \"================================== Dataloader Summary ====================================\", True)\n","        logger.write_line(\"Loader Type:\\t\\t\" + self.__class__.__name__, True)\n","        logger.write_line(\"Number of Voxel Bins: {}\".format(\n","            self.test_dataset.datasets[0].num_bins), True)\n","        logger.write_line(\"Number of Train Sequences: {}\".format(\n","            len(self.train_dataset)), True)\n","\n","def train_collate(sample_list):\n","    batch = dict()\n","    for field_name in sample_list[0]:\n","        if field_name == 'timestamp':\n","            batch['timestamp'] = [sample[field_name] for sample in sample_list]\n","        if field_name == 'seq_name':\n","            batch['seq_name'] = [sample[field_name] for sample in sample_list]\n","        if field_name == 'new_sequence':\n","            batch['new_sequence'] = [sample[field_name]\n","                                     for sample in sample_list]\n","        if field_name.startswith(\"event_volume\"):\n","            batch[field_name] = torch.stack(\n","                [sample[field_name] for sample in sample_list])\n","        if field_name.startswith(\"flow_gt\") or field_name.startswith('prev_flow_gt') or field_name.startswith('next_flow_gt'):\n","            if all(field_name in x for x in sample_list):\n","                batch[field_name] = torch.stack(\n","                    [sample[field_name][0] for sample in sample_list])\n","                batch[field_name + '_valid_mask'] = torch.stack(\n","                    [sample[field_name][1] for sample in sample_list])\n","\n","    return batch\n","\n","\n","def rec_train_collate(sample_list):\n","    seq_length = len(sample_list[0])\n","    seq_of_batch = []\n","    for i in range(seq_length):\n","        seq_of_batch.append(train_collate(\n","            [sample[i] for sample in sample_list]))\n","    return seq_of_batch"],"metadata":{"trusted":true,"id":"BMr6cf3sGb7Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## `base.py`"],"metadata":{"id":"0yC5BJmEGb7S"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","\n","class build_resnet_block(nn.Module):\n","    \"\"\"\n","    a resnet block which includes two general_conv2d\n","    \"\"\"\n","    def __init__(self, channels, layers=2, do_batch_norm=False):\n","        super(build_resnet_block,self).__init__()\n","        self._channels = channels\n","        self._layers = layers\n","\n","        self.res_block = nn.Sequential(*[general_conv2d(in_channels=self._channels,\n","                                             out_channels=self._channels,\n","                                             strides=1,\n","                                             do_batch_norm=do_batch_norm) for i in range(self._layers)])\n","\n","    def forward(self,input_res):\n","        inputs = input_res.clone()\n","        input_res = self.res_block(input_res)\n","        return input_res + inputs\n","\n","class upsample_conv2d_and_predict_flow(nn.Module):\n","    \"\"\"\n","    an upsample convolution layer which includes a nearest interpolate and a general_conv2d\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, ksize=3, do_batch_norm=False):\n","        super(upsample_conv2d_and_predict_flow, self).__init__()\n","        self._in_channels = in_channels\n","        self._out_channels = out_channels\n","        self._ksize = ksize\n","        self._do_batch_norm = do_batch_norm\n","\n","        self.general_conv2d = general_conv2d(\n","            in_channels=self._in_channels,\n","            out_channels=self._out_channels,\n","            ksize=self._ksize,\n","            strides=1,\n","            do_batch_norm=self._do_batch_norm,\n","            padding=0\n","        )\n","\n","        self.pad = nn.ReflectionPad2d(\n","            padding=(\n","                int((self._ksize - 1) / 2),\n","                int((self._ksize - 1) / 2),\n","                int((self._ksize - 1) / 2),\n","                int((self._ksize - 1) / 2)\n","            )\n","        )\n","\n","        self.predict_flow = general_conv2d(\n","            in_channels=self._out_channels,\n","            out_channels=2,\n","            ksize=1,\n","            strides=1,\n","            padding=0,\n","            activation='tanh'\n","        )\n","\n","    def forward(self, conv):\n","        shape = conv.shape\n","        conv = nn.functional.interpolate(conv,size=[shape[2]*2,shape[3]*2],mode='nearest')\n","        conv = self.pad(conv)\n","        conv = self.general_conv2d(conv)\n","\n","        flow = self.predict_flow(conv) * 256.\n","\n","        return torch.cat([conv,flow.clone()], dim=1), flow\n","\n","def general_conv2d(in_channels,out_channels, ksize=3, strides=2, padding=1, do_batch_norm=False, activation='relu',\n","                   attention=False):\n","    \"\"\"\n","    a general convolution layer which includes a conv2d, a relu and a batch_normalize\n","    \"\"\"\n","    layers = [nn.Conv2d(in_channels = in_channels,out_channels = out_channels,kernel_size = ksize,\n","                        stride=strides,padding=padding)]\n","\n","    if activation == 'relu':\n","        layers.append(nn.ReLU(inplace=True))\n","\n","        if do_batch_norm:\n","            layers.append(nn.BatchNorm2d(out_channels,eps=1e-5,momentum=0.99))\n","\n","    elif activation == 'tanh':\n","        layers.append(nn.Tanh())\n","\n","        if do_batch_norm:\n","            layers.append(nn.BatchNorm2d(out_channels,eps=1e-5,momentum=0.99))\n","\n","    if attention:\n","        # layers.append(Attention(out_channels, heads=2, dim_head=8, dropout=0.1))\n","        layers.append(BlockSparseAttention(in_dim=out_channels, heads=2, blocksize=16))\n","\n","    return nn.Sequential(*layers)"],"metadata":{"trusted":true,"id":"2HJxkwoEGb7S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## `evflownet.py`"],"metadata":{"id":"V533QO-UGb7S"}},{"cell_type":"markdown","source":["### Attention Mechanisms"],"metadata":{"id":"2CWFq71nGt_-"}},{"cell_type":"markdown","source":["For some reason, the self-attention models use more than the 334GB RAM limit that Google Colab's TPU has."],"metadata":{"id":"wEzWqpr_G4QK"}},{"cell_type":"code","source":["class Attention(nn.Module):\n","    def __init__(self, dim, heads=2, dim_head=8, dropout=0.1):\n","        super().__init__()\n","\n","        self.dim = dim\n","        self.dim_head = dim_head\n","        inner_dim = dim_head * heads\n","        project_out = not (heads == 1 and dim_head == dim)\n","\n","        self.heads = heads\n","        self.scale = math.sqrt(dim_head)\n","\n","        self.attend = nn.Softmax(dim=-1)\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.to_q = nn.Linear(dim, inner_dim)\n","        self.to_k = nn.Linear(dim, inner_dim)\n","        self.to_v = nn.Linear(dim, inner_dim)\n","\n","        self.to_out = nn.Sequential(\n","            nn.Linear(inner_dim, dim),\n","            nn.Dropout(dropout)\n","        ) if project_out else nn.Identity()\n","\n","    def forward(self, x):\n","        B, C, H, W = x.size()  # Shape: [B, C, H, W]\n","\n","        x = x.view(B, C, -1).transpose(1, 2)  # Shape: [B, H*W, C]\n","\n","        q = self.to_q(x)\n","        k = self.to_k(x)\n","        v = self.to_v(x)\n","\n","        q = rearrange(q, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n","        k = rearrange(k, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n","        v = rearrange(v, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n","\n","        dots = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n","        attn = self.attend(dots)\n","        attn = self.dropout(attn)\n","\n","        out = torch.matmul(attn, v)\n","        out = rearrange(out, \"b h n d -> b n (h d)\", h=self.heads, d=self.dim_head)\n","\n","        out = self.to_out(out)\n","\n","        out = out.transpose(1, 2).view(B, C, H, W)  # Shape: [B, C, H, W]\n","\n","        return out"],"metadata":{"trusted":true,"id":"NIND8uhAGb7S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def split_heads(x, heads):\n","    return torch.transpose(split_states(x, heads), 1, 2)\n","\n","def merge_heads(x):\n","    return merge_states(torch.transpose(x, 1, 2))\n","\n","def split_states(x, heads):\n","    x_shape = x.size()\n","    m = x_shape[-1]\n","    new_x_shape = x_shape[:-1] + (heads, m // heads)\n","    return torch.reshape(x, new_x_shape)\n","\n","def merge_states(x):\n","    x_shape = x.size()\n","    new_x_shape = x_shape[:-2] + (x_shape[-2] * x_shape[-1],)\n","    return torch.reshape(x, new_x_shape)\n","\n","class BlockSparseAttention(nn.Module):\n","    def __init__(self, in_dim, heads, blocksize=32):\n","        super(BlockSparseAttention, self).__init__()\n","        self.heads = heads\n","        self.blocksize = blocksize\n","        self.query = nn.Linear(in_dim, in_dim)\n","        self.key = nn.Linear(in_dim, in_dim)\n","        self.value = nn.Linear(in_dim, in_dim)\n","        self.scale = 1.0 / np.sqrt(in_dim // heads)\n","\n","    def forward(self, x):\n","        B, C, H, W = x.size()\n","        x = x.view(B, H * W, C)\n","        q = self.query(x)\n","        k = self.key(x)\n","        v = self.value(x)\n","\n","        q = split_heads(q, self.heads)\n","        k = split_heads(k, self.heads)\n","        v = split_heads(v, self.heads)\n","\n","        w = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n","        mask = torch.zeros_like(w).fill_(-1e9)\n","        n_ctx = q.size(2)\n","\n","        for i in range(0, n_ctx, self.blocksize):\n","            for j in range(0, n_ctx, self.blocksize):\n","                mask[:, :, i:i + self.blocksize, j:j + self.blocksize] = 0\n","\n","        w = w + mask\n","        w = F.softmax(w, dim=-1)\n","\n","        a = torch.matmul(w, v)\n","        a = merge_heads(a)\n","\n","        a = a.view(B, C, H, W)\n","        return a"],"metadata":{"id":"tj3-GySgqNxE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### EvFlowNet"],"metadata":{"id":"UicJAHYMGwui"}},{"cell_type":"code","source":["_BASE_CHANNELS = 8 # default: 64\n","\n","class EVFlowNet(nn.Module):\n","    def __init__(self, args):\n","        super(EVFlowNet,self).__init__()\n","        self._args = args\n","\n","        self.encoder1 = general_conv2d(\n","            in_channels=4,\n","            out_channels=_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm,\n","            attention=False\n","        )\n","        self.encoder2 = general_conv2d(\n","            in_channels=_BASE_CHANNELS,\n","            out_channels=2*_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm,\n","            attention=False\n","        )\n","        self.encoder3 = general_conv2d(\n","            in_channels=2*_BASE_CHANNELS,\n","            out_channels=4*_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm,\n","            attention=False\n","        )\n","        self.encoder4 = general_conv2d(\n","            in_channels=4*_BASE_CHANNELS,\n","            out_channels=8*_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm,\n","            attention=False\n","        )\n","\n","        self.resnet_block = nn.Sequential(*[build_resnet_block(8*_BASE_CHANNELS, do_batch_norm=not self._args.no_batch_norm) for i in range(2)])\n","\n","        self.decoder1 = upsample_conv2d_and_predict_flow(\n","            in_channels=16*_BASE_CHANNELS,\n","            out_channels=4*_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm\n","        )\n","        self.decoder2 = upsample_conv2d_and_predict_flow(\n","            in_channels=8*_BASE_CHANNELS+2,\n","            out_channels=2*_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm\n","        )\n","        self.decoder3 = upsample_conv2d_and_predict_flow(\n","            in_channels=4*_BASE_CHANNELS+2,\n","            out_channels=_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm\n","        )\n","        self.decoder4 = upsample_conv2d_and_predict_flow(\n","            in_channels=2*_BASE_CHANNELS+2,\n","            out_channels=int(_BASE_CHANNELS/2),\n","            do_batch_norm=not self._args.no_batch_norm\n","        )\n","\n","    def forward(self, inputs: torch.Tensor) -> Dict[str, torch.Tensor]: # [16, 4, 480, 640]\n","\n","        # encoder\n","        skip_connections = {}\n","        inputs = self.encoder1(inputs)\n","        # inputs = checkpoint(self.encoder1, inputs)\n","        skip_connections['skip0'] = inputs.clone() # torch.Size([16, 64, 240, 320])\n","\n","        inputs = self.encoder2(inputs)\n","        # inputs = checkpoint(self.encoder2, inputs)\n","        skip_connections['skip1'] = inputs.clone() # torch.Size([16, 128, 120, 160])\n","\n","        inputs = self.encoder3(inputs)\n","        # inputs = checkpoint(self.encoder3, inputs)\n","        skip_connections['skip2'] = inputs.clone() # torch.Size([16, 256, 60, 80])\n","\n","        inputs = self.encoder4(inputs)\n","        # inputs = checkpoint(self.encoder4, inputs)\n","        skip_connections['skip3'] = inputs.clone() # torch.Size([16, 512, 30, 40])\n","\n","        # transition\n","        inputs = self.resnet_block(inputs) # torch.Size([16, 512, 30, 40])\n","\n","        # decoder\n","        flow_dict = {}\n","        inputs = torch.cat([inputs, skip_connections['skip3']], dim=1) # torch.Size([16, 1024, 30, 40])\n","        inputs, flow = self.decoder1(inputs) # inputs: torch.Size([16, 258, 60, 80])\n","        flow_dict['flow0'] = flow.clone() # torch.Size([16, 2, 60, 80])\n","\n","        inputs = torch.cat([inputs, skip_connections['skip2']], dim=1) # torch.Size([16, 514, 60, 80])\n","        inputs, flow = self.decoder2(inputs) # inputs: torch.Size([16, 130, 120, 160])\n","        flow_dict['flow1'] = flow.clone() # torch.Size([16, 2, 120, 160])\n","\n","        inputs = torch.cat([inputs, skip_connections['skip1']], dim=1) # torch.Size([16, 258, 120, 160])\n","        inputs, flow = self.decoder3(inputs) # inputs: torch.Size([16, 66, 240, 320])\n","        flow_dict['flow2'] = flow.clone() # torch.Size([16, 2, 240, 320])\n","\n","        inputs = torch.cat([inputs, skip_connections['skip0']], dim=1) # torch.Size([16, 130, 240, 320])\n","        inputs, flow = self.decoder4(inputs) # inputs: torch.Size([16, 34, 480, 640])\n","        flow_dict['flow3'] = flow.clone() # torch.Size([16, 2, 480, 640])\n","\n","        return flow_dict"],"metadata":{"trusted":true,"id":"4jrPl2paGb7T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Image Preprocessing"],"metadata":{"id":"zG59jw9jM6k9"}},{"cell_type":"code","source":["class HistogramEqualization:\n","    def __call__(self, image):\n","        print(np.array(image).shape)\n","\n","        # Needs to have 3 channels or color passed in\n","        # runs into the problem of loss of information in dimensionality reduction\n","        np_image = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n","\n","#         np_image = np.array(image)\n","        eq_image = cv2.equalizeHist(np_image)\n","        return Image.fromarray(eq_image)\n","\n","# combined_transform = CombinedTransform(transform=tf.Compose([\n","#     tf.GaussianBlur(kernel_size=(5, 5)),\n","# ]))"],"metadata":{"id":"kriYRvGWNEIk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torchvision.transforms as transforms\n","from torchvision.transforms.functional import to_pil_image, to_tensor\n","import torch.nn.functional as nn_F\n","\n","class DSECPreprocess:\n","    def __init__(self, kernel_size=5, sigma=1.0):\n","        self.kernel_size = kernel_size\n","        self.sigma = sigma\n","\n","    def __call__(self, img):\n","        return self.preprocess_image(img)\n","\n","    def preprocess_image(self, img):\n","        img = img.to(torch.float32)\n","        img = img.mean(dim=0, keepdim=True)\n","        img = (img - img.min()) / (img.max() - img.min() + 1e-5)\n","\n","        img = self.gaussian_blur(img)\n","        # img = self.histogram_equalization(img)\n","\n","        return img\n","\n","    def gaussian_blur(self, img):\n","        _, channels, height, width = img.shape\n","        # img = img.unsqueeze(0)  # Add batch dimension\n","\n","        kernel = self.get_gaussian_kernel(self.kernel_size, self.sigma)\n","        kernel = kernel.expand(channels, 1, -1, -1)  # Expand to match the number of channels\n","\n","        img = img.double() # expects double (torch.float64)\n","        kernel = kernel.double()\n","        img = nn_F.conv2d(img, kernel, padding=self.kernel_size // 2, groups=channels)\n","\n","        # img = img.squeeze(0)  # Remove batch dimension\n","        return img\n","\n","    def get_gaussian_kernel(self, kernel_size, sigma):\n","        k = torch.arange(kernel_size).float()\n","        x = k - (kernel_size - 1) / 2\n","        gauss = torch.exp(-0.5 * (x / sigma)**2)\n","        gauss = gauss / gauss.sum()\n","        kernel = gauss[:, None] * gauss[None, :]\n","        return kernel[None, None, :, :]\n","\n","    def histogram_equalization(self, img):\n","        batch_size, channels, height, width = img.shape\n","\n","        img1 = img[:, :2]  # First two channels\n","        img2 = img[:, 2:]  # Last two channels\n","\n","        equalized_images1 = []\n","        equalized_images2 = []\n","\n","        for i in range(batch_size):\n","            img_i1 = to_pil_image(img1[i])\n","            img_i1 = transforms.functional.equalize(img_i1)\n","            img_i1 = to_tensor(img_i1)\n","            equalized_images1.append(img_i1)\n","\n","            img_i2 = to_pil_image(img2[i])\n","            img_i2 = transforms.functional.equalize(img_i2)\n","            img_i2 = to_tensor(img_i2)\n","            equalized_images2.append(img_i2)\n","\n","        equalized_img1 = torch.stack(equalized_images1, dim=0)\n","        equalized_img2 = torch.stack(equalized_images2, dim=0)\n","        return equalized_img1, equalized_img2"],"metadata":{"id":"mq9Q6jLvM8jh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a = torch.rand((16, 4, 480, 640))\n","b = DSECPreprocess()(a)\n","b.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aTP-xf29Sl8s","executionInfo":{"status":"ok","timestamp":1720952204927,"user_tz":-540,"elapsed":847,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}},"outputId":"d0539901-e65e-4dc7-f5c2-55eb0c374bda"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 4, 480, 640])"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["class CombinedTransform:\n","    def __init__(self, transform=tf.Compose([ tf.ToTensor() ])):\n","        self.transform = transform\n","\n","    def __call__(self, flow_dict):\n","        seed = np.random.randint(2147483647)\n","\n","        flow_columns = ['event_volume', 'flow_gt', 'prev_flow_gt', 'next_flow_gt']\n","        flow_columns = [c for c in flow_columns if c in list(flow_dict.keys())]\n","\n","        for col in flow_columns:\n","            torch.manual_seed(seed)\n","\n","            if type(flow_dict[col]) == list:\n","                flow_dict[col] = [ self.transform(img) for img in flow_dict[col] ]\n","            else:\n","                flow_dict[col] = self.transform(flow_dict[col])\n","\n","        return flow_dict"],"metadata":{"id":"HyemBQ4FOKgu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["combined_transform = CombinedTransform(transform=DSECPreprocess(kernel_size=5, sigma=1.0))"],"metadata":{"id":"b0W3J3r4NAu7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## `main.py`"],"metadata":{"id":"-W_HpclqGb7T"}},{"cell_type":"markdown","source":["Instead of changing the `Sequence`, just change how the data are loaded. Or iterate over the dataloader so that you use many images at once."],"metadata":{"id":"3lufwoSn32eN"}},{"cell_type":"code","source":["class RepresentationType(Enum):\n","    VOXEL = auto()\n","    STEPAN = auto()\n","\n","def set_seed(seed):\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","\n","def compute_epe_error(pred_flow: torch.Tensor, gt_flow: torch.Tensor):\n","    '''\n","    end-point-error (ground truthと予測値の二乗誤差)を計算\n","    pred_flow: torch.Tensor, Shape: torch.Size([B, 2, 480, 640]) => 予測したオプティカルフローデータ\n","    gt_flow: torch.Tensor, Shape: torch.Size([B, 2, 480, 640]) => 正解のオプティカルフローデータ\n","    '''\n","    epe = torch.mean(torch.mean(torch.norm(pred_flow - gt_flow, p=2, dim=1), dim=(1, 2)), dim=0)\n","    return epe\n","\n","def save_optical_flow_to_npy(flow: torch.Tensor, file_name: str):\n","    '''\n","    optical flowをnpyファイルに保存\n","    flow: torch.Tensor, Shape: torch.Size([2, 480, 640]) => オプティカルフローデータ\n","    file_name: str => ファイル名\n","    '''\n","    np.save(f\"{file_name}.npy\", flow.cpu().numpy())"],"metadata":{"trusted":true,"id":"k-O8nvDQGb7T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["args = cfg\n","set_seed(args.seed)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","'''\n","    ディレクトリ構造:\n","\n","    data\n","    ├─test\n","    |  ├─test_city\n","    |  |    ├─events_left\n","    |  |    |   ├─events.h5\n","    |  |    |   └─rectify_map.h5\n","    |  |    └─forward_timestamps.txt\n","    └─train\n","        ├─zurich_city_11_a\n","        |    ├─events_left\n","        |    |       ├─ events.h5\n","        |    |       └─ rectify_map.h5\n","        |    ├─ flow_forward\n","        |    |       ├─ 000134.png\n","        |    |       |.....\n","        |    └─ forward_timestamps.txt\n","        ├─zurich_city_11_b\n","        └─zurich_city_11_c\n","    '''\n","\n","# ------------------\n","#    Dataloader\n","# ------------------\n","\n","loader = DatasetProvider(\n","    dataset_path=Path(args.dataset_path),\n","    representation_type=RepresentationType.VOXEL,\n","    delta_t_ms=100,\n","    num_bins=4,\n","    transforms=combined_transform # Custom class\n",")\n","train_set = loader.get_train_dataset()\n","test_set = loader.get_test_dataset()\n","\n","# def split_train_valid(dataset):\n","#     train_indices = []\n","#     valid_indices = []\n","#     for idx in range(len(dataset)):\n","#         sample = dataset[idx]\n","#         if 'flow_gt_valid_mask' in sample and sample['flow_gt_valid_mask'].all():\n","#             valid_indices.append(idx)\n","#         else:\n","#             train_indices.append(idx)\n","#     train_subset = torch.utils.data.Subset(dataset, train_indices)\n","#     valid_subset = torch.utils.data.Subset(dataset, valid_indices)\n","#     return train_subset, valid_subset\n","\n","# train_set_split, valid_set_split = split_train_valid(train_set)\n","\n","collate_fn = train_collate\n","train_data = DataLoader(train_set, # train_set_split\n","                        batch_size=args.data_loader.train.batch_size,\n","                        shuffle=args.data_loader.train.shuffle,\n","                        collate_fn=collate_fn,\n","                        drop_last=False,\n","                        num_workers=os.cpu_count(),\n","                        pin_memory=True)\n","# valid_data = DataLoader(valid_set_split,\n","#                         batch_size=args.data_loader.train.batch_size,\n","#                         shuffle=args.data_loader.train.shuffle,\n","#                         collate_fn=collate_fn,\n","#                         drop_last=False,\n","#                         num_workers=os.cpu_count(),\n","#                         pin_memory=True)\n","test_data = DataLoader(test_set,\n","                       batch_size=args.data_loader.test.batch_size,\n","                       shuffle=args.data_loader.test.shuffle,\n","                       collate_fn=collate_fn,\n","                       drop_last=False,\n","                       num_workers=os.cpu_count(),\n","                       pin_memory=True)\n","\n","'''\n","train data:\n","    Type of batch: Dict\n","    Key: seq_name, Type: list\n","    Key: event_volume, Type: torch.Tensor, Shape: torch.Size([Batch, 4, 480, 640]) => イベントデータのバッチ\n","    Key: flow_gt, Type: torch.Tensor, Shape: torch.Size([Batch, 2, 480, 640]) => オプティカルフローデータのバッチ\n","    Key: flow_gt_valid_mask, Type: torch.Tensor, Shape: torch.Size([Batch, 1, 480, 640]) => オプティカルフローデータのvalid. ベースラインでは使わない\n","\n","test data:\n","    Type of batch: Dict\n","    Key: seq_name, Type: list\n","    Key: event_volume, Type: torch.Tensor, Shape: torch.Size([Batch, 4, 480, 640]) => イベントデータのバッチ\n","'''"],"metadata":{"trusted":true,"id":"2PL6GvxuGb7T","colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"status":"ok","timestamp":1720927384137,"user_tz":-540,"elapsed":7,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}},"outputId":"031642ce-5676-47ec-d7c8-6aaffdc09b7c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ntrain data:\\n    Type of batch: Dict\\n    Key: seq_name, Type: list\\n    Key: event_volume, Type: torch.Tensor, Shape: torch.Size([Batch, 4, 480, 640]) => イベントデータのバッチ\\n    Key: flow_gt, Type: torch.Tensor, Shape: torch.Size([Batch, 2, 480, 640]) => オプティカルフローデータのバッチ\\n    Key: flow_gt_valid_mask, Type: torch.Tensor, Shape: torch.Size([Batch, 1, 480, 640]) => オプティカルフローデータのvalid. ベースラインでは使わない\\n\\ntest data:\\n    Type of batch: Dict\\n    Key: seq_name, Type: list\\n    Key: event_volume, Type: torch.Tensor, Shape: torch.Size([Batch, 4, 480, 640]) => イベントデータのバッチ\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":72}]},{"cell_type":"code","source":["# ------------------\n","#       Model\n","# ------------------\n","model = EVFlowNet(args.train).to(device)\n","\n","# ------------------\n","#   optimizer\n","# ------------------\n","optimizer = torch.optim.Adam(model.parameters(), lr=args.train.initial_learning_rate, weight_decay=args.train.weight_decay)\n","scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=args.train.learning_rate_decay)\n","\n","loss_fn = TotalLoss(smoothness_weight=0.5)"],"metadata":{"id":"ZULZTTNcwxWL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# num_epochs = args.train.epochs\n","num_epochs = 1\n","\n","epe_losses = [[] for _ in range(num_epochs)]\n","overall_losses = [[] for _ in range(num_epochs)]"],"metadata":{"trusted":true,"id":"XOEcpMPHGb7T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BATCH_CONCAT = 2"],"metadata":{"id":"_aYlJLJa0Yqf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ------------------\n","#   Start training\n","# ------------------\n","model.train()\n","\n","for epoch in range(num_epochs):\n","\n","    total_loss = 0\n","    prev_event_volumes = [] # Acts as a queue\n","\n","    print(\"on epoch: {}\".format(epoch + 1))\n","    for i, batch in enumerate(tqdm(train_data)):\n","\n","        try:\n","            batch: Dict[str, Any]\n","\n","            event_image = batch[\"event_volume\"].to(device) # [B, 4, 480, 640]\n","            ground_truth_flow = batch[\"flow_gt\"].to(device) # [B, 2, 480, 640]\n","\n","            prev_event_volumes.append(event_image)\n","\n","            prev_ground_truth_flow = batch['prev_flow_gt'].to(device) # [B, 2, 480, 640]\n","            next_ground_truth_flow = batch['next_flow_gt'].to(device) # [B, 2, 480, 640]\n","\n","            # Temporal averaging\n","            # avg_flow_gt = torch.mean(torch.cat([\n","            #     ground_truth_flow.unsqueeze(1),\n","            #     prev_ground_truth_flow.unsqueeze(1),\n","            #     next_ground_truth_flow.unsqueeze(1)\n","            # ], dim=1), dim=1)\n","\n","            try:\n","                avg_event_image = torch.mean(torch.stack(prev_event_volumes, dim=0), dim=0)\n","            except RuntimeError:\n","                current_batch_size, C, H, W = event_image.shape\n","                zero_padding = torch.zeros((16 - current_batch_size, C, H, W), dtype=event_image.dtype, device=event_image.device)\n","                padded_event_image = torch.cat([event_image, zero_padding], dim=0)\n","\n","                prev_event_volumes = prev_event_volumes[:-1] + [padded_event_image]\n","                avg_event_image = torch.mean(torch.stack(prev_event_volumes, dim=0), dim=0)\n","\n","            print(avg_event_image.shape)\n","            flow_dict = model(avg_event_image) # 'flow3' is of shape [B, 2, 480, 640]\n","\n","            epe_loss: torch.Tensor = compute_epe_error(flow_dict['flow3'], ground_truth_flow)\n","            overall_loss: torch.Tensor = loss_fn(flow_dict,\n","                                         prev_ground_truth_flow,\n","                                         next_ground_truth_flow,\n","                                         model)\n","\n","            print(f\"batch {i} OVERALL LOSS: {overall_loss.item()}\")\n","            print(f\"batch {i} EPE LOSS: {epe_loss.item()}\")\n","            overall_losses[epoch].append(overall_loss.item())\n","            epe_losses[epoch].append(epe_loss.item())\n","\n","            optimizer.zero_grad()\n","            epe_loss.backward() # Change this to which loss function is to be updated\n","            optimizer.step()\n","\n","            total_loss += epe_loss.item() # This too\n","\n","            if len(prev_event_volumes) >= BATCH_CONCAT:\n","                prev_event_volumes.pop()\n","\n","        except KeyboardInterrupt:\n","            current_time = time.strftime(\"%Y%m%d%H%M%S\")\n","            model_path = f\"models/model_{current_time}.pth\" # /kaggle/input/\n","            torch.save(model.state_dict(), model_path)\n","            print(f\"Model saved to {model_path}\")\n","\n","            raise SystemExit(\"KeyboardInterrupt\")\n","\n","    scheduler.step()\n","\n","    print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_data)}')"],"metadata":{"trusted":true,"id":"YbeX9fnmGb7U","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"c8be894d-4f5e-4465-c18f-c207d6935804","executionInfo":{"status":"error","timestamp":1720927676365,"user_tz":-540,"elapsed":35897,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["on epoch: 1\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/126 [00:00<?, ?it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b3c4151b9a0>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n","    if w.is_alive():\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n","    assert self._parent_pid == os.getpid(), 'can only test a child process'\n","AssertionError: can only test a child process\n","Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b3c4151b9a0>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n","    Exception ignored in: self._shutdown_workers()<function _MultiProcessingDataLoaderIter.__del__ at 0x7b3c4151b9a0>\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n","    if w.is_alive():\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n","    assert self._parent_pid == os.getpid(), 'can only test a child process'\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n","AssertionError: can only test a child process\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n","Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b3c4151b9a0>\n","    if w.is_alive():\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n","Traceback (most recent call last):\n","      File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n","assert self._parent_pid == os.getpid(), 'can only test a child process'\n","AssertionError:     can only test a child process\n","self._shutdown_workers()\n","Exception ignored in:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n","    if w.is_alive():\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n","    <function _MultiProcessingDataLoaderIter.__del__ at 0x7b3c4151b9a0>assert self._parent_pid == os.getpid(), 'can only test a child process'\n","\n","AssertionErrorTraceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n","    self._shutdown_workers(): \n","can only test a child process  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n","    \n","Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b3c4151b9a0>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n","if w.is_alive():    \n","self._shutdown_workers()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n","    if w.is_alive():  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n","\n","      File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n","assert self._parent_pid == os.getpid(), 'can only test a child process'    \n","assert self._parent_pid == os.getpid(), 'can only test a child process'AssertionError\n",": AssertionErrorcan only test a child process\n",": can only test a child processException ignored in: \n","Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b3c4151b9a0><function _MultiProcessingDataLoaderIter.__del__ at 0x7b3c4151b9a0>\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n","Traceback (most recent call last):\n","      File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n","self._shutdown_workers()\n","    self._shutdown_workers()  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n","\n","      File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n","if w.is_alive():    \n","if w.is_alive():  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n","\n","      File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n","assert self._parent_pid == os.getpid(), 'can only test a child process'    \n","assert self._parent_pid == os.getpid(), 'can only test a child process'AssertionError\n",": AssertionErrorcan only test a child process: \n","can only test a child process\n","Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b3c4151b9a0>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n","    if w.is_alive():\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n","    assert self._parent_pid == os.getpid(), 'can only test a child process'\n","AssertionError: can only test a child process\n","Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b3c4151b9a0>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n","    if w.is_alive():\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n","    assert self._parent_pid == os.getpid(), 'can only test a child process'\n","AssertionError: can only test a child process\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([16, 1, 480, 640])\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/126 [00:35<?, ?it/s]\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"Given groups=1, weight of size [32, 4, 3, 3], expected input[16, 1, 480, 640] to have 4 channels, but got 1 channels instead","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-77-93e5e17dcf31>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_event_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mflow_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_event_image\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 'flow3' is of shape [B, 2, 480, 640]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mepe_loss\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_epe_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflow_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'flow3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth_flow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-60b8a3b06517>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mskip_connections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;31m# inputs = checkpoint(self.encoder1, inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mskip_connections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'skip0'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# torch.Size([16, 64, 240, 320])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 4, 3, 3], expected input[16, 1, 480, 640] to have 4 channels, but got 1 channels instead"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# epe_losses = list(map(lambda x: x.item(), epe_losses[0]))\n","# overall_losses = list(map(lambda x: x.item(), overall_losses[0]))\n","\n","plt.figure(figsize=(16, 9))\n","len_x = min(len(epe_losses), len(overall_losses))\n","\n","plt.plot(epe_losses[:len_x])\n","plt.plot(overall_losses[:len_x])\n","\n","plt.xlabel('Batch Number')\n","plt.ylabel('Loss')\n","\n","plt.grid()\n","plt.legend()\n","\n","plt.show()"],"metadata":{"trusted":true,"id":"lc4Q3acvGb7U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","# Create the directory if it doesn't exist\n","# if not os.path.exists('checkpoints'):\n","#     os.makedirs('checkpoints')\n","\n","current_time = time.strftime(\"%Y%m%d-%H%M%S\")\n","model_path = f\"models/model_{current_time}_epoch4.pth\"\n","torch.save(model.state_dict(), model_path)\n","print(f\"Model saved to {model_path}\")"],"metadata":{"trusted":true,"id":"FtltB8OsGb7U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","flow: torch.Tensor = torch.tensor([]).to(device)\n","\n","prev_event_volumes = [] # Acts as a queue\n","\n","with torch.no_grad():\n","    print(\"start test\")\n","    for batch in tqdm(test_data):\n","        batch: Dict[str, Any]\n","\n","        event_image = batch[\"event_volume\"].to(device)\n","        prev_event_volumes.append(event_image)\n","\n","        avg_event_image = torch.mean(torch.stack(prev_event_volumes, dim=0), dim=0)\n","\n","        batch_flow = model(avg_event_image) # [1, 2, 480, 640]\n","        flow = torch.cat((flow, batch_flow['flow3']), dim=0)  # [N, 2, 480, 640]\n","\n","        if len(prev_event_volumes) >= BATCH_CONCAT:\n","            prev_event_volumes.pop()\n","\n","    print(\"test done\")\n","# ------------------\n","#  save submission\n","# ------------------\n","current_time = time.strftime(\"%Y%m%d-%H%M%S\")\n","save_optical_flow_to_npy(flow, f'submissions/submission_{current_time}')"],"metadata":{"trusted":true,"id":"LdYOQdyRGb7U"},"execution_count":null,"outputs":[]}]}