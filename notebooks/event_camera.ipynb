{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["VXKT-9HLotlT","urABjD6Gohqg","ae2Eh6v8pOHD","fiL_SR0Vo_X9","ZMHKr3VBp5z-"],"authorship_tag":"ABX9TyPyWS98/jVpuyAz8jLtLzwu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h5Z6aMsPn-Ak","executionInfo":{"status":"ok","timestamp":1720088849441,"user_tz":-540,"elapsed":2326,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}},"outputId":"557bf535-8d78-4e1c-f4c2-dd5962be9d33"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# !pip install hydra-core omegaconf hdf5plugin h5py numba imageio imageio-ffmpeg tqdm torchvision --quiet"],"metadata":{"id":"PXse62gDptQS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import hydra\n","from omegaconf import DictConfig\n","from torch.utils.data import DataLoader\n","import random\n","import numpy as np\n","# from src.models.evflownet import EVFlowNet\n","# from src.datasets import DatasetProvider\n","from enum import Enum, auto\n","# from src.datasets import train_collate\n","from tqdm import tqdm\n","from pathlib import Path\n","from typing import Dict, Any\n","import os\n","import time\n","\n","import math\n","from pathlib import PurePath\n","from typing import Tuple\n","import cv2\n","import hdf5plugin\n","import h5py\n","from numba import jit\n","import imageio\n","imageio.plugins.freeimage.download()\n","import imageio.v3 as iio\n","from torchvision.transforms import RandomCrop\n","from torchvision import transforms as tf\n","from torch.utils.data import Dataset\n","\n","# from src.utils import RepresentationType, VoxelGrid, flow_16bit_to_float\n","from torch import nn"],"metadata":{"id":"0KP7GWQEoOGr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","project_dir = '/content/drive/My Drive/Colab Notebooks/event_camera_repo'\n","%cd {project_dir}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"saHWneBJp_dG","executionInfo":{"status":"ok","timestamp":1720089082049,"user_tz":-540,"elapsed":3,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}},"outputId":"c96c96fa-58ad-44bc-8050-8c23a33abb86"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Colab Notebooks/dl_comp\n"]}]},{"cell_type":"code","source":["from omegaconf import OmegaConf\n","from hydra import compose, initialize\n","\n","initialize(config_path=\"configs\")\n","cfg = compose(config_name=\"base\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PK-8xIbqhgm1","executionInfo":{"status":"ok","timestamp":1720089084771,"user_tz":-540,"elapsed":345,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}},"outputId":"10a7ba43-f3ef-4153-b345-65207de0529d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-9-5a981e68fd2e>:4: UserWarning: \n","The version_base parameter is not specified.\n","Please specify a compatability version level, or None.\n","Will assume defaults for version 1.1\n","  initialize(config_path=\"configs\")\n"]}]},{"cell_type":"markdown","source":["# `utils.py`"],"metadata":{"id":"VXKT-9HLotlT"}},{"cell_type":"code","source":["def set_seed(seed: int = 0) -> None:\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","class RepresentationType(Enum):\n","    VOXEL = auto()\n","    STEPAN = auto()\n","\n","\n","class EventRepresentation:\n","    def __init__(self):\n","        pass\n","\n","    def convert(self, events):\n","        raise NotImplementedError\n","\n","\n","class VoxelGrid(EventRepresentation):\n","    def __init__(self, input_size: tuple, normalize: bool):\n","        assert len(input_size) == 3\n","        self.voxel_grid = torch.zeros(\n","            (input_size), dtype=torch.float, requires_grad=False)\n","        self.nb_channels = input_size[0]\n","        self.normalize = normalize\n","\n","    def convert(self, events):\n","        C, H, W = self.voxel_grid.shape\n","        with torch.no_grad():\n","            self.voxel_grid = self.voxel_grid.to(events['p'].device)\n","            voxel_grid = self.voxel_grid.clone()\n","\n","            t_norm = events['t']\n","            t_norm = (C - 1) * (t_norm-t_norm[0]) / (t_norm[-1]-t_norm[0])\n","\n","            x0 = events['x'].int()\n","            y0 = events['y'].int()\n","            t0 = t_norm.int()\n","\n","            value = 2*events['p']-1\n","            #start_t = time()\n","            for xlim in [x0, x0+1]:\n","                for ylim in [y0, y0+1]:\n","                    for tlim in [t0, t0+1]:\n","\n","                        mask = (xlim < W) & (xlim >= 0) & (ylim < H) & (\n","                            ylim >= 0) & (tlim >= 0) & (tlim < self.nb_channels)\n","                        interp_weights = value * (1 - (xlim-events['x']).abs()) * (\n","                            1 - (ylim-events['y']).abs()) * (1 - (tlim - t_norm).abs())\n","                        index = H * W * tlim.long() + \\\n","                            W * ylim.long() + \\\n","                            xlim.long()\n","\n","                        voxel_grid.put_(\n","                            index[mask], interp_weights[mask], accumulate=True)\n","\n","            if self.normalize:\n","                mask = torch.nonzero(voxel_grid, as_tuple=True)\n","                if mask[0].size()[0] > 0:\n","                    mean = voxel_grid[mask].mean()\n","                    std = voxel_grid[mask].std()\n","                    if std > 0:\n","                        voxel_grid[mask] = (voxel_grid[mask] - mean) / std\n","                    else:\n","                        voxel_grid[mask] = voxel_grid[mask] - mean\n","\n","        return voxel_grid\n","\n","\n","class PolarityCount(EventRepresentation):\n","    def __init__(self, input_size: tuple):\n","        assert len(input_size) == 3\n","        self.voxel_grid = torch.zeros(\n","            (input_size), dtype=torch.float, requires_grad=False)\n","        self.nb_channels = input_size[0]\n","\n","    def convert(self, events):\n","        C, H, W = self.voxel_grid.shape\n","        with torch.no_grad():\n","            self.voxel_grid = self.voxel_grid.to(events['p'].device)\n","            voxel_grid = self.voxel_grid.clone()\n","\n","            x0 = events['x'].int()\n","            y0 = events['y'].int()\n","\n","            #start_t = time()\n","            for xlim in [x0, x0+1]:\n","                for ylim in [y0, y0+1]:\n","                    mask = (xlim < W) & (xlim >= 0) & (ylim < H) & (\n","                        ylim >= 0)\n","                    interp_weights = (1 - (xlim-events['x']).abs()) * (\n","                        1 - (ylim-events['y']).abs())\n","                    index = H * W * events['p'].long() + \\\n","                        W * ylim.long() + \\\n","                        xlim.long()\n","\n","                    voxel_grid.put_(\n","                        index[mask], interp_weights[mask], accumulate=True)\n","\n","        return voxel_grid\n","\n","\n","def flow_16bit_to_float(flow_16bit: np.ndarray):\n","    assert flow_16bit.dtype == np.uint16\n","    assert flow_16bit.ndim == 3\n","    h, w, c = flow_16bit.shape\n","    assert c == 3\n","\n","    valid2D = flow_16bit[..., 2] == 1\n","    assert valid2D.shape == (h, w)\n","    assert np.all(flow_16bit[~valid2D, -1] == 0)\n","    valid_map = np.where(valid2D)\n","\n","    # to actually compute something useful:\n","    flow_16bit = flow_16bit.astype('float')\n","\n","    flow_map = np.zeros((h, w, 2))\n","    flow_map[valid_map[0], valid_map[1], 0] = (\n","        flow_16bit[valid_map[0], valid_map[1], 0] - 2 ** 15) / 128\n","    flow_map[valid_map[0], valid_map[1], 1] = (\n","        flow_16bit[valid_map[0], valid_map[1], 1] - 2 ** 15) / 128\n","    return flow_map, valid2D"],"metadata":{"id":"31893RK0ov-q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# `datasets.py`"],"metadata":{"id":"urABjD6Gohqg"}},{"cell_type":"code","source":["VISU_INDEX = 1\n","\n","\n","class EventSlicer:\n","    def __init__(self, h5f: h5py.File):\n","        self.h5f = h5f\n","        print(h5f.filename, list(h5f.keys()))\n","\n","        self.events = dict()\n","        for dset_str in ['p', 'x', 'y', 't']:\n","            print('events/{}'.format(dset_str))\n","            self.events[dset_str] = self.h5f['events/{}'.format(dset_str)]\n","\n","        # This is the mapping from milliseconds to event index:\n","        # It is defined such that\n","        # (1) t[ms_to_idx[ms]] >= ms*1000\n","        # (2) t[ms_to_idx[ms] - 1] < ms*1000\n","        # ,where 'ms' is the time in milliseconds and 't' the event timestamps in microseconds.\n","        #\n","        # As an example, given 't' and 'ms':\n","        # t:    0     500    2100    5000    5000    7100    7200    7200    8100    9000\n","        # ms:   0       1       2       3       4       5       6       7       8       9\n","        #\n","        # we get\n","        #\n","        # ms_to_idx:\n","        #       0       2       2       3       3       3       5       5       8       9\n","        self.ms_to_idx = np.asarray(self.h5f['ms_to_idx'], dtype='int64')\n","\n","        self.t_offset = int(h5f['t_offset'][()])\n","        self.t_final = int(self.events['t'][-1]) + self.t_offset\n","\n","    def get_final_time_us(self):\n","        return self.t_final\n","\n","    def get_events(self, t_start_us: int, t_end_us: int) -> Dict[str, np.ndarray]:\n","        \"\"\"Get events (p, x, y, t) within the specified time window\n","        Parameters\n","        ----------\n","        t_start_us: start time in microseconds\n","        t_end_us: end time in microseconds\n","        Returns\n","        -------\n","        events: dictionary of (p, x, y, t) or None if the time window cannot be retrieved\n","        \"\"\"\n","        assert t_start_us < t_end_us\n","\n","        # We assume that the times are top-off-day, hence subtract offset:\n","        t_start_us -= self.t_offset\n","        t_end_us -= self.t_offset\n","\n","        t_start_ms, t_end_ms = self.get_conservative_window_ms(\n","            t_start_us, t_end_us)\n","        t_start_ms_idx = self.ms2idx(t_start_ms)\n","        t_end_ms_idx = self.ms2idx(t_end_ms)\n","        if t_start_ms_idx is None or t_end_ms_idx is None:\n","            print('Error', 'start', t_start_us, 'end', t_end_us)\n","            # Cannot guarantee window size anymore\n","            return None\n","\n","        events = dict()\n","        time_array_conservative = np.asarray(\n","            self.events['t'][t_start_ms_idx:t_end_ms_idx])\n","        idx_start_offset, idx_end_offset = self.get_time_indices_offsets(\n","            time_array_conservative, t_start_us, t_end_us)\n","        t_start_us_idx = t_start_ms_idx + idx_start_offset\n","        t_end_us_idx = t_start_ms_idx + idx_end_offset\n","        # Again add t_offset to get gps time\n","        events['t'] = time_array_conservative[idx_start_offset:idx_end_offset] + self.t_offset\n","        for dset_str in ['p', 'x', 'y']:\n","            events[dset_str] = np.asarray(\n","                self.events[dset_str][t_start_us_idx:t_end_us_idx])\n","            assert events[dset_str].size == events['t'].size\n","        return events\n","\n","    @staticmethod\n","    def get_conservative_window_ms(ts_start_us: int, ts_end_us) -> Tuple[int, int]:\n","        \"\"\"Compute a conservative time window of time with millisecond resolution.\n","        We have a time to index mapping for each millisecond. Hence, we need\n","        to compute the lower and upper millisecond to retrieve events.\n","        Parameters\n","        ----------\n","        ts_start_us:    start time in microseconds\n","        ts_end_us:      end time in microseconds\n","        Returns\n","        -------\n","        window_start_ms:    conservative start time in milliseconds\n","        window_end_ms:      conservative end time in milliseconds\n","        \"\"\"\n","        assert ts_end_us > ts_start_us\n","        window_start_ms = math.floor(ts_start_us/1000)\n","        window_end_ms = math.ceil(ts_end_us/1000)\n","        return window_start_ms, window_end_ms\n","\n","    @staticmethod\n","    @jit(nopython=True)\n","    def get_time_indices_offsets(\n","            time_array: np.ndarray,\n","            time_start_us: int,\n","            time_end_us: int) -> Tuple[int, int]:\n","        \"\"\"Compute index offset of start and end timestamps in microseconds\n","        Parameters\n","        ----------\n","        time_array:     timestamps (in us) of the events\n","        time_start_us:  start timestamp (in us)\n","        time_end_us:    end timestamp (in us)\n","        Returns\n","        -------\n","        idx_start:  Index within this array corresponding to time_start_us\n","        idx_end:    Index within this array corresponding to time_end_us\n","        such that (in non-edge cases)\n","        time_array[idx_start] >= time_start_us\n","        time_array[idx_end] >= time_end_us\n","        time_array[idx_start - 1] < time_start_us\n","        time_array[idx_end - 1] < time_end_us\n","        this means that\n","        time_start_us <= time_array[idx_start:idx_end] < time_end_us\n","        \"\"\"\n","\n","        assert time_array.ndim == 1\n","\n","        idx_start = -1\n","        if time_array[-1] < time_start_us:\n","\n","            # Return same index twice: array[x:x] is empty.\n","            return time_array.size, time_array.size\n","        else:\n","            for idx_from_start in range(0, time_array.size, 1):\n","                if time_array[idx_from_start] >= time_start_us:\n","                    idx_start = idx_from_start\n","                    break\n","        assert idx_start >= 0\n","\n","        idx_end = time_array.size\n","        for idx_from_end in range(time_array.size - 1, -1, -1):\n","            if time_array[idx_from_end] >= time_end_us:\n","                idx_end = idx_from_end\n","            else:\n","                break\n","\n","        assert time_array[idx_start] >= time_start_us\n","        if idx_end < time_array.size:\n","            assert time_array[idx_end] >= time_end_us\n","        if idx_start > 0:\n","            assert time_array[idx_start - 1] < time_start_us\n","        if idx_end > 0:\n","            assert time_array[idx_end - 1] < time_end_us\n","        return idx_start, idx_end\n","\n","    def ms2idx(self, time_ms: int) -> int:\n","        assert time_ms >= 0\n","        if time_ms >= self.ms_to_idx.size:\n","            return None\n","        return self.ms_to_idx[time_ms]\n","\n","\n","class Sequence(Dataset):\n","    def __init__(self, seq_path: Path, representation_type: RepresentationType, mode: str = 'test', delta_t_ms: int = 100,\n","                 num_bins: int = 4, transforms=[], name_idx=0, visualize=False, load_gt=False):\n","        assert num_bins >= 1\n","        assert delta_t_ms == 100\n","        assert seq_path.is_dir(), seq_path\n","        assert mode in {'train', 'test'}\n","        assert representation_type is not None\n","        '''\n","        ディレクトリ構造:\n","\n","        data\n","        ├─test\n","        |  ├─seq_1\n","        |  |    ├─events_left\n","        |  |    |   ├─events.h5\n","        |  |    |   └─rectify_map.h5\n","        |  |    └─forward_timestamps.txt\n","        └─train\n","            ├─seq_1\n","            |    ├─events_left\n","            |    |       ├─ events.h5\n","            |    |       └─ rectify_map.h5\n","            |    ├─ flow_forward\n","            |    |       ├─ 000134.png\n","            |    |       |.....\n","            |    └─ forward_timestamps.txt\n","            ├─seq_2\n","            └─seq_3\n","        '''\n","        self.seq_name = PurePath(seq_path).name\n","        self.mode = mode\n","        self.name_idx = name_idx\n","        self.visualize_samples = visualize\n","        self.load_gt = load_gt\n","        self.transforms = transforms\n","        if self.mode == \"test\":\n","            assert load_gt == False\n","            # Get Test Timestamp File\n","            ev_dir_location = seq_path / 'events_left'\n","            timestamp_file = seq_path / 'forward_timestamps.txt'\n","            flow_path = seq_path / 'flow_forward'\n","            timestamps_flow = np.loadtxt(\n","                seq_path / 'forward_timestamps.txt', delimiter=',', dtype='int64')\n","            self.indices = np.arange(len(timestamps_flow))\n","            self.timestamps_flow = timestamps_flow[:, 0]\n","\n","        elif self.mode == \"train\":\n","            ev_dir_location = seq_path / 'events_left'\n","            flow_path = seq_path / 'flow_forward'\n","            timestamp_file = seq_path / 'forward_timestamps.txt'\n","            self.flow_png = [Path(os.path.join(flow_path, img)) for img in sorted(\n","                os.listdir(flow_path))]\n","            timestamps_flow = np.loadtxt(\n","                seq_path / 'forward_timestamps.txt', delimiter=',', dtype='int64')\n","            self.indices = np.arange(len(timestamps_flow))\n","            self.timestamps_flow = timestamps_flow[:, 0]\n","        else:\n","            pass\n","        assert timestamp_file.is_file()\n","\n","        file = np.genfromtxt(\n","            timestamp_file,\n","            delimiter=','\n","        )\n","\n","        self.idx_to_visualize = file[:, 2] if file.shape[1] == 3 else []\n","\n","        # Save output dimensions\n","        self.height = 480\n","        self.width = 640\n","        self.num_bins = num_bins\n","\n","\n","        # Set event representation\n","        self.voxel_grid = VoxelGrid(\n","                (self.num_bins, self.height, self.width), normalize=True)\n","        self.delta_t_us = delta_t_ms * 1000\n","\n","        # Left events only\n","        ev_data_file = ev_dir_location / 'events.h5'\n","        ev_rect_file = ev_dir_location / 'rectify_map.h5'\n","\n","        h5f_location = h5py.File(str(ev_data_file), 'r')\n","        self.h5f = h5f_location\n","        self.event_slicer = EventSlicer(h5f_location)\n","\n","        self.h5rect = h5py.File(str(ev_rect_file), 'r')\n","        self.rectify_ev_map = self.h5rect['rectify_map'][()]\n","\n","\n","    def events_to_voxel_grid(self, p, t, x, y, device: str = 'cpu'):\n","        t = (t - t[0]).astype('float32')\n","        t = (t/t[-1])\n","        x = x.astype('float32')\n","        y = y.astype('float32')\n","        pol = p.astype('float32')\n","        event_data_torch = {\n","            'p': torch.from_numpy(pol),\n","            't': torch.from_numpy(t),\n","            'x': torch.from_numpy(x),\n","            'y': torch.from_numpy(y),\n","        }\n","        return self.voxel_grid.convert(event_data_torch)\n","\n","    def getHeightAndWidth(self):\n","        return self.height, self.width\n","\n","    @staticmethod\n","    def get_disparity_map(filepath: Path):\n","        assert filepath.is_file()\n","        disp_16bit = cv2.imread(str(filepath), cv2.IMREAD_ANYDEPTH)\n","        return disp_16bit.astype('float32')/256\n","\n","    @staticmethod\n","    def load_flow(flowfile: Path):\n","        assert flowfile.exists()\n","        assert flowfile.suffix == '.png'\n","        flow_16bit = iio.imread(str(flowfile), plugin='PNG-FI')\n","        flow, valid2D = flow_16bit_to_float(flow_16bit)\n","        return flow, valid2D\n","\n","    @staticmethod\n","    def close_callback(h5f):\n","        h5f.close()\n","\n","    def get_image_width_height(self):\n","        return self.height, self.width\n","\n","    def __len__(self):\n","        return len(self.timestamps_flow)\n","\n","    def rectify_events(self, x: np.ndarray, y: np.ndarray):\n","        # assert location in self.locations\n","        # From distorted to undistorted\n","        rectify_map = self.rectify_ev_map\n","        assert rectify_map.shape == (\n","            self.height, self.width, 2), rectify_map.shape\n","        assert x.max() < self.width\n","        assert y.max() < self.height\n","        return rectify_map[y, x]\n","\n","    def get_data(self, index) -> Dict[str, any]:\n","        ts_start: int = self.timestamps_flow[index] - self.delta_t_us\n","        ts_end: int = self.timestamps_flow[index]\n","\n","        file_index = self.indices[index]\n","\n","        output = {\n","            'file_index': file_index,\n","            'timestamp': self.timestamps_flow[index],\n","            'seq_name': self.seq_name\n","        }\n","        # Save sample for benchmark submission\n","        output['save_submission'] = file_index in self.idx_to_visualize\n","        output['visualize'] = self.visualize_samples\n","        event_data = self.event_slicer.get_events(\n","            ts_start, ts_end)\n","        p = event_data['p']\n","        t = event_data['t']\n","        x = event_data['x']\n","        y = event_data['y']\n","\n","        xy_rect = self.rectify_events(x, y)\n","        x_rect = xy_rect[:, 0]\n","        y_rect = xy_rect[:, 1]\n","\n","        if self.voxel_grid is None:\n","            raise NotImplementedError\n","        else:\n","            event_representation = self.events_to_voxel_grid(\n","                p, t, x_rect, y_rect)\n","            output['event_volume'] = event_representation\n","        output['name_map'] = self.name_idx\n","\n","        if self.load_gt:\n","            output['flow_gt'\n","                ] = [torch.tensor(x) for x in self.load_flow(self.flow_png[index])]\n","\n","            output['flow_gt'\n","                ][0] = torch.moveaxis(output['flow_gt'][0], -1, 0)\n","            output['flow_gt'\n","                ][1] = torch.unsqueeze(output['flow_gt'][1], 0)\n","        return output\n","\n","    def __getitem__(self, idx):\n","        sample = self.get_data(idx)\n","        return sample\n","\n","    def get_voxel_grid(self, idx):\n","\n","        if idx == 0:\n","            event_data = self.event_slicer.get_events(\n","                self.timestamps_flow[0] - self.delta_t_us, self.timestamps_flow[0])\n","        elif idx > 0 and idx <= self.__len__():\n","            event_data = self.event_slicer.get_events(\n","                self.timestamps_flow[idx-1], self.timestamps_flow[idx-1] + self.delta_t_us)\n","        else:\n","            raise IndexError\n","\n","        p = event_data['p']\n","        t = event_data['t']\n","        x = event_data['x']\n","        y = event_data['y']\n","\n","        xy_rect = self.rectify_events(x, y)\n","        x_rect = xy_rect[:, 0]\n","        y_rect = xy_rect[:, 1]\n","        return self.events_to_voxel_grid(p, t, x_rect, y_rect)\n","\n","    def get_event_count_image(self, ts_start, ts_end, num_bins, normalize=True):\n","        assert ts_end > ts_start\n","        delta_t_bin = (ts_end - ts_start) / num_bins\n","        ts_start_bin = np.linspace(\n","            ts_start, ts_end, num=num_bins, endpoint=False)\n","        ts_end_bin = ts_start_bin + delta_t_bin\n","        assert abs(ts_end_bin[-1] - ts_end) < 10.\n","        ts_end_bin[-1] = ts_end\n","\n","        event_count = torch.zeros(\n","            (num_bins, self.height, self.width), dtype=torch.float, requires_grad=False)\n","\n","        for i in range(num_bins):\n","            event_data = self.event_slicer.get_events(\n","                ts_start_bin[i], ts_end_bin[i])\n","            p = event_data['p']\n","            t = event_data['t']\n","            x = event_data['x']\n","            y = event_data['y']\n","\n","            t = (t - t[0]).astype('float32')\n","            t = (t/t[-1])\n","            x = x.astype('float32')\n","            y = y.astype('float32')\n","            pol = p.astype('float32')\n","            event_data_torch = {\n","                'p': torch.from_numpy(pol),\n","                't': torch.from_numpy(t),\n","                'x': torch.from_numpy(x),\n","                'y': torch.from_numpy(y),\n","            }\n","            x = event_data_torch['x']\n","            y = event_data_torch['y']\n","            xy_rect = self.rectify_events(x.int(), y.int())\n","            x_rect = torch.from_numpy(xy_rect[:, 0]).long()\n","            y_rect = torch.from_numpy(xy_rect[:, 1]).long()\n","            value = 2*event_data_torch['p']-1\n","            index = self.width*y_rect + x_rect\n","            mask = (x_rect < self.width) & (y_rect < self.height)\n","            event_count[i].put_(index[mask], value[mask], accumulate=True)\n","\n","        return event_count\n","\n","    @staticmethod\n","    def normalize_tensor(event_count):\n","        mask = torch.nonzero(event_count, as_tuple=True)\n","        if mask[0].size()[0] > 0:\n","            mean = event_count[mask].mean()\n","            std = event_count[mask].std()\n","            if std > 0:\n","                event_count[mask] = (event_count[mask] - mean) / std\n","            else:\n","                event_count[mask] = event_count[mask] - mean\n","        return event_count\n","\n","\n","class SequenceRecurrent(Sequence):\n","    def __init__(self, seq_path: Path, representation_type: RepresentationType, mode: str = 'test', delta_t_ms: int = 100,\n","                 num_bins: int = 15, transforms=None, sequence_length=1, name_idx=0, visualize=False, load_gt=False):\n","        super(SequenceRecurrent, self).__init__(seq_path, representation_type, mode, delta_t_ms, transforms=transforms,\n","                                                name_idx=name_idx, visualize=visualize, load_gt=load_gt)\n","        self.crop_size = self.transforms['randomcrop'] if 'randomcrop' in self.transforms else None\n","        self.sequence_length = sequence_length\n","        self.valid_indices = self.get_continuous_sequences()\n","\n","    def get_continuous_sequences(self):\n","        continuous_seq_idcs = []\n","        if self.sequence_length > 1:\n","            for i in range(len(self.timestamps_flow)-self.sequence_length+1):\n","                diff = self.timestamps_flow[i +\n","                                            self.sequence_length-1] - self.timestamps_flow[i]\n","                if diff < np.max([100000 * (self.sequence_length-1) + 1000, 101000]):\n","                    continuous_seq_idcs.append(i)\n","        else:\n","            for i in range(len(self.timestamps_flow)-1):\n","                diff = self.timestamps_flow[i+1] - self.timestamps_flow[i]\n","                if diff < np.max([100000 * (self.sequence_length-1) + 1000, 101000]):\n","                    continuous_seq_idcs.append(i)\n","        return continuous_seq_idcs\n","\n","    def __len__(self):\n","        return len(self.valid_indices)\n","\n","    def __getitem__(self, idx):\n","        assert idx >= 0\n","        assert idx < len(self)\n","\n","        # Valid index is the actual index we want to load, which guarantees a continuous sequence length\n","        valid_idx = self.valid_indices[idx]\n","\n","        sequence = []\n","        j = valid_idx\n","\n","        ts_cur = self.timestamps_flow[j]\n","        # Add first sample\n","        sample = self.get_data_sample(j)\n","        sequence.append(sample)\n","\n","        # Data augmentation according to first sample\n","        crop_window = None\n","        flip = None\n","        if 'crop_window' in sample.keys():\n","            crop_window = sample['crop_window']\n","        if 'flipped' in sample.keys():\n","            flip = sample['flipped']\n","\n","        for i in range(self.sequence_length-1):\n","            j += 1\n","            ts_old = ts_cur\n","            ts_cur = self.timestamps_flow[j]\n","            assert(ts_cur-ts_old < 100000 + 1000)\n","            sample = self.get_data_sample(\n","                j, crop_window=crop_window, flip=flip)\n","            sequence.append(sample)\n","\n","        # Check if the current sample is the first sample of a continuous sequence\n","        if idx == 0 or self.valid_indices[idx]-self.valid_indices[idx-1] != 1:\n","            sequence[0]['new_sequence'] = 1\n","            print(\"Timestamp {} is the first one of the next seq!\".format(\n","                self.timestamps_flow[self.valid_indices[idx]]))\n","        else:\n","            sequence[0]['new_sequence'] = 0\n","\n","        # random crop\n","        if self.crop_size is not None:\n","            i, j, h, w = RandomCrop.get_params(\n","                sample[\"event_volume_old\"], output_size=self.crop_size)\n","            keys_to_crop = [\"event_volume_old\", \"event_volume_new\",\n","                            \"flow_gt_event_volume_old\", \"flow_gt_event_volume_new\",\n","                            \"flow_gt_next\",]\n","\n","            for sample in sequence:\n","                for key, value in sample.items():\n","                    if key in keys_to_crop:\n","                        if isinstance(value, torch.Tensor):\n","                            sample[key] = tf.functional.crop(value, i, j, h, w)\n","                        elif isinstance(value, list) or isinstance(value, tuple):\n","                            sample[key] = [tf.functional.crop(\n","                                v, i, j, h, w) for v in value]\n","        return sequence\n","\n","\n","class DatasetProvider:\n","    def __init__(self, dataset_path: Path, representation_type: RepresentationType, delta_t_ms: int = 100, num_bins=4,\n","                config=None, visualize=False):\n","        test_path = Path(os.path.join(dataset_path, 'test'))\n","        train_path = Path(os.path.join(dataset_path, 'train'))\n","        assert dataset_path.is_dir(), str(dataset_path)\n","        assert test_path.is_dir(), str(test_path)\n","        assert delta_t_ms == 100\n","        self.config = config\n","        self.name_mapper_test = []\n","\n","        # Assemble test sequences\n","        test_sequences = list()\n","        for child in test_path.iterdir():\n","            self.name_mapper_test.append(str(child).split(\"/\")[-1])\n","            test_sequences.append(Sequence(child, representation_type, 'test', delta_t_ms, num_bins,\n","                                               transforms=[],\n","                                               name_idx=len(\n","                                                   self.name_mapper_test)-1,\n","                                               visualize=visualize))\n","\n","        self.test_dataset = torch.utils.data.ConcatDataset(test_sequences)\n","\n","        # Assemble train sequences\n","        available_seqs = os.listdir(train_path)\n","\n","        seqs = available_seqs\n","\n","        train_sequences: list[Sequence] = []\n","        for seq in seqs:\n","            extra_arg = dict()\n","            train_sequences.append(Sequence(Path(train_path) / seq,\n","                                   representation_type=representation_type, mode=\"train\",\n","                                   load_gt=True, **extra_arg))\n","            self.train_dataset: torch.utils.data.ConcatDataset[Sequence] = torch.utils.data.ConcatDataset(train_sequences)\n","\n","    def get_test_dataset(self):\n","        return self.test_dataset\n","\n","    def get_train_dataset(self):\n","        return self.train_dataset\n","\n","    def get_name_mapping_test(self):\n","        return self.name_mapper_test\n","\n","    def summary(self, logger):\n","        logger.write_line(\n","            \"================================== Dataloader Summary ====================================\", True)\n","        logger.write_line(\"Loader Type:\\t\\t\" + self.__class__.__name__, True)\n","        logger.write_line(\"Number of Voxel Bins: {}\".format(\n","            self.test_dataset.datasets[0].num_bins), True)\n","        logger.write_line(\"Number of Train Sequences: {}\".format(\n","            len(self.train_dataset)), True)\n","\n","def train_collate(sample_list):\n","    batch = dict()\n","    for field_name in sample_list[0]:\n","        if field_name == 'timestamp':\n","            batch['timestamp'] = [sample[field_name] for sample in sample_list]\n","        if field_name == 'seq_name':\n","            batch['seq_name'] = [sample[field_name] for sample in sample_list]\n","        if field_name == 'new_sequence':\n","            batch['new_sequence'] = [sample[field_name]\n","                                     for sample in sample_list]\n","        if field_name.startswith(\"event_volume\"):\n","            batch[field_name] = torch.stack(\n","                [sample[field_name] for sample in sample_list])\n","        if field_name.startswith(\"flow_gt\"):\n","            if all(field_name in x for x in sample_list):\n","                batch[field_name] = torch.stack(\n","                    [sample[field_name][0] for sample in sample_list])\n","                batch[field_name + '_valid_mask'] = torch.stack(\n","                    [sample[field_name][1] for sample in sample_list])\n","\n","    return batch\n","\n","\n","def rec_train_collate(sample_list):\n","    seq_length = len(sample_list[0])\n","    seq_of_batch = []\n","    for i in range(seq_length):\n","        seq_of_batch.append(train_collate(\n","            [sample[i] for sample in sample_list]))\n","    return seq_of_batch\n"],"metadata":{"id":"I5OhuTAKoZLN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# `base.py`"],"metadata":{"id":"ae2Eh6v8pOHD"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","\n","class build_resnet_block(nn.Module):\n","    \"\"\"\n","    a resnet block which includes two general_conv2d\n","    \"\"\"\n","    def __init__(self, channels, layers=2, do_batch_norm=False):\n","        super(build_resnet_block,self).__init__()\n","        self._channels = channels\n","        self._layers = layers\n","\n","        self.res_block = nn.Sequential(*[general_conv2d(in_channels=self._channels,\n","                                             out_channels=self._channels,\n","                                             strides=1,\n","                                             do_batch_norm=do_batch_norm) for i in range(self._layers)])\n","\n","    def forward(self,input_res):\n","        inputs = input_res.clone()\n","        input_res = self.res_block(input_res)\n","        return input_res + inputs\n","\n","class upsample_conv2d_and_predict_flow(nn.Module):\n","    \"\"\"\n","    an upsample convolution layer which includes a nearest interpolate and a general_conv2d\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, ksize=3, do_batch_norm=False):\n","        super(upsample_conv2d_and_predict_flow, self).__init__()\n","        self._in_channels = in_channels\n","        self._out_channels = out_channels\n","        self._ksize = ksize\n","        self._do_batch_norm = do_batch_norm\n","\n","        self.general_conv2d = general_conv2d(\n","            in_channels=self._in_channels,\n","            out_channels=self._out_channels,\n","            ksize=self._ksize,\n","            strides=1,\n","            do_batch_norm=self._do_batch_norm,\n","            padding=0\n","        )\n","\n","        self.pad = nn.ReflectionPad2d(\n","            padding=(\n","                int((self._ksize - 1) / 2),\n","                int((self._ksize - 1) / 2),\n","                int((self._ksize - 1) / 2),\n","                int((self._ksize - 1) / 2)\n","            )\n","        )\n","\n","        self.predict_flow = general_conv2d(\n","            in_channels=self._out_channels,\n","            out_channels=2,\n","            ksize=1,\n","            strides=1,\n","            padding=0,\n","            activation='tanh'\n","        )\n","\n","    def forward(self, conv):\n","        shape = conv.shape\n","        conv = nn.functional.interpolate(conv,size=[shape[2]*2,shape[3]*2],mode='nearest')\n","        conv = self.pad(conv)\n","        conv = self.general_conv2d(conv)\n","\n","        flow = self.predict_flow(conv) * 256.\n","\n","        return torch.cat([conv,flow.clone()], dim=1), flow\n","\n","def general_conv2d(in_channels,out_channels, ksize=3, strides=2, padding=1, do_batch_norm=False, activation='relu'):\n","    \"\"\"\n","    a general convolution layer which includes a conv2d, a relu and a batch_normalize\n","    \"\"\"\n","    if activation == 'relu':\n","        if do_batch_norm:\n","            conv2d = nn.Sequential(\n","                nn.Conv2d(in_channels = in_channels,out_channels = out_channels,kernel_size = ksize,\n","                        stride=strides,padding=padding),\n","                nn.ReLU(inplace=True),\n","                nn.BatchNorm2d(out_channels,eps=1e-5,momentum=0.99)\n","            )\n","        else:\n","            conv2d = nn.Sequential(\n","                nn.Conv2d(in_channels = in_channels,out_channels = out_channels,kernel_size = ksize,\n","                        stride=strides,padding=padding),\n","                nn.ReLU(inplace=True)\n","            )\n","    elif activation == 'tanh':\n","        if do_batch_norm:\n","            conv2d = nn.Sequential(\n","                nn.Conv2d(in_channels = in_channels,out_channels = out_channels,kernel_size = ksize,\n","                        stride=strides,padding=padding),\n","                nn.Tanh(),\n","                nn.BatchNorm2d(out_channels,eps=1e-5,momentum=0.99)\n","            )\n","        else:\n","            conv2d = nn.Sequential(\n","                nn.Conv2d(in_channels = in_channels,out_channels = out_channels,kernel_size = ksize,\n","                        stride=strides,padding=padding),\n","                nn.Tanh()\n","            )\n","    return conv2d"],"metadata":{"id":"VdJp3wsWpPYr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# `evflownet.py`"],"metadata":{"id":"fiL_SR0Vo_X9"}},{"cell_type":"code","source":["_BASE_CHANNELS = 64\n","\n","class EVFlowNet(nn.Module):\n","    def __init__(self, args):\n","        super(EVFlowNet,self).__init__()\n","        self._args = args\n","\n","        self.encoder1 = general_conv2d(\n","            in_channels=4,\n","            out_channels=_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm\n","        )\n","        self.encoder2 = general_conv2d(\n","            in_channels=_BASE_CHANNELS,\n","            out_channels=2*_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm\n","        )\n","        self.encoder3 = general_conv2d(\n","            in_channels=2*_BASE_CHANNELS,\n","            out_channels=4*_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm\n","        )\n","        self.encoder4 = general_conv2d(\n","            in_channels=4*_BASE_CHANNELS,\n","            out_channels=8*_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm\n","        )\n","\n","        self.resnet_block = nn.Sequential(*[build_resnet_block(8*_BASE_CHANNELS, do_batch_norm=not self._args.no_batch_norm) for i in range(2)])\n","\n","        self.decoder1 = upsample_conv2d_and_predict_flow(\n","            in_channels=16*_BASE_CHANNELS,\n","            out_channels=4*_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm\n","        )\n","        self.decoder2 = upsample_conv2d_and_predict_flow(\n","            in_channels=8*_BASE_CHANNELS+2,\n","            out_channels=2*_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm\n","        )\n","        self.decoder3 = upsample_conv2d_and_predict_flow(\n","            in_channels=4*_BASE_CHANNELS+2,\n","            out_channels=_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm\n","        )\n","        self.decoder4 = upsample_conv2d_and_predict_flow(\n","            in_channels=2*_BASE_CHANNELS+2,\n","            out_channels=int(_BASE_CHANNELS/2),\n","            do_batch_norm=not self._args.no_batch_norm\n","        )\n","\n","    def forward(self, inputs: torch.Tensor) -> Dict[str, torch.Tensor]:\n","        # encoder\n","        skip_connections = {}\n","        inputs = self.encoder1(inputs)\n","        skip_connections['skip0'] = inputs.clone()\n","        inputs = self.encoder2(inputs)\n","        skip_connections['skip1'] = inputs.clone()\n","        inputs = self.encoder3(inputs)\n","        skip_connections['skip2'] = inputs.clone()\n","        inputs = self.encoder4(inputs)\n","        skip_connections['skip3'] = inputs.clone()\n","\n","        # transition\n","        inputs = self.resnet_block(inputs)\n","\n","        # decoder\n","        flow_dict = {}\n","        inputs = torch.cat([inputs, skip_connections['skip3']], dim=1)\n","        inputs, flow = self.decoder1(inputs)\n","        flow_dict['flow0'] = flow.clone()\n","        print(flow.shape)\n","\n","        inputs = torch.cat([inputs, skip_connections['skip2']], dim=1)\n","        inputs, flow = self.decoder2(inputs)\n","        flow_dict['flow1'] = flow.clone()\n","        print(flow.shape)\n","\n","        inputs = torch.cat([inputs, skip_connections['skip1']], dim=1)\n","        inputs, flow = self.decoder3(inputs)\n","        flow_dict['flow2'] = flow.clone()\n","        print(flow.shape)\n","\n","        inputs = torch.cat([inputs, skip_connections['skip0']], dim=1)\n","        inputs, flow = self.decoder4(inputs)\n","        flow_dict['flow3'] = flow.clone()\n","        print(flow.shape)\n","\n","        # return flow_dict\n","        return flow_dict"],"metadata":{"id":"jS2nzOiqpD6i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    # from config import configs\n","    import time\n","    # from data_loader import EventData\n","\n","    args = cfg\n","\n","    # args = {\n","    #     'dataset_path': 'data',\n","    #     'seed': 42,\n","    #     'num_epoch': 100,\n","    #     'data_loader': {\n","    #         'common': {\n","    #             'num_voxel_bins': 15\n","    #         },\n","    #         'train': {\n","    #             'batch_size': 16,\n","    #             'shuffle': False\n","    #         },\n","    #         'test': {\n","    #             'batch_size': 1,\n","    #             'shuffle': False\n","    #         }\n","    #     },\n","    #     'train': {\n","    #         'no_batch_norm': False,\n","    #         'initial_learning_rate': 0.01,\n","    #         'weight_decay': 0.0001,\n","    #         'epochs': 10,\n","    #     }\n","    # }\n","\n","    model = EVFlowNet(args.train)# .cuda()\n","    input_ = torch.rand(8,4,256,256)# .cuda()\n","    a = time.time()\n","    output = model(input_)\n","    b = time.time()\n","    print(b-a)\n","    print(output['flow0'].shape, output['flow1'].shape, output['flow2'].shape, output['flow3'].shape)\n","    #print(model.state_dict().keys())\n","    #print(model)\n","    # import numpy as np\n","    # # args = configs()\n","    # model = EVFlowNet(args) #.cuda()\n","    # EventDataset = EventData(args['data_path'], 'train')\n","    # EventDataLoader = torch.utils.data.DataLoader(dataset=EventDataset, batch_size=args['batch_size'], shuffle=True)\n","    # #model = nn.DataParallel(model)\n","    # #model.load_state_dict(torch.load(args.load_path+'/model18'))\n","    # for input_, _, _, _ in EventDataLoader:\n","    #     input_ = input_.cuda()\n","    #     a = time.time()\n","    #     (model(input_))\n","    #     b = time.time()\n","    #     print(b-a)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ez0-ElUox5mN","executionInfo":{"status":"ok","timestamp":1720089103999,"user_tz":-540,"elapsed":8019,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}},"outputId":"7a2a199f-326c-4460-9b69-2edcfc0f7b50"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 2, 32, 32])\n","torch.Size([8, 2, 64, 64])\n","torch.Size([8, 2, 128, 128])\n","torch.Size([8, 2, 256, 256])\n","7.797534227371216\n","torch.Size([8, 2, 32, 32]) torch.Size([8, 2, 64, 64]) torch.Size([8, 2, 128, 128]) torch.Size([8, 2, 256, 256])\n"]}]},{"cell_type":"markdown","source":["**Ideal size:**\n","```\n","torch.Size([8, 2, 32, 32]) torch.Size([8, 2, 64, 64]) torch.Size([8, 2, 128, 128]) torch.Size([8, 2, 256, 256])\n","```"],"metadata":{"id":"BbWTJ6HIDi2x"}},{"cell_type":"markdown","source":["# ViTFlowNet"],"metadata":{"id":"iV-THyrcrtFf"}},{"cell_type":"code","source":["from torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights\n","from torchvision.transforms import Resize"],"metadata":{"id":"bzzVX_3lrym-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Autoencoder(nn.Module):\n","    def __init__(self):\n","        super(Autoencoder, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(4, 3, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU()\n","        )\n","        self.decoder = nn.Sequential(\n","            nn.Conv2d(3, 4, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU()\n","        )\n","\n","    def forward(self, x):\n","        encoded = self.encoder(x)\n","        decoded = self.decoder(encoded)\n","        return encoded, decoded"],"metadata":{"id":"3xvx4LSW7OX4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ViTFlowNet(nn.Module):\n","    def __init__(self, pretrained=True, do_batch_norm=False):\n","        super(ViTFlowNet, self).__init__()\n","\n","        self.autoencoder = Autoencoder()\n","        self.resize = Resize((224, 224)) # Image size that ViT was trained on\n","\n","        if pretrained:\n","            weights = ViT_B_16_Weights.IMAGENET1K_V1\n","        else:\n","            weights = None\n","\n","        self.vit = vit_b_16(weights=weights)\n","        self.vit.heads = nn.Identity()\n","\n","        self.projector = nn.Conv2d(768, 768, kernel_size=1)\n","\n","        self.decoder1 = upsample_conv2d_and_predict_flow(in_channels=768, out_channels=512, do_batch_norm=do_batch_norm)\n","        self.decoder2 = upsample_conv2d_and_predict_flow(in_channels=512 + 2, out_channels=256, do_batch_norm=do_batch_norm)\n","        self.decoder3 = upsample_conv2d_and_predict_flow(in_channels=256 + 2, out_channels=128, do_batch_norm=do_batch_norm)\n","        self.decoder4 = upsample_conv2d_and_predict_flow(in_channels=128 + 2, out_channels=2, do_batch_norm=do_batch_norm)\n","\n","    def forward(self, x):\n","        encoded, decoded = self.autoencoder(x)\n","        resized = self.resize(encoded)  # Resize to (224, 224) for ViT\n","\n","        vit_output = self.vit(resized)  # Output shape: [B, 768]\n","\n","        # Reshape to spatial dimensions (assuming ViT processes 16x16 patches and outputs 768 features per patch)\n","        batch_size = vit_output.size(0)\n","        vit_output = vit_output.reshape(batch_size, 768, 1, 1)  # Reshape to [B, 768, 14, 14]\n","\n","        # Project to maintain spatial dimensions\n","        # vit_output = self.projector(vit_output)  # Project flattened ViT output to spatial dimensions\n","\n","        # Pass through decoders\n","        x, flow0 = self.decoder1(vit_output)\n","        x, flow1 = self.decoder2(x)\n","        x, flow2 = self.decoder3(x)\n","        x, flow3 = self.decoder4(x)\n","\n","        # Interpolate flow3 to the desired final shape (480, 640)\n","        flow3 = nn.functional.interpolate(flow3, size=(480, 640), mode='bilinear', align_corners=False)\n","\n","        # return {'flow0': flow0, 'flow1': flow1, 'flow2': flow2, 'flow3': flow3}\n","        return flow3"],"metadata":{"id":"y_0EtLsJrz5J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class upsample_conv2d_and_predict_flow(nn.Module):\n","    def __init__(self, in_channels, out_channels, ksize=3, do_batch_norm=False):\n","        super(upsample_conv2d_and_predict_flow, self).__init__()\n","        self.general_conv2d = general_conv2d(in_channels=in_channels, out_channels=out_channels, ksize=ksize, strides=1, do_batch_norm=do_batch_norm, padding=0)\n","        self.pad = nn.ReflectionPad2d(padding=(ksize//2, ksize//2, ksize//2, ksize//2))\n","        self.predict_flow = general_conv2d(in_channels=out_channels, out_channels=2, ksize=1, strides=1, padding=0, activation='tanh')\n","\n","    def forward(self, x):\n","        shape = x.shape\n","        x = nn.functional.interpolate(x, size=(shape[2]*2, shape[3]*2), mode='nearest')\n","        x = self.pad(x)\n","        x = self.general_conv2d(x)\n","        flow = self.predict_flow(x) * 256.0\n","        return torch.cat([x, flow.clone()], dim=1), flow\n","\n","def general_conv2d(in_channels, out_channels, ksize=3, strides=2, padding=1, do_batch_norm=False, activation='relu'):\n","    layers = [\n","        nn.Conv2d(in_channels, out_channels, kernel_size=ksize, stride=strides, padding=padding),\n","    ]\n","    if activation == 'relu':\n","        layers.append(nn.ReLU(inplace=True))\n","    elif activation == 'tanh':\n","        layers.append(nn.Tanh())\n","    if do_batch_norm:\n","        layers.append(nn.BatchNorm2d(out_channels, eps=1e-5, momentum=0.99))\n","    return nn.Sequential(*layers)"],"metadata":{"id":"tKEIIFudGGsp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Other Stuff"],"metadata":{"id":"ZMHKr3VBp5z-"}},{"cell_type":"code","source":["# cosine scheduler\n","class CosineScheduler:\n","    def __init__(self, epochs, lr, warmup_length=5):\n","        \"\"\"\n","        Arguments\n","        ---------\n","        epochs : int\n","            学習のエポック数．\n","        lr : float\n","            学習率．\n","        warmup_length : int\n","            warmupを適用するエポック数．\n","        \"\"\"\n","        self.epochs = epochs\n","        self.lr = lr\n","        self.warmup = warmup_length\n","\n","    def __call__(self, epoch):\n","        \"\"\"\n","        Arguments\n","        ---------\n","        epoch : int\n","            現在のエポック数．\n","        \"\"\"\n","        progress = (epoch - self.warmup) / (self.epochs - self.warmup)\n","        progress = np.clip(progress, 0.0, 1.0)\n","        lr = self.lr * 0.5 * (1. + np.cos(np.pi * progress))\n","\n","        if self.warmup:\n","            lr = lr * min(1., (epoch+1) / self.warmup)\n","\n","        return lr"],"metadata":{"id":"xwaG26URz349"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# `main.py`"],"metadata":{"id":"RWLID2kFoXDW"}},{"cell_type":"code","source":["def set_lr(lr, optimizer):\n","    for param_group in optimizer.param_groups:\n","        param_group[\"lr\"] = lr"],"metadata":{"id":"1zKUixlJ0N71"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RepresentationType(Enum):\n","    VOXEL = auto()\n","    STEPAN = auto()\n","\n","def set_seed(seed):\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","\n","def compute_epe_error(pred_flow: torch.Tensor, gt_flow: torch.Tensor):\n","    '''\n","    end-point-error (ground truthと予測値の二乗誤差)を計算\n","    pred_flow: torch.Tensor, Shape: torch.Size([B, 2, 480, 640]) => 予測したオプティカルフローデータ\n","    gt_flow: torch.Tensor, Shape: torch.Size([B, 2, 480, 640]) => 正解のオプティカルフローデータ\n","    '''\n","    epe = torch.mean(torch.mean(torch.norm(pred_flow - gt_flow, p=2, dim=1), dim=(1, 2)), dim=0)\n","    return epe\n","\n","def compute_multiflow_epe_error(preds: Dict[str, torch.Tensor], target: Dict[str, torch.Tensor]):\n","    loss = 0\n","    for flow in ['flow0', 'flow1', 'flow2', 'flow3']:\n","        loss += torch.square(compute_epe_error(preds[flow], target[flow]))\n","    return torch.sqrt(loss)\n","\n","def save_optical_flow_to_npy(flow: torch.Tensor, file_name: str):\n","    '''\n","    optical flowをnpyファイルに保存\n","    flow: torch.Tensor, Shape: torch.Size([2, 480, 640]) => オプティカルフローデータ\n","    file_name: str => ファイル名\n","    '''\n","    np.save(f\"{file_name}.npy\", flow.cpu().numpy())"],"metadata":{"id":"IeQg3UMNoFS5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@hydra.main(version_base=None, config_path=\"configs\", config_name=\"base\")\n","def main(args: DictConfig):\n","    set_seed(args.seed)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    '''\n","        ディレクトリ構造:\n","\n","        data\n","        ├─test\n","        |  ├─test_city\n","        |  |    ├─events_left\n","        |  |    |   ├─events.h5\n","        |  |    |   └─rectify_map.h5\n","        |  |    └─forward_timestamps.txt\n","        └─train\n","            ├─zurich_city_11_a\n","            |    ├─events_left\n","            |    |       ├─ events.h5\n","            |    |       └─ rectify_map.h5\n","            |    ├─ flow_forward\n","            |    |       ├─ 000134.png\n","            |    |       |.....\n","            |    └─ forward_timestamps.txt\n","            ├─zurich_city_11_b\n","            └─zurich_city_11_c\n","        '''\n","\n","    # ------------------\n","    #    Dataloader\n","    # ------------------\n","    loader = DatasetProvider(\n","        dataset_path=Path(args.dataset_path),\n","        representation_type=RepresentationType.VOXEL,\n","        delta_t_ms=100,\n","        num_bins=4\n","    )\n","    train_set = loader.get_train_dataset()\n","    test_set = loader.get_test_dataset()\n","    collate_fn = train_collate\n","    train_data = DataLoader(train_set,\n","                                 batch_size=args.data_loader.train.batch_size,\n","                                 shuffle=args.data_loader.train.shuffle,\n","                                 collate_fn=collate_fn,\n","                                 drop_last=False)\n","    test_data = DataLoader(test_set,\n","                                 batch_size=args.data_loader.test.batch_size,\n","                                 shuffle=args.data_loader.test.shuffle,\n","                                 collate_fn=collate_fn,\n","                                 drop_last=False)\n","\n","    '''\n","    train data:\n","        Type of batch: Dict\n","        Key: seq_name, Type: list\n","        Key: event_volume, Type: torch.Tensor, Shape: torch.Size([Batch, 4, 480, 640]) => イベントデータのバッチ\n","        Key: flow_gt, Type: torch.Tensor, Shape: torch.Size([Batch, 2, 480, 640]) => オプティカルフローデータのバッチ\n","        Key: flow_gt_valid_mask, Type: torch.Tensor, Shape: torch.Size([Batch, 1, 480, 640]) => オプティカルフローデータのvalid. ベースラインでは使わない\n","\n","    test data:\n","        Type of batch: Dict\n","        Key: seq_name, Type: list\n","        Key: event_volume, Type: torch.Tensor, Shape: torch.Size([Batch, 4, 480, 640]) => イベントデータのバッチ\n","    '''\n","    # ------------------\n","    #       Model\n","    # ------------------\n","    model = EVFlowNet(args.train).to(device)\n","\n","\n","    # ------------------\n","    #   optimizer\n","    # ------------------\n","    optimizer = torch.optim.Adam(model.parameters(), lr=args.train.initial_learning_rate, weight_decay=args.train.weight_decay)\n","    scheduler = CosineScheduler(args.train.epochs, 0.0024, warmup_length=200)\n","\n","    # ------------------\n","    #   Start training\n","    # ------------------\n","    model.train()\n","    for epoch in range(args.train.epochs):\n","        new_lr = scheduler(epoch)\n","        set_lr(new_lr, optimizer)\n","\n","        total_loss = 0\n","        print(\"on epoch: {}\".format(epoch+1))\n","        for i, batch in enumerate(tqdm(train_data)):\n","            batch: Dict[str, Any]\n","            event_image = batch[\"event_volume\"].to(device) # [B, 4, 480, 640]\n","            ground_truth_flow = batch[\"flow_gt\"].to(device) # [B, 2, 480, 640]\n","            print(ground_truth_flow.shape)\n","            flow = model(event_image) # [B, 2, 480, 640]\n","            loss: torch.Tensor = compute_epe_error(flow, ground_truth_flow)\n","            print(f\"batch {i} loss: {loss.item()}\")\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            break\n","\n","            total_loss += loss.item()\n","        print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_data)}')\n","\n","        break\n","\n","    # # Create the directory if it doesn't exist\n","    # if not os.path.exists('checkpoints'):\n","    #     os.makedirs('checkpoints')\n","\n","    # current_time = time.strftime(\"%Y%m%d%H%M%S\")\n","    # model_path = f\"checkpoints/model_{current_time}.pth\"\n","    # torch.save(model.state_dict(), model_path)\n","    # print(f\"Model saved to {model_path}\")\n","\n","    # # ------------------\n","    # #   Start predicting\n","    # # ------------------\n","    # model.load_state_dict(torch.load(model_path, map_location=device))\n","    # model.eval()\n","    # flow: torch.Tensor = torch.tensor([]).to(device)\n","    # with torch.no_grad():\n","    #     print(\"start test\")\n","    #     for batch in tqdm(test_data):\n","    #         batch: Dict[str, Any]\n","    #         event_image = batch[\"event_volume_old\"].to(device)\n","    #         batch_flow = model(event_image) # [1, 2, 480, 640]\n","    #         flow = torch.cat((flow, batch_flow), dim=0)  # [N, 2, 480, 640]\n","    #     print(\"test done\")\n","    # # ------------------\n","    # #  save submission\n","    # # ------------------\n","    # file_name = \"submission.npy\"\n","    # save_optical_flow_to_npy(flow, file_name)"],"metadata":{"id":"W9VrcQ-7kiWL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    main(cfg)"],"metadata":{"id":"wDc86lShsTdn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fbdaf3cb-4f3a-4868-b99e-6889d4bebf02"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["data/test/seq_1/events_left/events.h5 ['events', 'ms_to_idx', 't_offset']\n","events/p\n","events/x\n","events/y\n","events/t\n","data/train/seq_3/events_left/events.h5 ['events', 'ms_to_idx', 't_offset']\n","events/p\n","events/x\n","events/y\n","events/t\n","data/train/seq_1/events_left/events.h5 ['events', 'ms_to_idx', 't_offset']\n","events/p\n","events/x\n","events/y\n","events/t\n","data/train/seq_2/events_left/events.h5 ['events', 'ms_to_idx', 't_offset']\n","events/p\n","events/x\n","events/y\n","events/t\n","on epoch: 1\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/126 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([16, 2, 480, 640])\n"]}]}]}