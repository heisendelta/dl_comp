{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8921598,"sourceType":"datasetVersion","datasetId":5366031},{"sourceId":8936349,"sourceType":"datasetVersion","datasetId":5376452}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[],"collapsed_sections":["tpYel0PeGb7O","iNELPnjTGb7Q"]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","project_dir = '/content/drive/My Drive/Colab Notebooks/matsuoo/dl/event_camera_repo'\n","%cd {project_dir}"],"metadata":{"id":"QTx90vTeGl3_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721099591718,"user_tz":-540,"elapsed":1958,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}},"outputId":"eb4810ba-d797-4c84-dc92-a2c4d97a7788"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/Colab Notebooks/matsuoo/dl/event_camera_repo\n"]}]},{"cell_type":"code","source":["!pip install hydra-core omegaconf hdf5plugin h5py numba imageio imageio-ffmpeg tqdm torchvision --quiet"],"metadata":{"trusted":true,"id":"fdfquW_iGb7K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import hydra\n","from omegaconf import DictConfig\n","from torch.utils.data import DataLoader\n","import random\n","import numpy as np\n","from enum import Enum, auto\n","from tqdm import tqdm\n","from pathlib import Path\n","from typing import Dict, Any\n","import os\n","import time\n","\n","import math\n","from pathlib import PurePath\n","from typing import Tuple\n","import cv2\n","import hdf5plugin\n","import h5py\n","from numba import jit\n","import imageio\n","imageio.plugins.freeimage.download()\n","import imageio.v3 as iio\n","from torchvision.transforms import RandomCrop\n","from torchvision import transforms as tf\n","from torch.utils.checkpoint import checkpoint\n","from torch.utils.data import Dataset\n","import torchvision.transforms.functional as F\n","\n","from torch import nn"],"metadata":{"trusted":true,"id":"tAW9ff9RGb7M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install einops\n","from einops.layers.torch import Rearrange\n","from einops import rearrange"],"metadata":{"trusted":true,"id":"EjOf_ZjXGb7N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721099635130,"user_tz":-540,"elapsed":15303,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}},"outputId":"f9f591f7-43a4-428d-e45f-8e862de8cc79"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n"]}]},{"cell_type":"code","source":["from omegaconf import OmegaConf\n","from hydra import compose, initialize\n","\n","cfg_string = '''\n","dataset_path: data\n","seed: 42\n","num_epoch: 100\n","data_loader:\n","    common:\n","        num_voxel_bins: 15\n","    train:\n","        batch_size: 16\n","        shuffle: false\n","    test:\n","        batch_size: 1\n","        shuffle: false\n","train:\n","    no_batch_norm: false\n","    initial_learning_rate: 0.01\n","    learning_rate_decay: 0.9\n","    weight_decay: 0.0001\n","    epochs: 10\n","'''\n","cfg = OmegaConf.create(cfg_string)"],"metadata":{"trusted":true,"id":"yfNzEvaIGb7O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## `utils.py`"],"metadata":{"id":"tpYel0PeGb7O"}},{"cell_type":"code","source":["def set_seed(seed: int = 0) -> None:\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","class RepresentationType(Enum):\n","    VOXEL = auto()\n","    STEPAN = auto()\n","\n","\n","class EventRepresentation:\n","    def __init__(self):\n","        pass\n","\n","    def convert(self, events):\n","        raise NotImplementedError\n","\n","\n","class VoxelGrid(EventRepresentation):\n","    def __init__(self, input_size: tuple, normalize: bool):\n","        assert len(input_size) == 3\n","        self.voxel_grid = torch.zeros(\n","            (input_size), dtype=torch.float, requires_grad=False)\n","        self.nb_channels = input_size[0]\n","        self.normalize = normalize\n","\n","    def convert(self, events):\n","        C, H, W = self.voxel_grid.shape\n","        with torch.no_grad():\n","            self.voxel_grid = self.voxel_grid.to(events['p'].device)\n","            voxel_grid = self.voxel_grid.clone()\n","\n","            t_norm = events['t']\n","            t_norm = (C - 1) * (t_norm-t_norm[0]) / (t_norm[-1]-t_norm[0])\n","\n","            x0 = events['x'].int()\n","            y0 = events['y'].int()\n","            t0 = t_norm.int()\n","\n","            value = 2*events['p']-1\n","            #start_t = time()\n","            for xlim in [x0, x0+1]:\n","                for ylim in [y0, y0+1]:\n","                    for tlim in [t0, t0+1]:\n","\n","                        mask = (xlim < W) & (xlim >= 0) & (ylim < H) & (\n","                            ylim >= 0) & (tlim >= 0) & (tlim < self.nb_channels)\n","                        interp_weights = value * (1 - (xlim-events['x']).abs()) * (\n","                            1 - (ylim-events['y']).abs()) * (1 - (tlim - t_norm).abs())\n","                        index = H * W * tlim.long() + \\\n","                            W * ylim.long() + \\\n","                            xlim.long()\n","\n","                        voxel_grid.put_(\n","                            index[mask], interp_weights[mask], accumulate=True)\n","\n","            if self.normalize:\n","                mask = torch.nonzero(voxel_grid, as_tuple=True)\n","                if mask[0].size()[0] > 0:\n","                    mean = voxel_grid[mask].mean()\n","                    std = voxel_grid[mask].std()\n","                    if std > 0:\n","                        voxel_grid[mask] = (voxel_grid[mask] - mean) / std\n","                    else:\n","                        voxel_grid[mask] = voxel_grid[mask] - mean\n","\n","        return voxel_grid\n","\n","\n","class PolarityCount(EventRepresentation):\n","    def __init__(self, input_size: tuple):\n","        assert len(input_size) == 3\n","        self.voxel_grid = torch.zeros(\n","            (input_size), dtype=torch.float, requires_grad=False)\n","        self.nb_channels = input_size[0]\n","\n","    def convert(self, events):\n","        C, H, W = self.voxel_grid.shape\n","        with torch.no_grad():\n","            self.voxel_grid = self.voxel_grid.to(events['p'].device)\n","            voxel_grid = self.voxel_grid.clone()\n","\n","            x0 = events['x'].int()\n","            y0 = events['y'].int()\n","\n","            #start_t = time()\n","            for xlim in [x0, x0+1]:\n","                for ylim in [y0, y0+1]:\n","                    mask = (xlim < W) & (xlim >= 0) & (ylim < H) & (\n","                        ylim >= 0)\n","                    interp_weights = (1 - (xlim-events['x']).abs()) * (\n","                        1 - (ylim-events['y']).abs())\n","                    index = H * W * events['p'].long() + \\\n","                        W * ylim.long() + \\\n","                        xlim.long()\n","\n","                    voxel_grid.put_(\n","                        index[mask], interp_weights[mask], accumulate=True)\n","\n","        return voxel_grid\n","\n","\n","def flow_16bit_to_float(flow_16bit: np.ndarray):\n","    assert flow_16bit.dtype == np.uint16\n","    assert flow_16bit.ndim == 3\n","    h, w, c = flow_16bit.shape\n","    assert c == 3\n","\n","    valid2D = flow_16bit[..., 2] == 1\n","    assert valid2D.shape == (h, w)\n","    assert np.all(flow_16bit[~valid2D, -1] == 0)\n","    valid_map = np.where(valid2D)\n","\n","    # to actually compute something useful:\n","    flow_16bit = flow_16bit.astype('float')\n","\n","    flow_map = np.zeros((h, w, 2))\n","    flow_map[valid_map[0], valid_map[1], 0] = (\n","        flow_16bit[valid_map[0], valid_map[1], 0] - 2 ** 15) / 128\n","    flow_map[valid_map[0], valid_map[1], 1] = (\n","        flow_16bit[valid_map[0], valid_map[1], 1] - 2 ** 15) / 128\n","    return flow_map, valid2D"],"metadata":{"trusted":true,"id":"8z3ZgnkfGb7P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def warp_images_with_flow(images, flow):\n","    dim3 = 0\n","    if images.dim() == 3:\n","        dim3 = 1\n","        images = images.unsqueeze(0)\n","        flow = flow.unsqueeze(0)\n","    height = images.shape[2]\n","    width = images.shape[3]\n","    flow_x,flow_y = flow[:,0,...],flow[:,1,...]\n","    coord_x, coord_y = torch.meshgrid(torch.arange(height), torch.arange(width))\n","\n","    if torch.cuda.is_available():\n","        pos_x = coord_x.reshape(height,width).type(torch.float32).cuda() + flow_x\n","        pos_y = coord_y.reshape(height,width).type(torch.float32).cuda() + flow_y\n","    else: # Troubleshoot without cuda\n","        pos_x = coord_x.reshape(height,width).type(torch.float32) + flow_x\n","        pos_y = coord_y.reshape(height,width).type(torch.float32) + flow_y\n","    pos_x = (pos_x-(height-1)/2)/((height-1)/2)\n","    pos_y = (pos_y-(width-1)/2)/((width-1)/2)\n","\n","    pos = torch.stack((pos_y,pos_x),3).type(torch.float32)\n","    result = torch.nn.functional.grid_sample(images, pos, mode='bilinear', padding_mode='zeros')\n","    if dim3 == 1:\n","        result = result.squeeze()\n","\n","    return result\n","\n","def charbonnier_loss(delta, alpha=0.45, epsilon=1e-3):\n","        loss = torch.mean(torch.pow((delta ** 2 + epsilon ** 2), alpha))\n","        return loss\n","\n","def compute_smoothness_loss(flow):\n","\n","    flow_ucrop = flow[..., 1:]\n","    flow_dcrop = flow[..., :-1]\n","    flow_lcrop = flow[..., 1:, :]\n","    flow_rcrop = flow[..., :-1, :]\n","\n","    flow_ulcrop = flow[..., 1:, 1:]\n","    flow_drcrop = flow[..., :-1, :-1]\n","    flow_dlcrop = flow[..., :-1, 1:]\n","    flow_urcrop = flow[..., 1:, :-1]\n","\n","    smoothness_loss = charbonnier_loss(flow_lcrop - flow_rcrop) +\\\n","                      charbonnier_loss(flow_ucrop - flow_dcrop) +\\\n","                      charbonnier_loss(flow_ulcrop - flow_drcrop) +\\\n","                      charbonnier_loss(flow_dlcrop - flow_urcrop)\n","    smoothness_loss /= 4.\n","\n","    return smoothness_loss\n","\n","def compute_photometric_loss(prev_images, next_images, flow_dict):\n","    total_photometric_loss = 0.\n","    loss_weight_sum = 0.\n","    for i in range(len(flow_dict)):\n","        for image_num in range(prev_images.shape[0]):\n","            flow = flow_dict[\"flow{}\".format(i)][image_num]\n","            height = flow.shape[1]\n","            width = flow.shape[2]\n","\n","            prev_images_resize = F.to_tensor(F.resize(F.to_pil_image(prev_images[image_num].cpu()),\n","                                                    [height, width]))\n","            next_images_resize = F.to_tensor(F.resize(F.to_pil_image(next_images[image_num].cpu()),\n","                                                    [height, width]))\n","\n","            if torch.cuda.is_available():\n","                prev_images_resize = prev_images_resize.cuda()\n","                next_images_resize = next_images_resize.cuda()\n","\n","            next_images_warped = warp_images_with_flow(next_images_resize, flow)\n","\n","            distance = next_images_warped - prev_images_resize\n","            photometric_loss = charbonnier_loss(distance)\n","            total_photometric_loss += photometric_loss\n","        loss_weight_sum += 1.\n","    total_photometric_loss /= loss_weight_sum\n","\n","    return total_photometric_loss\n","\n","\n","class TotalLoss(torch.nn.Module):\n","    def __init__(self, smoothness_weight, weight_decay_weight=1e-4):\n","        super(TotalLoss, self).__init__()\n","        self._smoothness_weight = smoothness_weight\n","        self._weight_decay_weight = weight_decay_weight\n","\n","    def forward(self, flow_dict, prev_image, next_image, EVFlowNet_model):\n","        # weight decay loss\n","        weight_decay_loss = 0\n","        for i in EVFlowNet_model.parameters():\n","            weight_decay_loss += torch.sum(i ** 2) / 2 * self._weight_decay_weight\n","\n","        # smoothness loss\n","        smoothness_loss = 0\n","        for i in range(len(flow_dict)):\n","            smoothness_loss += compute_smoothness_loss(flow_dict[\"flow{}\".format(i)])\n","        smoothness_loss *= self._smoothness_weight / 4.\n","\n","        # Photometric loss.\n","        photometric_loss = compute_photometric_loss(prev_image,\n","                                                    next_image,\n","                                                    flow_dict)\n","\n","        # Warped next image for debugging.\n","        #next_image_warped = warp_images_with_flow(next_image,\n","        #                                          flow_dict['flow3'])\n","\n","        loss = weight_decay_loss + photometric_loss + smoothness_loss\n","\n","        return loss"],"metadata":{"trusted":true,"id":"72oMFIZCGb7P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## `datasets.py`"],"metadata":{"id":"iNELPnjTGb7Q"}},{"cell_type":"code","source":["VISU_INDEX = 1\n","\n","\n","class EventSlicer:\n","    def __init__(self, h5f: h5py.File):\n","        self.h5f = h5f\n","\n","        self.events = dict()\n","        for dset_str in ['p', 'x', 'y', 't']:\n","            self.events[dset_str] = self.h5f['events/{}'.format(dset_str)]\n","\n","        # This is the mapping from milliseconds to event index:\n","        # It is defined such that\n","        # (1) t[ms_to_idx[ms]] >= ms*1000\n","        # (2) t[ms_to_idx[ms] - 1] < ms*1000\n","        # ,where 'ms' is the time in milliseconds and 't' the event timestamps in microseconds.\n","        #\n","        # As an example, given 't' and 'ms':\n","        # t:    0     500    2100    5000    5000    7100    7200    7200    8100    9000\n","        # ms:   0       1       2       3       4       5       6       7       8       9\n","        #\n","        # we get\n","        #\n","        # ms_to_idx:\n","        #       0       2       2       3       3       3       5       5       8       9\n","        self.ms_to_idx = np.asarray(self.h5f['ms_to_idx'], dtype='int64')\n","\n","        self.t_offset = int(h5f['t_offset'][()])\n","        self.t_final = int(self.events['t'][-1]) + self.t_offset\n","\n","    def get_final_time_us(self):\n","        return self.t_final\n","\n","    def get_events(self, t_start_us: int, t_end_us: int) -> Dict[str, np.ndarray]:\n","        \"\"\"Get events (p, x, y, t) within the specified time window\n","        Parameters\n","        ----------\n","        t_start_us: start time in microseconds\n","        t_end_us: end time in microseconds\n","        Returns\n","        -------\n","        events: dictionary of (p, x, y, t) or None if the time window cannot be retrieved\n","        \"\"\"\n","        assert t_start_us < t_end_us\n","\n","        # We assume that the times are top-off-day, hence subtract offset:\n","        t_start_us -= self.t_offset\n","        t_end_us -= self.t_offset\n","\n","        t_start_ms, t_end_ms = self.get_conservative_window_ms(\n","            t_start_us, t_end_us)\n","        t_start_ms_idx = self.ms2idx(t_start_ms)\n","        t_end_ms_idx = self.ms2idx(t_end_ms)\n","        if t_start_ms_idx is None or t_end_ms_idx is None:\n","            print('Error', 'start', t_start_us, 'end', t_end_us)\n","            # Cannot guarantee window size anymore\n","            return None\n","\n","        events = dict()\n","        time_array_conservative = np.asarray(\n","            self.events['t'][t_start_ms_idx:t_end_ms_idx])\n","        idx_start_offset, idx_end_offset = self.get_time_indices_offsets(\n","            time_array_conservative, t_start_us, t_end_us)\n","        t_start_us_idx = t_start_ms_idx + idx_start_offset\n","        t_end_us_idx = t_start_ms_idx + idx_end_offset\n","        # Again add t_offset to get gps time\n","        events['t'] = time_array_conservative[idx_start_offset:idx_end_offset] + self.t_offset\n","        for dset_str in ['p', 'x', 'y']:\n","            events[dset_str] = np.asarray(\n","                self.events[dset_str][t_start_us_idx:t_end_us_idx])\n","            assert events[dset_str].size == events['t'].size\n","        return events\n","\n","    @staticmethod\n","    def get_conservative_window_ms(ts_start_us: int, ts_end_us) -> Tuple[int, int]:\n","        \"\"\"Compute a conservative time window of time with millisecond resolution.\n","        We have a time to index mapping for each millisecond. Hence, we need\n","        to compute the lower and upper millisecond to retrieve events.\n","        Parameters\n","        ----------\n","        ts_start_us:    start time in microseconds\n","        ts_end_us:      end time in microseconds\n","        Returns\n","        -------\n","        window_start_ms:    conservative start time in milliseconds\n","        window_end_ms:      conservative end time in milliseconds\n","        \"\"\"\n","        assert ts_end_us > ts_start_us\n","        window_start_ms = math.floor(ts_start_us/1000)\n","        window_end_ms = math.ceil(ts_end_us/1000)\n","        return window_start_ms, window_end_ms\n","\n","    @staticmethod\n","    @jit(nopython=True)\n","    def get_time_indices_offsets(\n","            time_array: np.ndarray,\n","            time_start_us: int,\n","            time_end_us: int) -> Tuple[int, int]:\n","        \"\"\"Compute index offset of start and end timestamps in microseconds\n","        Parameters\n","        ----------\n","        time_array:     timestamps (in us) of the events\n","        time_start_us:  start timestamp (in us)\n","        time_end_us:    end timestamp (in us)\n","        Returns\n","        -------\n","        idx_start:  Index within this array corresponding to time_start_us\n","        idx_end:    Index within this array corresponding to time_end_us\n","        such that (in non-edge cases)\n","        time_array[idx_start] >= time_start_us\n","        time_array[idx_end] >= time_end_us\n","        time_array[idx_start - 1] < time_start_us\n","        time_array[idx_end - 1] < time_end_us\n","        this means that\n","        time_start_us <= time_array[idx_start:idx_end] < time_end_us\n","        \"\"\"\n","\n","        assert time_array.ndim == 1\n","\n","        idx_start = -1\n","        if time_array[-1] < time_start_us:\n","\n","            # Return same index twice: array[x:x] is empty.\n","            return time_array.size, time_array.size\n","        else:\n","            for idx_from_start in range(0, time_array.size, 1):\n","                if time_array[idx_from_start] >= time_start_us:\n","                    idx_start = idx_from_start\n","                    break\n","        assert idx_start >= 0\n","\n","        idx_end = time_array.size\n","        for idx_from_end in range(time_array.size - 1, -1, -1):\n","            if time_array[idx_from_end] >= time_end_us:\n","                idx_end = idx_from_end\n","            else:\n","                break\n","\n","        assert time_array[idx_start] >= time_start_us\n","        if idx_end < time_array.size:\n","            assert time_array[idx_end] >= time_end_us\n","        if idx_start > 0:\n","            assert time_array[idx_start - 1] < time_start_us\n","        if idx_end > 0:\n","            assert time_array[idx_end - 1] < time_end_us\n","        return idx_start, idx_end\n","\n","    def ms2idx(self, time_ms: int) -> int:\n","        assert time_ms >= 0\n","        if time_ms >= self.ms_to_idx.size:\n","            return None\n","        return self.ms_to_idx[time_ms]\n","\n","\n","class Sequence(Dataset):\n","    def __init__(self, seq_path: Path, representation_type: RepresentationType, mode: str = 'test', delta_t_ms: int = 100,\n","                 num_bins: int = 4, transforms=[], name_idx=0, visualize=False, load_gt=False):\n","        assert num_bins >= 1\n","        assert delta_t_ms == 100\n","        assert seq_path.is_dir(), seq_path\n","        assert mode in {'train', 'test'}\n","        assert representation_type is not None\n","        '''\n","        ディレクトリ構造:\n","\n","        data\n","        ├─test\n","        |  ├─seq_1\n","        |  |    ├─events_left\n","        |  |    |   ├─events.h5\n","        |  |    |   └─rectify_map.h5\n","        |  |    └─forward_timestamps.txt\n","        └─train\n","            ├─seq_1\n","            |    ├─events_left\n","            |    |       ├─ events.h5\n","            |    |       └─ rectify_map.h5\n","            |    ├─flow_forward\n","            |    |       ├─ 000134.png\n","            |    |       |.....\n","            |    └─forward_timestamps.txt\n","            ├─seq_2\n","            └─seq_3\n","        '''\n","        self.seq_name = PurePath(seq_path).name\n","        self.mode = mode\n","        self.name_idx = name_idx\n","        self.visualize_samples = visualize\n","        self.load_gt = load_gt\n","        self.transforms = transforms\n","        if self.mode == \"test\":\n","            assert load_gt == False\n","            # Get Test Timestamp File\n","            ev_dir_location = seq_path / 'events_left'\n","            timestamp_file = seq_path / 'forward_timestamps.txt'\n","            flow_path = seq_path / 'flow_forward'\n","            timestamps_flow = np.loadtxt(\n","                seq_path / 'forward_timestamps.txt', delimiter=',', dtype='int64')\n","            self.indices = np.arange(len(timestamps_flow))\n","            self.timestamps_flow = timestamps_flow[:, 0]\n","\n","        elif self.mode == \"train\":\n","            ev_dir_location = seq_path / 'events_left'\n","            flow_path = seq_path / 'flow_forward'\n","            timestamp_file = seq_path / 'forward_timestamps.txt'\n","            self.flow_png = [Path(os.path.join(flow_path, img)) for img in sorted(\n","                os.listdir(flow_path))]\n","            timestamps_flow = np.loadtxt(\n","                seq_path / 'forward_timestamps.txt', delimiter=',', dtype='int64')\n","            self.indices = np.arange(len(timestamps_flow))\n","            self.timestamps_flow = timestamps_flow[:, 0]\n","        else:\n","            pass\n","        assert timestamp_file.is_file()\n","\n","        file = np.genfromtxt(\n","            timestamp_file,\n","            delimiter=','\n","        )\n","\n","        self.idx_to_visualize = file[:, 2] if file.shape[1] == 3 else []\n","\n","        # Save output dimensions\n","        self.height = 480\n","        self.width = 640\n","        self.num_bins = num_bins\n","\n","\n","        # Set event representation\n","        self.voxel_grid = VoxelGrid(\n","                (self.num_bins, self.height, self.width), normalize=True)\n","        self.delta_t_us = delta_t_ms * 1000\n","\n","        # Left events only\n","        ev_data_file = ev_dir_location / 'events.h5'\n","        ev_rect_file = ev_dir_location / 'rectify_map.h5'\n","\n","        h5f_location = h5py.File(str(ev_data_file), 'r')\n","        self.h5f = h5f_location\n","        self.event_slicer = EventSlicer(h5f_location)\n","\n","        self.h5rect = h5py.File(str(ev_rect_file), 'r')\n","        self.rectify_ev_map = self.h5rect['rectify_map'][()]\n","\n","\n","    def events_to_voxel_grid(self, p, t, x, y, device: str = 'cpu'):\n","        t = (t - t[0]).astype('float32')\n","        t = (t/t[-1])\n","        x = x.astype('float32')\n","        y = y.astype('float32')\n","        pol = p.astype('float32')\n","        event_data_torch = {\n","            'p': torch.from_numpy(pol),\n","            't': torch.from_numpy(t),\n","            'x': torch.from_numpy(x),\n","            'y': torch.from_numpy(y),\n","        }\n","        return self.voxel_grid.convert(event_data_torch)\n","\n","    def getHeightAndWidth(self):\n","        return self.height, self.width\n","\n","    @staticmethod\n","    def get_disparity_map(filepath: Path):\n","        assert filepath.is_file()\n","        disp_16bit = cv2.imread(str(filepath), cv2.IMREAD_ANYDEPTH)\n","        return disp_16bit.astype('float32')/256\n","\n","    @staticmethod\n","    def load_flow(flowfile: Path):\n","        assert flowfile.exists()\n","        assert flowfile.suffix == '.png'\n","        flow_16bit = iio.imread(str(flowfile), plugin='PNG-FI')\n","        flow, valid2D = flow_16bit_to_float(flow_16bit)\n","        return flow, valid2D\n","\n","    @staticmethod\n","    def close_callback(h5f):\n","        h5f.close()\n","\n","    def get_image_width_height(self):\n","        return self.height, self.width\n","\n","    def __len__(self):\n","        # Ignore the first and last images as their own\n","        return len(self.timestamps_flow) # - 2\n","\n","    def rectify_events(self, x: np.ndarray, y: np.ndarray):\n","        # assert location in self.locations\n","        # From distorted to undistorted\n","        rectify_map = self.rectify_ev_map\n","        assert rectify_map.shape == (\n","            self.height, self.width, 2), rectify_map.shape\n","        assert x.max() < self.width\n","        assert y.max() < self.height\n","        return rectify_map[y, x]\n","\n","    def get_data(self, index) -> Dict[str, any]:\n","        # Adjust index to skip the first element\n","#         index += 1\n","\n","        ts_start: int = self.timestamps_flow[index] - self.delta_t_us\n","        ts_end: int = self.timestamps_flow[index]\n","\n","        file_index = self.indices[index]\n","\n","        output = {\n","            'file_index': file_index,\n","            'timestamp': self.timestamps_flow[index],\n","            'seq_name': self.seq_name\n","        }\n","        # Save sample for benchmark submission\n","        output['save_submission'] = file_index in self.idx_to_visualize\n","        output['visualize'] = self.visualize_samples\n","        event_data = self.event_slicer.get_events(\n","            ts_start, ts_end)\n","        p = event_data['p']\n","        t = event_data['t']\n","        x = event_data['x']\n","        y = event_data['y']\n","\n","        xy_rect = self.rectify_events(x, y)\n","        x_rect = xy_rect[:, 0]\n","        y_rect = xy_rect[:, 1]\n","\n","        if self.voxel_grid is None:\n","            raise NotImplementedError\n","        else:\n","            event_representation = self.events_to_voxel_grid(\n","                p, t, x_rect, y_rect)\n","            output['event_volume'] = event_representation\n","        output['name_map'] = self.name_idx\n","\n","        if self.load_gt:\n","            output['flow_gt'] = [torch.tensor(x) for x in self.load_flow(self.flow_png[index])]\n","            output['flow_gt'][0] = torch.moveaxis(output['flow_gt'][0], -1, 0)\n","            output['flow_gt'][1] = torch.unsqueeze(output['flow_gt'][1], 0)\n","\n","            flow_gt_shape = [tensor.shape for tensor in output['flow_gt']]\n","            zero_flow_gt = [torch.zeros_like(tensor) for tensor in output['flow_gt']]\n","\n","            # Load previous image\n","            if index > 0:\n","                output['prev_flow_gt'] = [torch.tensor(x) for x in self.load_flow(self.flow_png[index - 1])]\n","                output['prev_flow_gt'][0] = torch.moveaxis(output['prev_flow_gt'][0], -1, 0)\n","                output['prev_flow_gt'][1] = torch.unsqueeze(output['prev_flow_gt'][1], 0)\n","            else:\n","                output['prev_flow_gt'] = zero_flow_gt\n","\n","            # Load next image\n","            if index < len(self.timestamps_flow) - 1:\n","                output['next_flow_gt'] = [torch.tensor(x) for x in self.load_flow(self.flow_png[index + 1])]\n","                output['next_flow_gt'][0] = torch.moveaxis(output['next_flow_gt'][0], -1, 0)\n","                output['next_flow_gt'][1] = torch.unsqueeze(output['next_flow_gt'][1], 0)\n","            else:\n","                output['next_flow_gt'] = zero_flow_gt\n","\n","        return output\n","\n","    def __getitem__(self, idx):\n","        # Adjust index to skip the first element\n","        sample = self.get_data(idx) # idx + 1\n","\n","        if self.transforms:\n","            sample = self.transforms(sample)\n","\n","        return sample\n","\n","    def get_voxel_grid(self, idx):\n","\n","        if idx == 0:\n","            event_data = self.event_slicer.get_events(\n","                self.timestamps_flow[0] - self.delta_t_us, self.timestamps_flow[0])\n","        elif idx > 0 and idx <= self.__len__():\n","            event_data = self.event_slicer.get_events(\n","                self.timestamps_flow[idx-1], self.timestamps_flow[idx-1] + self.delta_t_us)\n","        else:\n","            raise IndexError\n","\n","        p = event_data['p']\n","        t = event_data['t']\n","        x = event_data['x']\n","        y = event_data['y']\n","\n","        xy_rect = self.rectify_events(x, y)\n","        x_rect = xy_rect[:, 0]\n","        y_rect = xy_rect[:, 1]\n","        return self.events_to_voxel_grid(p, t, x_rect, y_rect)\n","\n","    def get_event_count_image(self, ts_start, ts_end, num_bins, normalize=True):\n","        assert ts_end > ts_start\n","        delta_t_bin = (ts_end - ts_start) / num_bins\n","        ts_start_bin = np.linspace(\n","            ts_start, ts_end, num=num_bins, endpoint=False)\n","        ts_end_bin = ts_start_bin + delta_t_bin\n","        assert abs(ts_end_bin[-1] - ts_end) < 10.\n","        ts_end_bin[-1] = ts_end\n","\n","        event_count = torch.zeros(\n","            (num_bins, self.height, self.width), dtype=torch.float, requires_grad=False)\n","\n","        for i in range(num_bins):\n","            event_data = self.event_slicer.get_events(\n","                ts_start_bin[i], ts_end_bin[i])\n","            p = event_data['p']\n","            t = event_data['t']\n","            x = event_data['x']\n","            y = event_data['y']\n","\n","            t = (t - t[0]).astype('float32')\n","            t = (t/t[-1])\n","            x = x.astype('float32')\n","            y = y.astype('float32')\n","            pol = p.astype('float32')\n","            event_data_torch = {\n","                'p': torch.from_numpy(pol),\n","                't': torch.from_numpy(t),\n","                'x': torch.from_numpy(x),\n","                'y': torch.from_numpy(y),\n","            }\n","            x = event_data_torch['x']\n","            y = event_data_torch['y']\n","            xy_rect = self.rectify_events(x.int(), y.int())\n","            x_rect = torch.from_numpy(xy_rect[:, 0]).long()\n","            y_rect = torch.from_numpy(xy_rect[:, 1]).long()\n","            value = 2*event_data_torch['p']-1\n","            index = self.width*y_rect + x_rect\n","            mask = (x_rect < self.width) & (y_rect < self.height)\n","            event_count[i].put_(index[mask], value[mask], accumulate=True)\n","\n","        return event_count\n","\n","    @staticmethod\n","    def normalize_tensor(event_count):\n","        mask = torch.nonzero(event_count, as_tuple=True)\n","        if mask[0].size()[0] > 0:\n","            mean = event_count[mask].mean()\n","            std = event_count[mask].std()\n","            if std > 0:\n","                event_count[mask] = (event_count[mask] - mean) / std\n","            else:\n","                event_count[mask] = event_count[mask] - mean\n","        return event_count\n","\n","\n","class SequenceRecurrent(Sequence):\n","    def __init__(self, seq_path: Path, representation_type: RepresentationType, mode: str = 'test', delta_t_ms: int = 100,\n","                 num_bins: int = 15, transforms=None, sequence_length=1, name_idx=0, visualize=False, load_gt=False):\n","        super(SequenceRecurrent, self).__init__(seq_path, representation_type, mode, delta_t_ms, transforms=transforms,\n","                                                name_idx=name_idx, visualize=visualize, load_gt=load_gt)\n","        self.crop_size = self.transforms['randomcrop'] if 'randomcrop' in self.transforms else None\n","        self.sequence_length = sequence_length\n","        self.valid_indices = self.get_continuous_sequences()\n","\n","    def get_continuous_sequences(self):\n","        continuous_seq_idcs = []\n","        if self.sequence_length > 1:\n","            for i in range(len(self.timestamps_flow)-self.sequence_length+1):\n","                diff = self.timestamps_flow[i + self.sequence_length-1] - self.timestamps_flow[i]\n","                if diff < np.max([100000 * (self.sequence_length-1) + 1000, 101000]):\n","                    continuous_seq_idcs.append(i)\n","        else:\n","            for i in range(len(self.timestamps_flow)-1):\n","                diff = self.timestamps_flow[i+1] - self.timestamps_flow[i]\n","                if diff < np.max([100000 * (self.sequence_length-1) + 1000, 101000]):\n","                    continuous_seq_idcs.append(i)\n","        return continuous_seq_idcs\n","\n","    def __len__(self):\n","        return len(self.valid_indices)\n","\n","    def __getitem__(self, idx):\n","        assert idx >= 0\n","        assert idx < len(self)\n","\n","        # Valid index is the actual index we want to load, which guarantees a continuous sequence length\n","        valid_idx = self.valid_indices[idx]\n","\n","        sequence = []\n","        j = valid_idx\n","\n","        ts_cur = self.timestamps_flow[j]\n","        # Add first sample\n","        sample = self.get_data_sample(j)\n","        sequence.append(sample)\n","\n","        # Data augmentation according to first sample\n","        crop_window = None\n","        flip = None\n","        if 'crop_window' in sample.keys():\n","            crop_window = sample['crop_window']\n","        if 'flipped' in sample.keys():\n","            flip = sample['flipped']\n","\n","        for i in range(self.sequence_length-1):\n","            j += 1\n","            ts_old = ts_cur\n","            ts_cur = self.timestamps_flow[j]\n","            assert(ts_cur-ts_old < 100000 + 1000)\n","            sample = self.get_data_sample(\n","                j, crop_window=crop_window, flip=flip)\n","            sequence.append(sample)\n","\n","        # Check if the current sample is the first sample of a continuous sequence\n","        if idx == 0 or self.valid_indices[idx]-self.valid_indices[idx-1] != 1:\n","            sequence[0]['new_sequence'] = 1\n","            print(\"Timestamp {} is the first one of the next seq!\".format(\n","                self.timestamps_flow[self.valid_indices[idx]]))\n","        else:\n","            sequence[0]['new_sequence'] = 0\n","\n","        # random crop\n","        if self.crop_size is not None:\n","            i, j, h, w = RandomCrop.get_params(\n","                sample[\"event_volume_old\"], output_size=self.crop_size)\n","            keys_to_crop = [\"event_volume_old\", \"event_volume_new\",\n","                            \"flow_gt_event_volume_old\", \"flow_gt_event_volume_new\",\n","                            \"flow_gt_next\",]\n","\n","            for sample in sequence:\n","                for key, value in sample.items():\n","                    if key in keys_to_crop:\n","                        if isinstance(value, torch.Tensor):\n","                            sample[key] = tf.functional.crop(value, i, j, h, w)\n","                        elif isinstance(value, list) or isinstance(value, tuple):\n","                            sample[key] = [tf.functional.crop(v, i, j, h, w) for v in value]\n","        return sequence\n","\n","\n","class DatasetProvider:\n","    def __init__(self, dataset_path: Path, representation_type: RepresentationType, delta_t_ms: int = 100, num_bins=4, config=None, visualize=False, transforms=None):\n","        test_path = Path(os.path.join(dataset_path, 'test'))\n","        train_path = Path(os.path.join(dataset_path, 'train'))\n","        assert dataset_path.is_dir(), str(dataset_path)\n","        assert test_path.is_dir(), str(test_path)\n","        assert delta_t_ms == 100\n","        self.config = config\n","        self.name_mapper_test = []\n","\n","        if transforms:\n","            self.transforms = transforms\n","        else:\n","            self.transforms = tf.Compose([\n","                transforms.ToTensor(),  # Convert image to PyTorch tensor\n","            ])\n","\n","        # Assemble test sequences\n","        test_sequences = list()\n","        for child in test_path.iterdir():\n","            self.name_mapper_test.append(str(child).split(\"/\")[-1])\n","            test_sequences.append(Sequence(child, representation_type, 'test', delta_t_ms, num_bins,\n","                                               name_idx=len(self.name_mapper_test) - 1,\n","                                               visualize=visualize,\n","                                               transforms=self.transforms))\n","\n","        self.test_dataset = torch.utils.data.ConcatDataset(test_sequences)\n","\n","        # Assemble train sequences\n","        available_seqs = os.listdir(train_path)\n","\n","        seqs = available_seqs\n","\n","        train_sequences: list[Sequence] = []\n","        for seq in seqs:\n","            extra_arg = dict()\n","            train_sequences.append(Sequence(Path(train_path) / seq,\n","                                   representation_type=representation_type, mode=\"train\",\n","                                   load_gt=True, **extra_arg, transforms=self.transforms))\n","            self.train_dataset: torch.utils.data.ConcatDataset[Sequence] = torch.utils.data.ConcatDataset(train_sequences)\n","\n","    def get_test_dataset(self):\n","        return self.test_dataset\n","\n","    def get_train_dataset(self):\n","        return self.train_dataset\n","\n","    def get_name_mapping_test(self):\n","        return self.name_mapper_test\n","\n","    def summary(self, logger):\n","        logger.write_line(\n","            \"================================== Dataloader Summary ====================================\", True)\n","        logger.write_line(\"Loader Type:\\t\\t\" + self.__class__.__name__, True)\n","        logger.write_line(\"Number of Voxel Bins: {}\".format(\n","            self.test_dataset.datasets[0].num_bins), True)\n","        logger.write_line(\"Number of Train Sequences: {}\".format(\n","            len(self.train_dataset)), True)\n","\n","def train_collate(sample_list):\n","    batch = dict()\n","    for field_name in sample_list[0]:\n","        if field_name == 'timestamp':\n","            batch['timestamp'] = [sample[field_name] for sample in sample_list]\n","        if field_name == 'seq_name':\n","            batch['seq_name'] = [sample[field_name] for sample in sample_list]\n","        if field_name == 'new_sequence':\n","            batch['new_sequence'] = [sample[field_name]\n","                                     for sample in sample_list]\n","        if field_name.startswith(\"event_volume\"):\n","            batch[field_name] = torch.stack(\n","                [sample[field_name] for sample in sample_list])\n","        if field_name.startswith(\"flow_gt\") or field_name.startswith('prev_flow_gt') or field_name.startswith('next_flow_gt'):\n","            if all(field_name in x for x in sample_list):\n","                batch[field_name] = torch.stack(\n","                    [sample[field_name][0] for sample in sample_list])\n","                batch[field_name + '_valid_mask'] = torch.stack(\n","                    [sample[field_name][1] for sample in sample_list])\n","\n","    return batch\n","\n","\n","def rec_train_collate(sample_list):\n","    seq_length = len(sample_list[0])\n","    seq_of_batch = []\n","    for i in range(seq_length):\n","        seq_of_batch.append(train_collate(\n","            [sample[i] for sample in sample_list]))\n","    return seq_of_batch"],"metadata":{"trusted":true,"id":"BMr6cf3sGb7Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## `base.py`"],"metadata":{"id":"0yC5BJmEGb7S"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","\n","class build_resnet_block(nn.Module):\n","    \"\"\"\n","    a resnet block which includes two general_conv2d\n","    \"\"\"\n","    def __init__(self, channels, layers=2, do_batch_norm=False):\n","        super(build_resnet_block,self).__init__()\n","        self._channels = channels\n","        self._layers = layers\n","\n","        self.res_block = nn.Sequential(*[general_conv2d(in_channels=self._channels,\n","                                             out_channels=self._channels,\n","                                             strides=1,\n","                                             do_batch_norm=do_batch_norm) for i in range(self._layers)])\n","\n","    def forward(self,input_res):\n","        inputs = input_res.clone()\n","        input_res = self.res_block(input_res)\n","        return input_res + inputs\n","\n","class upsample_conv2d_and_predict_flow(nn.Module):\n","    \"\"\"\n","    an upsample convolution layer which includes a nearest interpolate and a general_conv2d\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, ksize=3, do_batch_norm=False):\n","        super(upsample_conv2d_and_predict_flow, self).__init__()\n","        self._in_channels = in_channels\n","        self._out_channels = out_channels\n","        self._ksize = ksize\n","        self._do_batch_norm = do_batch_norm\n","\n","        self.general_conv2d = general_conv2d(\n","            in_channels=self._in_channels,\n","            out_channels=self._out_channels,\n","            ksize=self._ksize,\n","            strides=1,\n","            do_batch_norm=self._do_batch_norm,\n","            padding=0\n","        )\n","\n","        self.pad = nn.ReflectionPad2d(\n","            padding=(\n","                int((self._ksize - 1) / 2),\n","                int((self._ksize - 1) / 2),\n","                int((self._ksize - 1) / 2),\n","                int((self._ksize - 1) / 2)\n","            )\n","        )\n","\n","        self.predict_flow = general_conv2d(\n","            in_channels=self._out_channels,\n","            out_channels=2,\n","            ksize=1,\n","            strides=1,\n","            padding=0,\n","            activation='tanh'\n","        )\n","\n","    def forward(self, conv):\n","        shape = conv.shape\n","        conv = nn.functional.interpolate(conv,size=[shape[2]*2,shape[3]*2],mode='nearest')\n","        conv = self.pad(conv)\n","        conv = self.general_conv2d(conv)\n","\n","        flow = self.predict_flow(conv) * 256.\n","\n","        return torch.cat([conv,flow.clone()], dim=1), flow\n","\n","def general_conv2d(in_channels,out_channels, ksize=3, strides=2, padding=1, do_batch_norm=False, activation='relu',\n","                   attention=False):\n","    \"\"\"\n","    a general convolution layer which includes a conv2d, a relu and a batch_normalize\n","    \"\"\"\n","    layers = [nn.Conv2d(in_channels = in_channels,out_channels = out_channels,kernel_size = ksize,\n","                        stride=strides,padding=padding)]\n","\n","    if activation == 'relu':\n","        layers.append(nn.ReLU(inplace=True))\n","\n","        if do_batch_norm:\n","            layers.append(nn.BatchNorm2d(out_channels,eps=1e-5,momentum=0.99))\n","\n","    elif activation == 'tanh':\n","        layers.append(nn.Tanh())\n","\n","        if do_batch_norm:\n","            layers.append(nn.BatchNorm2d(out_channels,eps=1e-5,momentum=0.99))\n","\n","    if attention:\n","        # layers.append(Attention(out_channels, heads=2, dim_head=8, dropout=0.1))\n","        layers.append(BlockSparseAttention(in_dim=out_channels, heads=2, blocksize=16))\n","\n","    return nn.Sequential(*layers)"],"metadata":{"trusted":true,"id":"2HJxkwoEGb7S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## `evflownet.py`"],"metadata":{"id":"V533QO-UGb7S"}},{"cell_type":"markdown","source":["### EvFlowNet"],"metadata":{"id":"UicJAHYMGwui"}},{"cell_type":"code","source":["_BASE_CHANNELS = 32 # default: 64\n","\n","class EVFlowNet_with_LSTM(nn.Module):\n","    def __init__(self, args):\n","        super(EVFlowNet_with_LSTM, self).__init__()\n","        self._args = args\n","\n","        self.encoder1 = general_conv2d(\n","            in_channels=4,\n","            out_channels=_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm,\n","            attention=False\n","        )\n","        self.encoder2 = general_conv2d(\n","            in_channels=_BASE_CHANNELS,\n","            out_channels=2*_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm,\n","            attention=False\n","        )\n","        self.encoder3 = general_conv2d(\n","            in_channels=2*_BASE_CHANNELS,\n","            out_channels=4*_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm,\n","            attention=False\n","        )\n","        self.encoder4 = general_conv2d(\n","            in_channels=4*_BASE_CHANNELS,\n","            out_channels=8*_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm,\n","            attention=False\n","        )\n","\n","        self.resnet_block = nn.Sequential(*[build_resnet_block(8*_BASE_CHANNELS, do_batch_norm=not self._args.no_batch_norm) for i in range(2)])\n","\n","        self.decoder1 = upsample_conv2d_and_predict_flow(\n","            in_channels=16*_BASE_CHANNELS,\n","            out_channels=4*_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm\n","        )\n","        self.decoder2 = upsample_conv2d_and_predict_flow(\n","            in_channels=8*_BASE_CHANNELS+2,\n","            out_channels=2*_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm\n","        )\n","        self.decoder3 = upsample_conv2d_and_predict_flow(\n","            in_channels=4*_BASE_CHANNELS+2,\n","            out_channels=_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm\n","        )\n","        self.decoder4 = upsample_conv2d_and_predict_flow(\n","            in_channels=2*_BASE_CHANNELS+2,\n","            out_channels=int(_BASE_CHANNELS/2),\n","            do_batch_norm=not self._args.no_batch_norm\n","        )\n","\n","        self.lstm = nn.LSTM(\n","            input_size=8*_BASE_CHANNELS*30*40,\n","            hidden_size=8*_BASE_CHANNELS*30*40,\n","            num_layers=2,\n","            batch_first=True\n","        )\n","        # self.lstm = nn.LSTM(input_size=8*_BASE_CHANNELS, hidden_size=16*_BASE_CHANNELS, num_layers=2, batch_first=True, bidirectional=False)\n","\n","    def forward(self, inputs: torch.Tensor) -> Dict[str, torch.Tensor]: # [16, 3, 4, 480, 640]\n","        B, T, C, H, W = inputs.size()\n","\n","        inputs = inputs.view(B * T, C, H, W) # [B * T, C, H, W]\n","\n","        # encoder (for feature extraction)\n","        skip_connections = {}\n","        # inputs = self.encoder1(inputs)\n","        inputs = checkpoint(self.encoder1, inputs)\n","        skip_connections['skip0'] = inputs.clone() # torch.Size([48, 64, 240, 320])\n","\n","        # inputs = self.encoder2(inputs)\n","        inputs = checkpoint(self.encoder2, inputs)\n","        skip_connections['skip1'] = inputs.clone() # torch.Size([48, 128, 120, 160])\n","\n","        # inputs = self.encoder3(inputs)\n","        inputs = checkpoint(self.encoder3, inputs)\n","        skip_connections['skip2'] = inputs.clone() # torch.Size([48, 256, 60, 80])\n","\n","        # inputs = self.encoder4(inputs)\n","        inputs = checkpoint(self.encoder4, inputs)\n","        skip_connections['skip3'] = inputs.clone() # torch.Size([48, 512, 30, 40])\n","\n","        # transition (for feature extraction)\n","        inputs = self.resnet_block(inputs) # torch.Size([48, 512, 30, 40])\n","\n","        print(inputs.shape)\n","        # lstm (can't handle input before feature extraction)\n","        inputs = inputs.view(B, T, -1)  # [B, T, 8*_BASE_CHANNELS*30*40]\n","        print(inputs.shape)\n","        print(self.lstm.input_size)\n","\n","        lstm_out, _ = self.lstm(inputs)  # [B, T, hidden_size]\n","        lstm_out = lstm_out[:, -1, :]  # [B, hidden_size]\n","\n","        lstm_out = lstm_out.view(B, 8*_BASE_CHANNELS, 30, 40)\n","\n","        # decoder\n","        flow_dict = {}\n","        inputs = torch.cat([inputs, skip_connections['skip3']], dim=1) # torch.Size([16, 1024, 30, 40])\n","        inputs, flow = self.decoder1(inputs) # inputs: torch.Size([16, 258, 60, 80])\n","        flow_dict['flow0'] = flow.clone() # torch.Size([16, 2, 60, 80])\n","\n","        inputs = torch.cat([inputs, skip_connections['skip2']], dim=1) # torch.Size([16, 514, 60, 80])\n","        inputs, flow = self.decoder2(inputs) # inputs: torch.Size([16, 130, 120, 160])\n","        flow_dict['flow1'] = flow.clone() # torch.Size([16, 2, 120, 160])\n","\n","        inputs = torch.cat([inputs, skip_connections['skip1']], dim=1) # torch.Size([16, 258, 120, 160])\n","        inputs, flow = self.decoder3(inputs) # inputs: torch.Size([16, 66, 240, 320])\n","        flow_dict['flow2'] = flow.clone() # torch.Size([16, 2, 240, 320])\n","\n","        inputs = torch.cat([inputs, skip_connections['skip0']], dim=1) # torch.Size([16, 130, 240, 320])\n","        inputs, flow = self.decoder4(inputs) # inputs: torch.Size([16, 34, 480, 640])\n","        flow_dict['flow3'] = flow.clone() # torch.Size([16, 2, 480, 640])\n","\n","        return flow_dict"],"metadata":{"id":"JMxmGMUWHuou"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_BASE_CHANNELS = 16\n","\n","class EVFlowNet_with_LSTM_lite(nn.Module):\n","    def __init__(self, args):\n","        super(EVFlowNet_with_LSTM_lite, self).__init__()\n","        self._args = args\n","\n","        self.encoder1 = general_conv2d(\n","            in_channels=4,\n","            out_channels=_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm,\n","            attention=False\n","        )\n","        self.encoder2 = general_conv2d(\n","            in_channels=_BASE_CHANNELS,\n","            out_channels=2*_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm,\n","            attention=False\n","        )\n","        self.encoder3 = general_conv2d(\n","            in_channels=2*_BASE_CHANNELS,\n","            out_channels=4*_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm,\n","            attention=False\n","        )\n","        self.encoder4 = general_conv2d(\n","            in_channels=4*_BASE_CHANNELS,\n","            out_channels=8*_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm,\n","            attention=False\n","        )\n","\n","        self.resnet_block = nn.Sequential(*[build_resnet_block(8*_BASE_CHANNELS, do_batch_norm=not self._args.no_batch_norm) for _ in range(2)])\n","\n","        self.lstm = nn.LSTM(\n","            input_size=8*_BASE_CHANNELS*15*20,\n","            hidden_size=8*_BASE_CHANNELS*15*20,\n","            num_layers=1,\n","            batch_first=True\n","        )\n","\n","        self.decoder1 = upsample_conv2d_and_predict_flow(\n","            in_channels=8*_BASE_CHANNELS + 8*_BASE_CHANNELS,\n","            out_channels=4*_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm\n","        )\n","        self.decoder2 = upsample_conv2d_and_predict_flow(\n","            in_channels=4*_BASE_CHANNELS + 4*_BASE_CHANNELS + 2,\n","            out_channels=2*_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm\n","        )\n","        self.decoder3 = upsample_conv2d_and_predict_flow(\n","            in_channels=2*_BASE_CHANNELS + 2*_BASE_CHANNELS + 2,\n","            out_channels=_BASE_CHANNELS,\n","            do_batch_norm=not self._args.no_batch_norm\n","        )\n","        self.decoder4 = upsample_conv2d_and_predict_flow(\n","            in_channels=_BASE_CHANNELS + _BASE_CHANNELS + 2,\n","            out_channels=int(_BASE_CHANNELS/2),\n","            do_batch_norm=not self._args.no_batch_norm\n","        )\n","\n","    def forward(self, inputs: torch.Tensor) -> Dict[str, torch.Tensor]:\n","        B, T, C, H, W = inputs.size()\n","\n","        skip_connections = {}\n","\n","        inputs = inputs.view(-1, C, H, W)\n","        inputs = self.encoder1(inputs)\n","        skip_connections['skip0'] = inputs.clone()\n","\n","        inputs = self.encoder2(inputs)\n","        skip_connections['skip1'] = inputs.clone()\n","\n","        inputs = self.encoder3(inputs)\n","        skip_connections['skip2'] = inputs.clone()\n","\n","        inputs = self.encoder4(inputs)\n","        skip_connections['skip3'] = inputs.clone()\n","\n","        inputs = self.resnet_block(inputs)\n","\n","        inputs = inputs.view(B, T, -1)\n","        lstm_out, _ = self.lstm(inputs)\n","        lstm_out = lstm_out[:, -1, :]\n","\n","        lstm_out = lstm_out.view(B, 8*_BASE_CHANNELS, 15, 20)\n","\n","        flow_dict = {}\n","        inputs = torch.cat([lstm_out, skip_connections['skip3'].view(B, 8*_BASE_CHANNELS, 15, 20)], dim=1)\n","        inputs, flow = self.decoder1(inputs)\n","        flow_dict['flow0'] = flow.clone()\n","\n","        inputs = torch.cat([inputs, skip_connections['skip2'].view(B, 4*_BASE_CHANNELS, 30, 40)], dim=1)\n","        inputs, flow = self.decoder2(inputs)\n","        flow_dict['flow1'] = flow.clone()\n","\n","        inputs = torch.cat([inputs, skip_connections['skip1'].view(B, 2*_BASE_CHANNELS, 60, 80)], dim=1)\n","        inputs, flow = self.decoder3(inputs)\n","        flow_dict['flow2'] = flow.clone()\n","\n","        inputs = torch.cat([inputs, skip_connections['skip0'].view(B, _BASE_CHANNELS, 120, 160)], dim=1)\n","        inputs, flow = self.decoder4(inputs)\n","        flow_dict['flow3'] = flow.clone()\n","\n","        return flow_dict"],"metadata":{"id":"wttfEcHUfcTJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Image Preprocessing"],"metadata":{"id":"zG59jw9jM6k9"}},{"cell_type":"code","source":["class CombinedTransform:\n","    def __init__(self, transform=tf.Compose([ tf.ToTensor() ])):\n","        self.transform = transform\n","\n","    def __call__(self, flow_dict):\n","        seed = np.random.randint(2147483647)\n","\n","        flow_columns = ['event_volume', 'flow_gt', 'prev_flow_gt', 'next_flow_gt']\n","        flow_columns = [c for c in flow_columns if c in list(flow_dict.keys())]\n","\n","        for col in flow_columns:\n","            torch.manual_seed(seed)\n","\n","            if type(flow_dict[col]) == list:\n","                flow_dict[col] = [ self.transform(img) for img in flow_dict[col] ]\n","            else:\n","                flow_dict[col] = self.transform(flow_dict[col])\n","\n","        return flow_dict\n","\n","combined_transform = CombinedTransform(transform=tf.Compose([\n","    tf.GaussianBlur(kernel_size=(5, 5)),\n","]))"],"metadata":{"id":"HyemBQ4FOKgu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## `main.py`"],"metadata":{"id":"-W_HpclqGb7T"}},{"cell_type":"markdown","source":["Instead of changing the `Sequence`, just change how the data are loaded. Or iterate over the dataloader so that you use many images at once."],"metadata":{"id":"3lufwoSn32eN"}},{"cell_type":"code","source":["class RepresentationType(Enum):\n","    VOXEL = auto()\n","    STEPAN = auto()\n","\n","def set_seed(seed):\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","\n","def compute_epe_error(pred_flow: torch.Tensor, gt_flow: torch.Tensor):\n","    '''\n","    end-point-error (ground truthと予測値の二乗誤差)を計算\n","    pred_flow: torch.Tensor, Shape: torch.Size([B, 2, 480, 640]) => 予測したオプティカルフローデータ\n","    gt_flow: torch.Tensor, Shape: torch.Size([B, 2, 480, 640]) => 正解のオプティカルフローデータ\n","    '''\n","    epe = torch.mean(torch.mean(torch.norm(pred_flow - gt_flow, p=2, dim=1), dim=(1, 2)), dim=0)\n","    return epe\n","\n","def save_optical_flow_to_npy(flow: torch.Tensor, file_name: str):\n","    '''\n","    optical flowをnpyファイルに保存\n","    flow: torch.Tensor, Shape: torch.Size([2, 480, 640]) => オプティカルフローデータ\n","    file_name: str => ファイル名\n","    '''\n","    np.save(f\"{file_name}.npy\", flow.cpu().numpy())"],"metadata":{"trusted":true,"id":"k-O8nvDQGb7T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["args = cfg\n","set_seed(args.seed)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","'''\n","    ディレクトリ構造:\n","\n","    data\n","    ├─test\n","    |  ├─test_city\n","    |  |    ├─events_left\n","    |  |    |   ├─events.h5\n","    |  |    |   └─rectify_map.h5\n","    |  |    └─forward_timestamps.txt\n","    └─train\n","        ├─zurich_city_11_a\n","        |    ├─events_left\n","        |    |       ├─ events.h5\n","        |    |       └─ rectify_map.h5\n","        |    ├─ flow_forward\n","        |    |       ├─ 000134.png\n","        |    |       |.....\n","        |    └─ forward_timestamps.txt\n","        ├─zurich_city_11_b\n","        └─zurich_city_11_c\n","    '''\n","\n","# ------------------\n","#    Dataloader\n","# ------------------\n","\n","loader = DatasetProvider(\n","    dataset_path=Path(args.dataset_path),\n","    representation_type=RepresentationType.VOXEL,\n","    delta_t_ms=100,\n","    num_bins=4,\n","    transforms=combined_transform # Custom class\n",")\n","train_set = loader.get_train_dataset()\n","test_set = loader.get_test_dataset()\n","\n","# def split_train_valid(dataset):\n","#     train_indices = []\n","#     valid_indices = []\n","#     for idx in range(len(dataset)):\n","#         sample = dataset[idx]\n","#         if 'flow_gt_valid_mask' in sample and sample['flow_gt_valid_mask'].all():\n","#             valid_indices.append(idx)\n","#         else:\n","#             train_indices.append(idx)\n","#     train_subset = torch.utils.data.Subset(dataset, train_indices)\n","#     valid_subset = torch.utils.data.Subset(dataset, valid_indices)\n","#     return train_subset, valid_subset\n","\n","# train_set_split, valid_set_split = split_train_valid(train_set)\n","\n","collate_fn = train_collate\n","train_data = DataLoader(train_set, # train_set_split\n","                        batch_size=args.data_loader.train.batch_size,\n","                        shuffle=args.data_loader.train.shuffle,\n","                        collate_fn=collate_fn,\n","                        drop_last=False,\n","                        num_workers=os.cpu_count(),\n","                        pin_memory=True)\n","# valid_data = DataLoader(valid_set_split,\n","#                         batch_size=args.data_loader.train.batch_size,\n","#                         shuffle=args.data_loader.train.shuffle,\n","#                         collate_fn=collate_fn,\n","#                         drop_last=False,\n","#                         num_workers=os.cpu_count(),\n","#                         pin_memory=True)\n","test_data = DataLoader(test_set,\n","                       batch_size=args.data_loader.test.batch_size,\n","                       shuffle=args.data_loader.test.shuffle,\n","                       collate_fn=collate_fn,\n","                       drop_last=False,\n","                       num_workers=os.cpu_count(),\n","                       pin_memory=True)\n","\n","'''\n","train data:\n","    Type of batch: Dict\n","    Key: seq_name, Type: list\n","    Key: event_volume, Type: torch.Tensor, Shape: torch.Size([Batch, 4, 480, 640]) => イベントデータのバッチ\n","    Key: flow_gt, Type: torch.Tensor, Shape: torch.Size([Batch, 2, 480, 640]) => オプティカルフローデータのバッチ\n","    Key: flow_gt_valid_mask, Type: torch.Tensor, Shape: torch.Size([Batch, 1, 480, 640]) => オプティカルフローデータのvalid. ベースラインでは使わない\n","\n","test data:\n","    Type of batch: Dict\n","    Key: seq_name, Type: list\n","    Key: event_volume, Type: torch.Tensor, Shape: torch.Size([Batch, 4, 480, 640]) => イベントデータのバッチ\n","'''"],"metadata":{"trusted":true,"id":"2PL6GvxuGb7T","colab":{"base_uri":"https://localhost:8080/","height":107},"executionInfo":{"status":"ok","timestamp":1721099636493,"user_tz":-540,"elapsed":815,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}},"outputId":"688d3f0d-fcb5-4c2b-900b-b4e7f6ec3916"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ntrain data:\\n    Type of batch: Dict\\n    Key: seq_name, Type: list\\n    Key: event_volume, Type: torch.Tensor, Shape: torch.Size([Batch, 4, 480, 640]) => イベントデータのバッチ\\n    Key: flow_gt, Type: torch.Tensor, Shape: torch.Size([Batch, 2, 480, 640]) => オプティカルフローデータのバッチ\\n    Key: flow_gt_valid_mask, Type: torch.Tensor, Shape: torch.Size([Batch, 1, 480, 640]) => オプティカルフローデータのvalid. ベースラインでは使わない\\n\\ntest data:\\n    Type of batch: Dict\\n    Key: seq_name, Type: list\\n    Key: event_volume, Type: torch.Tensor, Shape: torch.Size([Batch, 4, 480, 640]) => イベントデータのバッチ\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["# ------------------\n","#       Model\n","# ------------------\n","model = EVFlowNet_with_LSTM_lite(args.train).to(device)\n","\n","# ------------------\n","#   optimizer\n","# ------------------\n","optimizer = torch.optim.Adam(model.parameters(), lr=args.train.initial_learning_rate, weight_decay=args.train.weight_decay)\n","scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=args.train.learning_rate_decay)\n","\n","loss_fn = TotalLoss(smoothness_weight=0.5)"],"metadata":{"id":"ZULZTTNcwxWL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# num_epochs = args.train.epochs\n","num_epochs = 1\n","\n","epe_losses = [[] for _ in range(num_epochs)]\n","overall_losses = [[] for _ in range(num_epochs)]"],"metadata":{"trusted":true,"id":"XOEcpMPHGb7T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BATCH_CONCAT = 3"],"metadata":{"id":"_aYlJLJa0Yqf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ------------------\n","#   Start training\n","# ------------------\n","model.train()\n","\n","for epoch in range(num_epochs):\n","\n","    total_loss = 0\n","    prev_event_volumes = [torch.zeros([args.data_loader.train.batch_size, 4, 480, 640])] * BATCH_CONCAT # Acts as a queue\n","\n","    print(\"on epoch: {}\".format(epoch + 1))\n","    for i, batch in enumerate(tqdm(train_data)):\n","\n","        try:\n","            batch: Dict[str, Any]\n","\n","            event_image = batch[\"event_volume\"].to(device) # [B, 4, 480, 640]\n","            ground_truth_flow = batch[\"flow_gt\"].to(device) # [B, 2, 480, 640]\n","\n","            prev_event_volumes.append(event_image)\n","\n","            prev_ground_truth_flow = batch['prev_flow_gt'].to(device) # [B, 2, 480, 640]\n","            next_ground_truth_flow = batch['next_flow_gt'].to(device) # [B, 2, 480, 640]\n","\n","            prev_volumes = prev_event_volumes[-BATCH_CONCAT:] # Ensures always the last BATCH_CONCAT elements chosen\n","            prev_volumes_reshaped = torch.stack(prev_volumes, dim=1) # [B, C=BATCH_CONCAT, 4, 480, 640]\n","\n","            print(len(prev_volumes))\n","            print(prev_volumes[0].shape)\n","            print(prev_volumes_reshaped.shape)\n","\n","            flow_dict = model(prev_volumes_reshaped) # 'flow3' is of shape [B, 2, 480, 640]\n","\n","            epe_loss: torch.Tensor = compute_epe_error(flow_dict['flow3'], ground_truth_flow)\n","            overall_loss: torch.Tensor = loss_fn(flow_dict,\n","                                         prev_ground_truth_flow,\n","                                         next_ground_truth_flow,\n","                                         model)\n","\n","            print(f\"batch {i} OVERALL LOSS: {overall_loss.item()}\")\n","            print(f\"batch {i} EPE LOSS: {epe_loss.item()}\")\n","            overall_losses[epoch].append(overall_loss.item())\n","            epe_losses[epoch].append(epe_loss.item())\n","\n","            optimizer.zero_grad()\n","            epe_loss.backward() # Change this to which loss function is to be updated\n","            optimizer.step()\n","\n","            total_loss += epe_loss.item() # This too\n","\n","            while len(prev_event_volumes) >= BATCH_CONCAT:\n","                prev_event_volumes.pop(0)\n","\n","        except KeyboardInterrupt:\n","            current_time = time.strftime(\"%Y%m%d%H%M%S\")\n","            model_path = f\"models/model_{current_time}.pth\" # /kaggle/input/\n","            torch.save(model.state_dict(), model_path)\n","            print(f\"Model saved to {model_path}\")\n","\n","            raise SystemExit(\"KeyboardInterrupt\")\n","\n","    scheduler.step()\n","\n","    print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_data)}')"],"metadata":{"trusted":true,"id":"YbeX9fnmGb7U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# epe_losses = list(map(lambda x: x.item(), epe_losses[0]))\n","# overall_losses = list(map(lambda x: x.item(), overall_losses[0]))\n","\n","plt.figure(figsize=(16, 9))\n","len_x = min(len(epe_losses), len(overall_losses))\n","\n","plt.plot(epe_losses[:len_x])\n","plt.plot(overall_losses[:len_x])\n","\n","plt.xlabel('Batch Number')\n","plt.ylabel('Loss')\n","\n","plt.grid()\n","plt.legend()\n","\n","plt.show()"],"metadata":{"trusted":true,"id":"lc4Q3acvGb7U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","# Create the directory if it doesn't exist\n","# if not os.path.exists('checkpoints'):\n","#     os.makedirs('checkpoints')\n","\n","current_time = time.strftime(\"%Y%m%d-%H%M%S\")\n","model_path = f\"models/model_{current_time}_epoch4.pth\"\n","torch.save(model.state_dict(), model_path)\n","print(f\"Model saved to {model_path}\")"],"metadata":{"trusted":true,"id":"FtltB8OsGb7U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","flow: torch.Tensor = torch.tensor([]).to(device)\n","\n","prev_event_volumes = [] # Acts as a queue\n","\n","with torch.no_grad():\n","    print(\"start test\")\n","    for batch in tqdm(test_data):\n","        batch: Dict[str, Any]\n","\n","        event_image = batch[\"event_volume\"].to(device)\n","        prev_event_volumes.append(event_image)\n","\n","        avg_event_image = torch.mean(torch.stack(prev_event_volumes, dim=0), dim=0)\n","\n","        batch_flow = model(avg_event_image) # [1, 2, 480, 640]\n","        flow = torch.cat((flow, batch_flow['flow3']), dim=0)  # [N, 2, 480, 640]\n","\n","        if len(prev_event_volumes) >= BATCH_CONCAT:\n","            prev_event_volumes.pop()\n","\n","    print(\"test done\")\n","# ------------------\n","#  save submission\n","# ------------------\n","current_time = time.strftime(\"%Y%m%d-%H%M%S\")\n","save_optical_flow_to_npy(flow, f'submissions/submission_{current_time}')"],"metadata":{"trusted":true,"id":"LdYOQdyRGb7U"},"execution_count":null,"outputs":[]}]}