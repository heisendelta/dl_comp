{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":8921598,"sourceType":"datasetVersion","datasetId":5366031}],"dockerImageVersionId":30748,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[],"collapsed_sections":["tpYel0PeGb7O","iNELPnjTGb7Q","Asa6m7ccKl4E"]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","project_dir = '/content/drive/My Drive/Colab Notebooks/matsuoo/dl/event_camera_repo'\n","%cd {project_dir}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qPUOD4mDtEbi","executionInfo":{"status":"ok","timestamp":1721136351279,"user_tz":-540,"elapsed":2735,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}},"outputId":"fb6f1115-fd43-43ed-ed98-a9b865f85bf3"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/Colab Notebooks/matsuoo/dl/event_camera_repo\n"]}]},{"cell_type":"code","source":["!pip install hydra-core omegaconf hdf5plugin h5py numba imageio imageio-ffmpeg tqdm torchvision --quiet"],"metadata":{"id":"fdfquW_iGb7K","colab":{"base_uri":"https://localhost:8080/"},"outputId":"df85b786-1d5b-4d76-dee2-50a5c2034b86","execution":{"iopub.status.busy":"2024-07-16T12:25:08.752711Z","iopub.execute_input":"2024-07-16T12:25:08.753096Z","iopub.status.idle":"2024-07-16T12:25:18.691026Z","shell.execute_reply.started":"2024-07-16T12:25:08.753067Z","shell.execute_reply":"2024-07-16T12:25:18.689911Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1721136217153,"user_tz":-540,"elapsed":101125,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["import torch\n","import hydra\n","from omegaconf import DictConfig\n","from torch.utils.data import DataLoader\n","import random\n","import numpy as np\n","from enum import Enum, auto\n","from tqdm import tqdm\n","from pathlib import Path\n","from typing import Dict, Any\n","import os\n","import time\n","\n","import math\n","from pathlib import PurePath\n","from typing import Tuple\n","import cv2\n","import hdf5plugin\n","import h5py\n","from numba import jit\n","import imageio\n","imageio.plugins.freeimage.download()\n","import imageio.v3 as iio\n","from torchvision.transforms import RandomCrop\n","from torchvision import transforms as tf\n","from torch.utils.checkpoint import checkpoint\n","from torch.utils.data import Dataset\n","import torchvision.transforms.functional as F\n","import torch.nn.functional as nn_F\n","\n","from torch import nn"],"metadata":{"id":"tAW9ff9RGb7M","colab":{"base_uri":"https://localhost:8080/"},"outputId":"aa62de2b-db40-49b1-d90e-9d8ff049bc80","execution":{"iopub.status.busy":"2024-07-16T12:25:18.692654Z","iopub.execute_input":"2024-07-16T12:25:18.692935Z","iopub.status.idle":"2024-07-16T12:25:47.403550Z","shell.execute_reply.started":"2024-07-16T12:25:18.692904Z","shell.execute_reply":"2024-07-16T12:25:47.402497Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1721136234319,"user_tz":-540,"elapsed":17173,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Imageio: 'libfreeimage-3.16.0-linux64.so' was not found on your computer; downloading it now.\n","Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/freeimage/libfreeimage-3.16.0-linux64.so (4.6 MB)\n","Downloading: 8192/4830080 bytes (0.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b90112/4830080 bytes (1.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b630784/4830080 bytes (13.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b4136960/4830080 bytes (85.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b4830080/4830080 bytes (100.0%)\n","  Done\n","File saved as /root/.imageio/freeimage/libfreeimage-3.16.0-linux64.so.\n"]}]},{"cell_type":"code","source":["!pip install einops\n","from einops.layers.torch import Rearrange\n","from einops import rearrange"],"metadata":{"id":"EjOf_ZjXGb7N","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bfcf8483-57fe-4bcc-cb13-7e4e9a0ec21d","execution":{"iopub.status.busy":"2024-07-16T12:25:47.404757Z","iopub.execute_input":"2024-07-16T12:25:47.405218Z","iopub.status.idle":"2024-07-16T12:25:50.770790Z","shell.execute_reply.started":"2024-07-16T12:25:47.405187Z","shell.execute_reply":"2024-07-16T12:25:50.769630Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1721136265662,"user_tz":-540,"elapsed":31346,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting einops\n","  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m798.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.8.0\n"]}]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"ZGFPKK5nCkDE","execution":{"iopub.status.busy":"2024-07-16T12:26:48.226891Z","iopub.execute_input":"2024-07-16T12:26:48.227695Z","iopub.status.idle":"2024-07-16T12:26:48.231675Z","shell.execute_reply.started":"2024-07-16T12:26:48.227656Z","shell.execute_reply":"2024-07-16T12:26:48.230972Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1721136265662,"user_tz":-540,"elapsed":14,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## `utils.py`"],"metadata":{"id":"tpYel0PeGb7O"}},{"cell_type":"code","source":["def set_seed(seed: int = 0) -> None:\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","class RepresentationType(Enum):\n","    VOXEL = auto()\n","    STEPAN = auto()\n","\n","\n","class EventRepresentation:\n","    def __init__(self):\n","        pass\n","\n","    def convert(self, events):\n","        raise NotImplementedError\n","\n","\n","class VoxelGrid(EventRepresentation):\n","    def __init__(self, input_size: tuple, normalize: bool):\n","        assert len(input_size) == 3\n","        self.voxel_grid = torch.zeros(\n","            (input_size), dtype=torch.float, requires_grad=False)\n","        self.nb_channels = input_size[0]\n","        self.normalize = normalize\n","\n","    def convert(self, events):\n","        C, H, W = self.voxel_grid.shape\n","        with torch.no_grad():\n","            self.voxel_grid = self.voxel_grid.to(events['p'].device)\n","            voxel_grid = self.voxel_grid.clone()\n","\n","            t_norm = events['t']\n","            t_norm = (C - 1) * (t_norm-t_norm[0]) / (t_norm[-1]-t_norm[0])\n","\n","            x0 = events['x'].int()\n","            y0 = events['y'].int()\n","            t0 = t_norm.int()\n","\n","            value = 2*events['p']-1\n","            #start_t = time()\n","            for xlim in [x0, x0+1]:\n","                for ylim in [y0, y0+1]:\n","                    for tlim in [t0, t0+1]:\n","\n","                        mask = (xlim < W) & (xlim >= 0) & (ylim < H) & (\n","                            ylim >= 0) & (tlim >= 0) & (tlim < self.nb_channels)\n","                        interp_weights = value * (1 - (xlim-events['x']).abs()) * (\n","                            1 - (ylim-events['y']).abs()) * (1 - (tlim - t_norm).abs())\n","                        index = H * W * tlim.long() + \\\n","                            W * ylim.long() + \\\n","                            xlim.long()\n","\n","                        voxel_grid.put_(\n","                            index[mask], interp_weights[mask], accumulate=True)\n","\n","            if self.normalize:\n","                mask = torch.nonzero(voxel_grid, as_tuple=True)\n","                if mask[0].size()[0] > 0:\n","                    mean = voxel_grid[mask].mean()\n","                    std = voxel_grid[mask].std()\n","                    if std > 0:\n","                        voxel_grid[mask] = (voxel_grid[mask] - mean) / std\n","                    else:\n","                        voxel_grid[mask] = voxel_grid[mask] - mean\n","\n","        return voxel_grid\n","\n","\n","class PolarityCount(EventRepresentation):\n","    def __init__(self, input_size: tuple):\n","        assert len(input_size) == 3\n","        self.voxel_grid = torch.zeros(\n","            (input_size), dtype=torch.float, requires_grad=False)\n","        self.nb_channels = input_size[0]\n","\n","    def convert(self, events):\n","        C, H, W = self.voxel_grid.shape\n","        with torch.no_grad():\n","            self.voxel_grid = self.voxel_grid.to(events['p'].device)\n","            voxel_grid = self.voxel_grid.clone()\n","\n","            x0 = events['x'].int()\n","            y0 = events['y'].int()\n","\n","            #start_t = time()\n","            for xlim in [x0, x0+1]:\n","                for ylim in [y0, y0+1]:\n","                    mask = (xlim < W) & (xlim >= 0) & (ylim < H) & (\n","                        ylim >= 0)\n","                    interp_weights = (1 - (xlim-events['x']).abs()) * (\n","                        1 - (ylim-events['y']).abs())\n","                    index = H * W * events['p'].long() + \\\n","                        W * ylim.long() + \\\n","                        xlim.long()\n","\n","                    voxel_grid.put_(\n","                        index[mask], interp_weights[mask], accumulate=True)\n","\n","        return voxel_grid\n","\n","\n","def flow_16bit_to_float(flow_16bit: np.ndarray):\n","    assert flow_16bit.dtype == np.uint16\n","    assert flow_16bit.ndim == 3\n","    h, w, c = flow_16bit.shape\n","    assert c == 3\n","\n","    valid2D = flow_16bit[..., 2] == 1\n","    assert valid2D.shape == (h, w)\n","    assert np.all(flow_16bit[~valid2D, -1] == 0)\n","    valid_map = np.where(valid2D)\n","\n","    # to actually compute something useful:\n","    flow_16bit = flow_16bit.astype('float')\n","\n","    flow_map = np.zeros((h, w, 2))\n","    flow_map[valid_map[0], valid_map[1], 0] = (\n","        flow_16bit[valid_map[0], valid_map[1], 0] - 2 ** 15) / 128\n","    flow_map[valid_map[0], valid_map[1], 1] = (\n","        flow_16bit[valid_map[0], valid_map[1], 1] - 2 ** 15) / 128\n","    return flow_map, valid2D"],"metadata":{"id":"8z3ZgnkfGb7P","execution":{"iopub.status.busy":"2024-07-16T12:26:48.238775Z","iopub.execute_input":"2024-07-16T12:26:48.239029Z","iopub.status.idle":"2024-07-16T12:26:48.256779Z","shell.execute_reply.started":"2024-07-16T12:26:48.239004Z","shell.execute_reply":"2024-07-16T12:26:48.256092Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1721136265662,"user_tz":-540,"elapsed":13,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def warp_images_with_flow(images, flow):\n","    dim3 = 0\n","    if images.dim() == 3:\n","        dim3 = 1\n","        images = images.unsqueeze(0)\n","        flow = flow.unsqueeze(0)\n","    height = images.shape[2]\n","    width = images.shape[3]\n","    flow_x,flow_y = flow[:,0,...],flow[:,1,...]\n","    coord_x, coord_y = torch.meshgrid(torch.arange(height), torch.arange(width))\n","\n","    if torch.cuda.is_available():\n","        pos_x = coord_x.reshape(height,width).type(torch.float32).cuda() + flow_x\n","        pos_y = coord_y.reshape(height,width).type(torch.float32).cuda() + flow_y\n","    else: # Troubleshoot without cuda\n","        pos_x = coord_x.reshape(height,width).type(torch.float32) + flow_x\n","        pos_y = coord_y.reshape(height,width).type(torch.float32) + flow_y\n","    pos_x = (pos_x-(height-1)/2)/((height-1)/2)\n","    pos_y = (pos_y-(width-1)/2)/((width-1)/2)\n","\n","    pos = torch.stack((pos_y,pos_x),3).type(torch.float32)\n","    result = torch.nn.functional.grid_sample(images, pos, mode='bilinear', padding_mode='zeros')\n","    if dim3 == 1:\n","        result = result.squeeze()\n","\n","    return result\n","\n","def charbonnier_loss(delta, alpha=0.45, epsilon=1e-3):\n","        loss = torch.mean(torch.pow((delta ** 2 + epsilon ** 2), alpha))\n","        return loss\n","\n","def compute_smoothness_loss(flow):\n","\n","    flow_ucrop = flow[..., 1:]\n","    flow_dcrop = flow[..., :-1]\n","    flow_lcrop = flow[..., 1:, :]\n","    flow_rcrop = flow[..., :-1, :]\n","\n","    flow_ulcrop = flow[..., 1:, 1:]\n","    flow_drcrop = flow[..., :-1, :-1]\n","    flow_dlcrop = flow[..., :-1, 1:]\n","    flow_urcrop = flow[..., 1:, :-1]\n","\n","    smoothness_loss = charbonnier_loss(flow_lcrop - flow_rcrop) +\\\n","                      charbonnier_loss(flow_ucrop - flow_dcrop) +\\\n","                      charbonnier_loss(flow_ulcrop - flow_drcrop) +\\\n","                      charbonnier_loss(flow_dlcrop - flow_urcrop)\n","    smoothness_loss /= 4.\n","\n","    return smoothness_loss\n","\n","def compute_photometric_loss(prev_images, next_images, flow_dict):\n","    total_photometric_loss = 0.\n","    loss_weight_sum = 0.\n","    for i in range(len(flow_dict)):\n","        for image_num in range(prev_images.shape[0]):\n","            flow = flow_dict[\"flow{}\".format(i)][image_num]\n","            height = flow.shape[1]\n","            width = flow.shape[2]\n","\n","            prev_images_resize = F.to_tensor(F.resize(F.to_pil_image(prev_images[image_num].cpu()),\n","                                                    [height, width]))\n","            next_images_resize = F.to_tensor(F.resize(F.to_pil_image(next_images[image_num].cpu()),\n","                                                    [height, width]))\n","\n","            if torch.cuda.is_available():\n","                prev_images_resize = prev_images_resize.cuda()\n","                next_images_resize = next_images_resize.cuda()\n","\n","            next_images_warped = warp_images_with_flow(next_images_resize, flow)\n","\n","            distance = next_images_warped - prev_images_resize\n","            photometric_loss = charbonnier_loss(distance)\n","            total_photometric_loss += photometric_loss\n","        loss_weight_sum += 1.\n","    total_photometric_loss /= loss_weight_sum\n","\n","    return total_photometric_loss\n","\n","\n","class TotalLoss(torch.nn.Module):\n","    def __init__(self, smoothness_weight, weight_decay_weight=1e-4):\n","        super(TotalLoss, self).__init__()\n","        self._smoothness_weight = smoothness_weight\n","        self._weight_decay_weight = weight_decay_weight\n","\n","    def forward(self, flow_dict, prev_image, next_image, EVFlowNet_model):\n","        # weight decay loss\n","        weight_decay_loss = 0\n","        for i in EVFlowNet_model.parameters():\n","            weight_decay_loss += torch.sum(i ** 2) / 2 * self._weight_decay_weight\n","\n","        # smoothness loss\n","        smoothness_loss = 0\n","        for i in range(len(flow_dict)):\n","            smoothness_loss += compute_smoothness_loss(flow_dict[\"flow{}\".format(i)])\n","        smoothness_loss *= self._smoothness_weight / 4.\n","\n","        # Photometric loss.\n","        photometric_loss = compute_photometric_loss(prev_image,\n","                                                    next_image,\n","                                                    flow_dict)\n","\n","        # Warped next image for debugging.\n","        #next_image_warped = warp_images_with_flow(next_image,\n","        #                                          flow_dict['flow3'])\n","\n","        loss = weight_decay_loss + photometric_loss + smoothness_loss\n","\n","        return loss"],"metadata":{"id":"72oMFIZCGb7P","execution":{"iopub.status.busy":"2024-07-16T12:26:48.257965Z","iopub.execute_input":"2024-07-16T12:26:48.258317Z","iopub.status.idle":"2024-07-16T12:26:48.273989Z","shell.execute_reply.started":"2024-07-16T12:26:48.258290Z","shell.execute_reply":"2024-07-16T12:26:48.273372Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1721136265662,"user_tz":-540,"elapsed":2,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Logging"],"metadata":{"id":"4HVuxiIAMXY7"}},{"cell_type":"markdown","source":["```python\n","import sys\n","import logging\n","import datetime\n","import time\n","\n","class StreamToLogger:\n","    def __init__(self, logger, log_level=logging.INFO):\n","        self.logger = logger\n","        self.log_level = log_level\n","        self.linebuf = ''\n","\n","    def write(self, buf):\n","        for line in buf.rstrip().splitlines():\n","            self.logger.log(self.log_level, line.rstrip())\n","            sys.__stdout__.write(line + '\\n')\n","\n","    def flush(self):\n","        pass\n","\n","# Create logger\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","logger.setLevel(logging.INFO)\n","\n","# Create file handler which logs even debug messages\n","current_time = time.strftime(\"%Y%m%d-%H%M%S\")\n","logfile = f'../../logs/log_{current_time}.log'\n","fh = logging.FileHandler(logfile)\n","fh.setLevel(logging.DEBUG)\n","\n","# Create console handler with a higher log level\n","ch = logging.StreamHandler(sys.__stdout__)\n","ch.setLevel(logging.INFO)\n","\n","# Create formatter and add it to the handlers\n","formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n","fh.setFormatter(formatter)\n","ch.setFormatter(formatter)\n","\n","# Add the handlers to the logger\n","logger.addHandler(fh)\n","logger.addHandler(ch)\n","\n","# Ensure the log file is created\n","with open(logfile, 'w') as file:\n","    file.write('')  # Create an empty log file\n","\n","# Redirect stdout and stderr to the logger\n","sys.stdout = StreamToLogger(logger, logging.INFO)\n","sys.stderr = StreamToLogger(logger, logging.ERROR)\n","```"],"metadata":{"id":"l1Giqrq2Qwm4"}},{"cell_type":"markdown","source":["## `datasets.py`"],"metadata":{"id":"iNELPnjTGb7Q"}},{"cell_type":"code","source":["VISU_INDEX = 1\n","\n","\n","class EventSlicer:\n","    def __init__(self, h5f: h5py.File):\n","        self.h5f = h5f\n","\n","        self.events = dict()\n","        for dset_str in ['p', 'x', 'y', 't']:\n","            self.events[dset_str] = self.h5f['events/{}'.format(dset_str)]\n","\n","        # This is the mapping from milliseconds to event index:\n","        # It is defined such that\n","        # (1) t[ms_to_idx[ms]] >= ms*1000\n","        # (2) t[ms_to_idx[ms] - 1] < ms*1000\n","        # ,where 'ms' is the time in milliseconds and 't' the event timestamps in microseconds.\n","        #\n","        # As an example, given 't' and 'ms':\n","        # t:    0     500    2100    5000    5000    7100    7200    7200    8100    9000\n","        # ms:   0       1       2       3       4       5       6       7       8       9\n","        #\n","        # we get\n","        #\n","        # ms_to_idx:\n","        #       0       2       2       3       3       3       5       5       8       9\n","        self.ms_to_idx = np.asarray(self.h5f['ms_to_idx'], dtype='int64')\n","\n","        self.t_offset = int(h5f['t_offset'][()])\n","        self.t_final = int(self.events['t'][-1]) + self.t_offset\n","\n","    def get_final_time_us(self):\n","        return self.t_final\n","\n","    def get_events(self, t_start_us: int, t_end_us: int) -> Dict[str, np.ndarray]:\n","        \"\"\"Get events (p, x, y, t) within the specified time window\n","        Parameters\n","        ----------\n","        t_start_us: start time in microseconds\n","        t_end_us: end time in microseconds\n","        Returns\n","        -------\n","        events: dictionary of (p, x, y, t) or None if the time window cannot be retrieved\n","        \"\"\"\n","        assert t_start_us < t_end_us\n","\n","        # We assume that the times are top-off-day, hence subtract offset:\n","        t_start_us -= self.t_offset\n","        t_end_us -= self.t_offset\n","\n","        t_start_ms, t_end_ms = self.get_conservative_window_ms(\n","            t_start_us, t_end_us)\n","        t_start_ms_idx = self.ms2idx(t_start_ms)\n","        t_end_ms_idx = self.ms2idx(t_end_ms)\n","        if t_start_ms_idx is None or t_end_ms_idx is None:\n","            print('Error', 'start', t_start_us, 'end', t_end_us)\n","            # Cannot guarantee window size anymore\n","            return None\n","\n","        events = dict()\n","        time_array_conservative = np.asarray(\n","            self.events['t'][t_start_ms_idx:t_end_ms_idx])\n","        idx_start_offset, idx_end_offset = self.get_time_indices_offsets(\n","            time_array_conservative, t_start_us, t_end_us)\n","        t_start_us_idx = t_start_ms_idx + idx_start_offset\n","        t_end_us_idx = t_start_ms_idx + idx_end_offset\n","        # Again add t_offset to get gps time\n","        events['t'] = time_array_conservative[idx_start_offset:idx_end_offset] + self.t_offset\n","        for dset_str in ['p', 'x', 'y']:\n","            events[dset_str] = np.asarray(\n","                self.events[dset_str][t_start_us_idx:t_end_us_idx])\n","            assert events[dset_str].size == events['t'].size\n","        return events\n","\n","    @staticmethod\n","    def get_conservative_window_ms(ts_start_us: int, ts_end_us) -> Tuple[int, int]:\n","        \"\"\"Compute a conservative time window of time with millisecond resolution.\n","        We have a time to index mapping for each millisecond. Hence, we need\n","        to compute the lower and upper millisecond to retrieve events.\n","        Parameters\n","        ----------\n","        ts_start_us:    start time in microseconds\n","        ts_end_us:      end time in microseconds\n","        Returns\n","        -------\n","        window_start_ms:    conservative start time in milliseconds\n","        window_end_ms:      conservative end time in milliseconds\n","        \"\"\"\n","        assert ts_end_us > ts_start_us\n","        window_start_ms = math.floor(ts_start_us/1000)\n","        window_end_ms = math.ceil(ts_end_us/1000)\n","        return window_start_ms, window_end_ms\n","\n","    @staticmethod\n","    @jit(nopython=True)\n","    def get_time_indices_offsets(\n","            time_array: np.ndarray,\n","            time_start_us: int,\n","            time_end_us: int) -> Tuple[int, int]:\n","        \"\"\"Compute index offset of start and end timestamps in microseconds\n","        Parameters\n","        ----------\n","        time_array:     timestamps (in us) of the events\n","        time_start_us:  start timestamp (in us)\n","        time_end_us:    end timestamp (in us)\n","        Returns\n","        -------\n","        idx_start:  Index within this array corresponding to time_start_us\n","        idx_end:    Index within this array corresponding to time_end_us\n","        such that (in non-edge cases)\n","        time_array[idx_start] >= time_start_us\n","        time_array[idx_end] >= time_end_us\n","        time_array[idx_start - 1] < time_start_us\n","        time_array[idx_end - 1] < time_end_us\n","        this means that\n","        time_start_us <= time_array[idx_start:idx_end] < time_end_us\n","        \"\"\"\n","\n","        assert time_array.ndim == 1\n","\n","        idx_start = -1\n","        if time_array[-1] < time_start_us:\n","\n","            # Return same index twice: array[x:x] is empty.\n","            return time_array.size, time_array.size\n","        else:\n","            for idx_from_start in range(0, time_array.size, 1):\n","                if time_array[idx_from_start] >= time_start_us:\n","                    idx_start = idx_from_start\n","                    break\n","        assert idx_start >= 0\n","\n","        idx_end = time_array.size\n","        for idx_from_end in range(time_array.size - 1, -1, -1):\n","            if time_array[idx_from_end] >= time_end_us:\n","                idx_end = idx_from_end\n","            else:\n","                break\n","\n","        assert time_array[idx_start] >= time_start_us\n","        if idx_end < time_array.size:\n","            assert time_array[idx_end] >= time_end_us\n","        if idx_start > 0:\n","            assert time_array[idx_start - 1] < time_start_us\n","        if idx_end > 0:\n","            assert time_array[idx_end - 1] < time_end_us\n","        return idx_start, idx_end\n","\n","    def ms2idx(self, time_ms: int) -> int:\n","        assert time_ms >= 0\n","        if time_ms >= self.ms_to_idx.size:\n","            return None\n","        return self.ms_to_idx[time_ms]\n","\n","\n","class Sequence(Dataset):\n","    def __init__(self, seq_path: Path, representation_type: RepresentationType, mode: str = 'test', delta_t_ms: int = 100,\n","                 num_bins: int = 4, transforms=[], name_idx=0, visualize=False, load_gt=False):\n","        assert num_bins >= 1\n","        assert delta_t_ms == 100\n","        assert seq_path.is_dir(), seq_path\n","        assert mode in {'train', 'test'}\n","        assert representation_type is not None\n","        '''\n","        ディレクトリ構造:\n","\n","        data\n","        ├─test\n","        |  ├─seq_1\n","        |  |    ├─events_left\n","        |  |    |   ├─events.h5\n","        |  |    |   └─rectify_map.h5\n","        |  |    └─forward_timestamps.txt\n","        └─train\n","            ├─seq_1\n","            |    ├─events_left\n","            |    |       ├─ events.h5\n","            |    |       └─ rectify_map.h5\n","            |    ├─flow_forward\n","            |    |       ├─ 000134.png\n","            |    |       |.....\n","            |    └─forward_timestamps.txt\n","            ├─seq_2\n","            └─seq_3\n","        '''\n","        self.seq_name = PurePath(seq_path).name\n","        self.mode = mode\n","        self.name_idx = name_idx\n","        self.visualize_samples = visualize\n","        self.load_gt = load_gt\n","        self.transforms = transforms\n","        if self.mode == \"test\":\n","            assert load_gt == False\n","            # Get Test Timestamp File\n","            ev_dir_location = seq_path / 'events_left'\n","            timestamp_file = seq_path / 'forward_timestamps.txt'\n","            flow_path = seq_path / 'flow_forward'\n","            timestamps_flow = np.loadtxt(\n","                seq_path / 'forward_timestamps.txt', delimiter=',', dtype='int64')\n","            self.indices = np.arange(len(timestamps_flow))\n","            self.timestamps_flow = timestamps_flow[:, 0]\n","\n","        elif self.mode == \"train\":\n","            ev_dir_location = seq_path / 'events_left'\n","            flow_path = seq_path / 'flow_forward'\n","            timestamp_file = seq_path / 'forward_timestamps.txt'\n","            self.flow_png = [Path(os.path.join(flow_path, img)) for img in sorted(\n","                os.listdir(flow_path))]\n","            timestamps_flow = np.loadtxt(\n","                seq_path / 'forward_timestamps.txt', delimiter=',', dtype='int64')\n","            self.indices = np.arange(len(timestamps_flow))\n","            self.timestamps_flow = timestamps_flow[:, 0]\n","        else:\n","            pass\n","        assert timestamp_file.is_file()\n","\n","        file = np.genfromtxt(\n","            timestamp_file,\n","            delimiter=','\n","        )\n","\n","        self.idx_to_visualize = file[:, 2] if file.shape[1] == 3 else []\n","\n","        # Save output dimensions\n","        self.height = 480\n","        self.width = 640\n","        self.num_bins = num_bins\n","\n","\n","        # Set event representation\n","        self.voxel_grid = VoxelGrid(\n","                (self.num_bins, self.height, self.width), normalize=True)\n","        self.delta_t_us = delta_t_ms * 1000\n","\n","        # Left events only\n","        ev_data_file = ev_dir_location / 'events.h5'\n","        ev_rect_file = ev_dir_location / 'rectify_map.h5'\n","\n","        h5f_location = h5py.File(str(ev_data_file), 'r')\n","        self.h5f = h5f_location\n","        self.event_slicer = EventSlicer(h5f_location)\n","\n","        self.h5rect = h5py.File(str(ev_rect_file), 'r')\n","        self.rectify_ev_map = self.h5rect['rectify_map'][()]\n","\n","\n","    def events_to_voxel_grid(self, p, t, x, y, device: str = 'cpu'):\n","        t = (t - t[0]).astype('float32')\n","        t = (t/t[-1])\n","        x = x.astype('float32')\n","        y = y.astype('float32')\n","        pol = p.astype('float32')\n","        event_data_torch = {\n","            'p': torch.from_numpy(pol),\n","            't': torch.from_numpy(t),\n","            'x': torch.from_numpy(x),\n","            'y': torch.from_numpy(y),\n","        }\n","        return self.voxel_grid.convert(event_data_torch)\n","\n","    def getHeightAndWidth(self):\n","        return self.height, self.width\n","\n","    @staticmethod\n","    def get_disparity_map(filepath: Path):\n","        assert filepath.is_file()\n","        disp_16bit = cv2.imread(str(filepath), cv2.IMREAD_ANYDEPTH)\n","        return disp_16bit.astype('float32')/256\n","\n","    @staticmethod\n","    def load_flow(flowfile: Path):\n","        assert flowfile.exists()\n","        assert flowfile.suffix == '.png'\n","        flow_16bit = iio.imread(str(flowfile), plugin='PNG-FI')\n","        flow, valid2D = flow_16bit_to_float(flow_16bit)\n","        return flow, valid2D\n","\n","    @staticmethod\n","    def close_callback(h5f):\n","        h5f.close()\n","\n","    def get_image_width_height(self):\n","        return self.height, self.width\n","\n","    def __len__(self):\n","        # Ignore the first and last images as their own\n","        return len(self.timestamps_flow) # - 2\n","\n","    def rectify_events(self, x: np.ndarray, y: np.ndarray):\n","        # assert location in self.locations\n","        # From distorted to undistorted\n","        rectify_map = self.rectify_ev_map\n","        assert rectify_map.shape == (\n","            self.height, self.width, 2), rectify_map.shape\n","        assert x.max() < self.width\n","        assert y.max() < self.height\n","        return rectify_map[y, x]\n","\n","    def get_data(self, index) -> Dict[str, any]:\n","        # Adjust index to skip the first element\n","#         index += 1\n","\n","        ts_start: int = self.timestamps_flow[index] - self.delta_t_us\n","        ts_end: int = self.timestamps_flow[index]\n","\n","        file_index = self.indices[index]\n","\n","        output = {\n","            'file_index': file_index,\n","            'timestamp': self.timestamps_flow[index],\n","            'seq_name': self.seq_name\n","        }\n","        # Save sample for benchmark submission\n","        output['save_submission'] = file_index in self.idx_to_visualize\n","        output['visualize'] = self.visualize_samples\n","        event_data = self.event_slicer.get_events(\n","            ts_start, ts_end)\n","        p = event_data['p']\n","        t = event_data['t']\n","        x = event_data['x']\n","        y = event_data['y']\n","\n","        xy_rect = self.rectify_events(x, y)\n","        x_rect = xy_rect[:, 0]\n","        y_rect = xy_rect[:, 1]\n","\n","        if self.voxel_grid is None:\n","            raise NotImplementedError\n","        else:\n","            event_representation = self.events_to_voxel_grid(\n","                p, t, x_rect, y_rect)\n","            output['event_volume'] = event_representation\n","        output['name_map'] = self.name_idx\n","\n","        if self.load_gt:\n","            output['flow_gt'] = [torch.tensor(x) for x in self.load_flow(self.flow_png[index])]\n","            output['flow_gt'][0] = torch.moveaxis(output['flow_gt'][0], -1, 0)\n","            output['flow_gt'][1] = torch.unsqueeze(output['flow_gt'][1], 0)\n","\n","            flow_gt_shape = [tensor.shape for tensor in output['flow_gt']]\n","            zero_flow_gt = [torch.zeros_like(tensor) for tensor in output['flow_gt']]\n","\n","            # Load previous image\n","            if index > 0:\n","                output['prev_flow_gt'] = [torch.tensor(x) for x in self.load_flow(self.flow_png[index - 1])]\n","                output['prev_flow_gt'][0] = torch.moveaxis(output['prev_flow_gt'][0], -1, 0)\n","                output['prev_flow_gt'][1] = torch.unsqueeze(output['prev_flow_gt'][1], 0)\n","            else:\n","                output['prev_flow_gt'] = zero_flow_gt\n","\n","            # Load next image\n","            if index < len(self.timestamps_flow) - 1:\n","                output['next_flow_gt'] = [torch.tensor(x) for x in self.load_flow(self.flow_png[index + 1])]\n","                output['next_flow_gt'][0] = torch.moveaxis(output['next_flow_gt'][0], -1, 0)\n","                output['next_flow_gt'][1] = torch.unsqueeze(output['next_flow_gt'][1], 0)\n","            else:\n","                output['next_flow_gt'] = zero_flow_gt\n","\n","        return output\n","\n","    def __getitem__(self, idx):\n","        # Adjust index to skip the first element\n","        sample = self.get_data(idx) # idx + 1\n","\n","        if self.transforms:\n","            sample = self.transforms(sample)\n","\n","        return sample\n","\n","    def get_voxel_grid(self, idx):\n","\n","        if idx == 0:\n","            event_data = self.event_slicer.get_events(\n","                self.timestamps_flow[0] - self.delta_t_us, self.timestamps_flow[0])\n","        elif idx > 0 and idx <= self.__len__():\n","            event_data = self.event_slicer.get_events(\n","                self.timestamps_flow[idx-1], self.timestamps_flow[idx-1] + self.delta_t_us)\n","        else:\n","            raise IndexError\n","\n","        p = event_data['p']\n","        t = event_data['t']\n","        x = event_data['x']\n","        y = event_data['y']\n","\n","        xy_rect = self.rectify_events(x, y)\n","        x_rect = xy_rect[:, 0]\n","        y_rect = xy_rect[:, 1]\n","        return self.events_to_voxel_grid(p, t, x_rect, y_rect)\n","\n","    def get_event_count_image(self, ts_start, ts_end, num_bins, normalize=True):\n","        assert ts_end > ts_start\n","        delta_t_bin = (ts_end - ts_start) / num_bins\n","        ts_start_bin = np.linspace(\n","            ts_start, ts_end, num=num_bins, endpoint=False)\n","        ts_end_bin = ts_start_bin + delta_t_bin\n","        assert abs(ts_end_bin[-1] - ts_end) < 10.\n","        ts_end_bin[-1] = ts_end\n","\n","        event_count = torch.zeros(\n","            (num_bins, self.height, self.width), dtype=torch.float, requires_grad=False)\n","\n","        for i in range(num_bins):\n","            event_data = self.event_slicer.get_events(\n","                ts_start_bin[i], ts_end_bin[i])\n","            p = event_data['p']\n","            t = event_data['t']\n","            x = event_data['x']\n","            y = event_data['y']\n","\n","            t = (t - t[0]).astype('float32')\n","            t = (t/t[-1])\n","            x = x.astype('float32')\n","            y = y.astype('float32')\n","            pol = p.astype('float32')\n","            event_data_torch = {\n","                'p': torch.from_numpy(pol),\n","                't': torch.from_numpy(t),\n","                'x': torch.from_numpy(x),\n","                'y': torch.from_numpy(y),\n","            }\n","            x = event_data_torch['x']\n","            y = event_data_torch['y']\n","            xy_rect = self.rectify_events(x.int(), y.int())\n","            x_rect = torch.from_numpy(xy_rect[:, 0]).long()\n","            y_rect = torch.from_numpy(xy_rect[:, 1]).long()\n","            value = 2*event_data_torch['p']-1\n","            index = self.width*y_rect + x_rect\n","            mask = (x_rect < self.width) & (y_rect < self.height)\n","            event_count[i].put_(index[mask], value[mask], accumulate=True)\n","\n","        return event_count\n","\n","    @staticmethod\n","    def normalize_tensor(event_count):\n","        mask = torch.nonzero(event_count, as_tuple=True)\n","        if mask[0].size()[0] > 0:\n","            mean = event_count[mask].mean()\n","            std = event_count[mask].std()\n","            if std > 0:\n","                event_count[mask] = (event_count[mask] - mean) / std\n","            else:\n","                event_count[mask] = event_count[mask] - mean\n","        return event_count\n","\n","\n","class SequenceRecurrent(Sequence):\n","    def __init__(self, seq_path: Path, representation_type: RepresentationType, mode: str = 'test', delta_t_ms: int = 100,\n","                 num_bins: int = 15, transforms=None, sequence_length=1, name_idx=0, visualize=False, load_gt=False):\n","        super(SequenceRecurrent, self).__init__(seq_path, representation_type, mode, delta_t_ms, transforms=transforms,\n","                                                name_idx=name_idx, visualize=visualize, load_gt=load_gt)\n","        self.crop_size = self.transforms['randomcrop'] if 'randomcrop' in self.transforms else None\n","        self.sequence_length = sequence_length\n","        self.valid_indices = self.get_continuous_sequences()\n","\n","    def get_continuous_sequences(self):\n","        continuous_seq_idcs = []\n","        if self.sequence_length > 1:\n","            for i in range(len(self.timestamps_flow)-self.sequence_length+1):\n","                diff = self.timestamps_flow[i + self.sequence_length-1] - self.timestamps_flow[i]\n","                if diff < np.max([100000 * (self.sequence_length-1) + 1000, 101000]):\n","                    continuous_seq_idcs.append(i)\n","        else:\n","            for i in range(len(self.timestamps_flow)-1):\n","                diff = self.timestamps_flow[i+1] - self.timestamps_flow[i]\n","                if diff < np.max([100000 * (self.sequence_length-1) + 1000, 101000]):\n","                    continuous_seq_idcs.append(i)\n","        return continuous_seq_idcs\n","\n","    def __len__(self):\n","        return len(self.valid_indices)\n","\n","    def __getitem__(self, idx):\n","        assert idx >= 0\n","        assert idx < len(self)\n","\n","        # Valid index is the actual index we want to load, which guarantees a continuous sequence length\n","        valid_idx = self.valid_indices[idx]\n","\n","        sequence = []\n","        j = valid_idx\n","\n","        ts_cur = self.timestamps_flow[j]\n","        # Add first sample\n","        sample = self.get_data_sample(j)\n","        sequence.append(sample)\n","\n","        # Data augmentation according to first sample\n","        crop_window = None\n","        flip = None\n","        if 'crop_window' in sample.keys():\n","            crop_window = sample['crop_window']\n","        if 'flipped' in sample.keys():\n","            flip = sample['flipped']\n","\n","        for i in range(self.sequence_length-1):\n","            j += 1\n","            ts_old = ts_cur\n","            ts_cur = self.timestamps_flow[j]\n","            assert(ts_cur-ts_old < 100000 + 1000)\n","            sample = self.get_data_sample(\n","                j, crop_window=crop_window, flip=flip)\n","            sequence.append(sample)\n","\n","        # Check if the current sample is the first sample of a continuous sequence\n","        if idx == 0 or self.valid_indices[idx]-self.valid_indices[idx-1] != 1:\n","            sequence[0]['new_sequence'] = 1\n","            print(\"Timestamp {} is the first one of the next seq!\".format(\n","                self.timestamps_flow[self.valid_indices[idx]]))\n","        else:\n","            sequence[0]['new_sequence'] = 0\n","\n","        # random crop\n","        if self.crop_size is not None:\n","            i, j, h, w = RandomCrop.get_params(\n","                sample[\"event_volume_old\"], output_size=self.crop_size)\n","            keys_to_crop = [\"event_volume_old\", \"event_volume_new\",\n","                            \"flow_gt_event_volume_old\", \"flow_gt_event_volume_new\",\n","                            \"flow_gt_next\",]\n","\n","            for sample in sequence:\n","                for key, value in sample.items():\n","                    if key in keys_to_crop:\n","                        if isinstance(value, torch.Tensor):\n","                            sample[key] = tf.functional.crop(value, i, j, h, w)\n","                        elif isinstance(value, list) or isinstance(value, tuple):\n","                            sample[key] = [tf.functional.crop(v, i, j, h, w) for v in value]\n","        return sequence\n","\n","\n","class DatasetProvider:\n","    def __init__(self, dataset_path: Path, representation_type: RepresentationType, delta_t_ms: int = 100, num_bins=4, config=None, visualize=False, transforms=None):\n","        test_path = Path(os.path.join(dataset_path, 'test'))\n","        train_path = Path(os.path.join(dataset_path, 'train'))\n","        assert dataset_path.is_dir(), str(dataset_path)\n","        assert test_path.is_dir(), str(test_path)\n","        assert delta_t_ms == 100\n","        self.config = config\n","        self.name_mapper_test = []\n","\n","        if transforms:\n","            self.transforms = transforms\n","        else:\n","            self.transforms = tf.Compose([\n","                transforms.ToTensor(),  # Convert image to PyTorch tensor\n","            ])\n","\n","        # Assemble test sequences\n","        test_sequences = list()\n","        for child in test_path.iterdir():\n","            self.name_mapper_test.append(str(child).split(\"/\")[-1])\n","            test_sequences.append(Sequence(child, representation_type, 'test', delta_t_ms, num_bins,\n","                                               name_idx=len(self.name_mapper_test) - 1,\n","                                               visualize=visualize,\n","                                               transforms=self.transforms))\n","\n","        self.test_dataset = torch.utils.data.ConcatDataset(test_sequences)\n","\n","        # Assemble train sequences\n","        available_seqs = os.listdir(train_path)\n","\n","        seqs = available_seqs\n","\n","        train_sequences: list[Sequence] = []\n","        for seq in seqs:\n","            extra_arg = dict()\n","            train_sequences.append(Sequence(Path(train_path) / seq,\n","                                   representation_type=representation_type, mode=\"train\",\n","                                   load_gt=True, **extra_arg, transforms=self.transforms))\n","            self.train_dataset: torch.utils.data.ConcatDataset[Sequence] = torch.utils.data.ConcatDataset(train_sequences)\n","\n","    def get_test_dataset(self):\n","        return self.test_dataset\n","\n","    def get_train_dataset(self):\n","        return self.train_dataset\n","\n","    def get_name_mapping_test(self):\n","        return self.name_mapper_test\n","\n","    def summary(self, logger):\n","        logger.write_line(\n","            \"================================== Dataloader Summary ====================================\", True)\n","        logger.write_line(\"Loader Type:\\t\\t\" + self.__class__.__name__, True)\n","        logger.write_line(\"Number of Voxel Bins: {}\".format(\n","            self.test_dataset.datasets[0].num_bins), True)\n","        logger.write_line(\"Number of Train Sequences: {}\".format(\n","            len(self.train_dataset)), True)\n","\n","def train_collate(sample_list):\n","    batch = dict()\n","    for field_name in sample_list[0]:\n","        if field_name == 'timestamp':\n","            batch['timestamp'] = [sample[field_name] for sample in sample_list]\n","        if field_name == 'seq_name':\n","            batch['seq_name'] = [sample[field_name] for sample in sample_list]\n","        if field_name == 'new_sequence':\n","            batch['new_sequence'] = [sample[field_name]\n","                                     for sample in sample_list]\n","        if field_name.startswith(\"event_volume\"):\n","            batch[field_name] = torch.stack(\n","                [sample[field_name] for sample in sample_list])\n","        if field_name.startswith(\"flow_gt\") or field_name.startswith('prev_flow_gt') or field_name.startswith('next_flow_gt'):\n","            if all(field_name in x for x in sample_list):\n","                batch[field_name] = torch.stack(\n","                    [sample[field_name][0] for sample in sample_list])\n","                batch[field_name + '_valid_mask'] = torch.stack(\n","                    [sample[field_name][1] for sample in sample_list])\n","\n","    return batch\n","\n","\n","def rec_train_collate(sample_list):\n","    seq_length = len(sample_list[0])\n","    seq_of_batch = []\n","    for i in range(seq_length):\n","        seq_of_batch.append(train_collate(\n","            [sample[i] for sample in sample_list]))\n","    return seq_of_batch"],"metadata":{"id":"BMr6cf3sGb7Q","execution":{"iopub.status.busy":"2024-07-16T12:26:48.437496Z","iopub.execute_input":"2024-07-16T12:26:48.437753Z","iopub.status.idle":"2024-07-16T12:26:48.509188Z","shell.execute_reply.started":"2024-07-16T12:26:48.437729Z","shell.execute_reply":"2024-07-16T12:26:48.508601Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1721136267569,"user_tz":-540,"elapsed":5,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## PCLNet"],"metadata":{"id":"5MpHeit75O-h"}},{"cell_type":"markdown","source":["## From the github"],"metadata":{"id":"W-FQAttFXDKX"}},{"cell_type":"code","source":["import torch.utils.model_zoo as model_zoo\n","\n","__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n","           'resnet152']\n","\n","model_urls = {\n","    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n","    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n","    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n","    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n","    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n","}\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=1, bias=False)\n","\n","\n","def conv1x1(in_planes, out_planes, stride=1):\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = conv1x1(inplanes, planes)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = conv1x1(planes, planes * self.expansion)\n","        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","\n","    def __init__(self, block, layers, num_classes=1000):\n","        super(ResNet, self).__init__()\n","        self.inplanes = 64\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n","                               bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 64, layers[0])\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n","        # self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        # self.fc = nn.Linear(512 * block.expansion, num_classes)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","    def _make_layer(self, block, planes, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                conv1x1(self.inplanes, planes * block.expansion, stride),\n","                nn.BatchNorm2d(planes * block.expansion),\n","            )\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride, downsample))\n","        self.inplanes = planes * block.expansion\n","        for _ in range(1, blocks):\n","            layers.append(block(self.inplanes, planes))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        endpoint = []\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        endpoint.append(x) # output here\n","\n","        x = self.maxpool(x)\n","        x = self.layer1(x)\n","        endpoint.append(x)\n","        x = self.layer2(x)\n","        endpoint.append(x)\n","        x = self.layer3(x)\n","        endpoint.append(x)\n","        x = self.layer4(x)\n","        endpoint.append(x)\n","\n","        return endpoint\n","\n","\n","def resnet18(pretrained=False, **kwargs):\n","    \"\"\"Constructs a ResNet-18 model.\n","\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","    \"\"\"\n","    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n","    if pretrained:\n","        state = model.state_dict()\n","        state_ckp = model_zoo.load_url(model_urls['resnet18'])\n","        cnt = 0\n","        for k, val in state_ckp.items():\n","            if k in state.keys():\n","                state[k] = val\n","                cnt += 1\n","        model.load_state_dict(state)\n","        print (\"RestNet checkpoint loaded: %d\" % cnt)\n","    return model\n","\n","\n","def resnet34(pretrained=False, **kwargs):\n","    \"\"\"Constructs a ResNet-34 model.\n","\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","    \"\"\"\n","    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n","    if pretrained:\n","        state = model.state_dict()\n","        state_ckp = model_zoo.load_url(model_urls['resnet34'])\n","        cnt = 0\n","        for k, val in state_ckp.items():\n","            if k in state.keys():\n","                state[k] = val\n","                cnt += 1\n","        model.load_state_dict(state)\n","        print (\"RestNet checkpoint loaded: %d\" % cnt)\n","    return model\n","\n","\n","def resnet50(pretrained=False, **kwargs):\n","    \"\"\"Constructs a ResNet-50 model.\n","\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","    \"\"\"\n","    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n","    if pretrained:\n","        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n","    return model\n","\n","\n","def resnet101(pretrained=False, **kwargs):\n","    \"\"\"Constructs a ResNet-101 model.\n","\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","    \"\"\"\n","    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n","    if pretrained:\n","        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n","    return model\n","\n","\n","def resnet152(pretrained=False, **kwargs):\n","    \"\"\"Constructs a ResNet-152 model.\n","\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","    \"\"\"\n","    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n","    if pretrained:\n","        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n","    return model"],"metadata":{"id":"8R-omgjZS8n0","execution":{"iopub.status.busy":"2024-07-16T12:26:48.600851Z","iopub.execute_input":"2024-07-16T12:26:48.601087Z","iopub.status.idle":"2024-07-16T12:26:48.624949Z","shell.execute_reply.started":"2024-07-16T12:26:48.601063Z","shell.execute_reply":"2024-07-16T12:26:48.624261Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1721136267569,"user_tz":-540,"elapsed":3,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["from torch.autograd import Variable\n","\n","class ConvLSTMCell(nn.Module):\n","    def __init__(self, input_channels, hidden_channels, kernel_size, bias=True):\n","        super(ConvLSTMCell, self).__init__()\n","\n","        assert hidden_channels % 2 == 0\n","\n","        self.input_channels = input_channels\n","        self.hidden_channels = hidden_channels\n","        self.bias = bias\n","        self.kernel_size = kernel_size\n","\n","        self.padding = int((kernel_size - 1) / 2)\n","        self.Gates = nn.Conv2d(self.input_channels + self.hidden_channels , 4*self.hidden_channels,\n","                self.kernel_size, 1, self.padding, bias=True)\n","\n","    def forward(self, x, h, c):\n","\n","        stacked_inputs = torch.cat((x, h), 1)\n","        gates = self.Gates(stacked_inputs)\n","\n","        # chunk across the channel dimension\n","        xi, xf, xo, xg = gates.chunk(4, 1)\n","\n","        # apply sigmoid non linearity\n","        xi = torch.sigmoid(xi)\n","        xf = torch.sigmoid(xf)\n","        xo = torch.sigmoid(xo)\n","        xg = torch.tanh(xg)\n","\n","        # compute current cell and hidden state\n","        c = (xf * c) + (xi * xg)\n","        h = xo * torch.tanh(c)\n","\n","        return h, c\n","\n","    def init_hidden(self, batch_size, hidden, shape):\n","        return (Variable(torch.zeros(batch_size, hidden, shape[0], shape[1])),\n","                Variable(torch.zeros(batch_size, hidden, shape[0], shape[1])))\n","\n","\n","class ConvLSTM(nn.Module):\n","    def __init__(self, input_channels, hidden_channels, kernel_size, step=1, effective_step=[1], bias=True):\n","        super(ConvLSTM, self).__init__()\n","        self.input_channels = [input_channels] + hidden_channels\n","        self.hidden_channels = hidden_channels\n","        self.kernel_size = kernel_size\n","        self.num_layers = len(hidden_channels)\n","        self.step = step\n","        self.bias = bias\n","        self.effective_step = effective_step\n","        self._all_layers = []\n","        for i in range(self.num_layers):\n","            name = 'cell{}'.format(i)\n","            cell = ConvLSTMCell(self.input_channels[i], self.hidden_channels[i], self.kernel_size, self.bias)\n","            setattr(self, name, cell)\n","            self._all_layers.append(cell)\n","\n","    def forward(self, input):\n","        #input : (num, seq_len, channel, H,W)\n","        internal_state = []\n","        outputs = []\n","        for step in range(self.step):\n","            x = input[:, step, :,:,:]\n","            for i in range(self.num_layers):\n","                # all cells are initialized in the first step\n","                name = 'cell{}'.format(i)\n","                if step == 0:\n","                    bsize, _, height, width = x.size()\n","                    (h, c) = getattr(self, name).init_hidden(batch_size=bsize, hidden=self.hidden_channels[i],\n","                            shape=(height, width))\n","                    internal_state.append((h, c))\n","\n","                # do forward\n","                (h, c) = internal_state[i]\n","                x, new_c = getattr(self, name)(x, h, c)\n","                internal_state[i] = (x, new_c)\n","            # only record effective steps\n","            if step in self.effective_step:\n","                outputs.append(x)\n","        return outputs, (x, new_c)"],"metadata":{"id":"yD2nx5HATAgK","execution":{"iopub.status.busy":"2024-07-16T12:26:48.626266Z","iopub.execute_input":"2024-07-16T12:26:48.626524Z","iopub.status.idle":"2024-07-16T12:26:48.643278Z","shell.execute_reply.started":"2024-07-16T12:26:48.626501Z","shell.execute_reply":"2024-07-16T12:26:48.642649Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1721136267570,"user_tz":-540,"elapsed":4,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1):\n","    if type(in_planes) == np.int64:\n","        in_planes = np.asscalar(in_planes)\n","    return nn.Sequential(\n","        nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n","                  padding=padding, dilation=dilation, bias=True),\n","        nn.LeakyReLU(0.1))\n","\n","\n","def predict_flow(in_planes):\n","    if type(in_planes) == np.int64:\n","        in_planes = np.asscalar(in_planes)\n","    return nn.Conv2d(in_planes, 2, kernel_size=3, stride=1, padding=1, bias=True)\n","\n","\n","def deconv(in_planes, out_planes, kernel_size=4, stride=2, padding=1):\n","    if type(in_planes) == np.int64:\n","        in_planes = np.asscalar(in_planes)\n","    return nn.ConvTranspose2d(in_planes, out_planes, kernel_size, stride, padding, bias=True)\n","\n","def in_f(flow):\n","    return nn_F.interpolate(flow, size=(480, 640), mode='bilinear', align_corners=False)\n","\n","\n","class PCLNet(nn.Module):\n","    \"\"\"\n","    PCLNet: Unsupervised Learning for Optical Flow Estimation Using Pyramid Convolution LSTM\n","    Author: Shuosen Guan\n","    \"\"\"\n","\n","    def __init__(self, args):\n","\n","        super(PCLNet, self).__init__()\n","        self.args = args\n","\n","        snippet_len = args.snippet_len\n","        self.feature_net = eval(args.backbone)(pretrained=True, num_classes=args.class_num)\n","        if args.freeze_vgg:\n","            for p in self.feature_net.parameters():\n","                p.required_grad = False\n","            print(\"[>>>> Feature head frozen.<<<<]\")\n","\n","        # Motion Encoding\n","        # in_size: 1/2\n","        self.clstm_encoder_1 = ConvLSTM(input_channels=64, hidden_channels=[64],\n","                                        kernel_size=3, step=snippet_len, effective_step=list(range(snippet_len)))\n","        # in_size: 1/4\n","        self.clstm_encoder_2 = ConvLSTM(input_channels=64, hidden_channels=[64],\n","                                        kernel_size=3, step=snippet_len, effective_step=list(range(snippet_len)))\n","        # in_size: 1/8\n","        self.clstm_encoder_3 = ConvLSTM(input_channels=128, hidden_channels=[128],\n","                                        kernel_size=3, step=snippet_len, effective_step=list(range(snippet_len)))\n","        # in_size: 1/16\n","        self.clstm_encoder_4 = ConvLSTM(input_channels=256, hidden_channels=[256],\n","                                        kernel_size=3, step=snippet_len, effective_step=list(range(snippet_len)))\n","\n","        self.conv_B1    = conv(64, 64, stride=1, kernel_size=3, padding=1)\n","        self.conv_S1_1  = conv(64, 64, stride=1, kernel_size=3, padding=1)\n","        self.conv_S1_2  = conv(64, 64, stride=1, kernel_size=3, padding=1)\n","        self.conv_D1    = conv(64, 64, stride=2)\n","        self.Pool1      = nn.MaxPool2d(8, 8)\n","\n","        self.conv_B2    = conv(64, 64, stride=1, kernel_size=3, padding=1)\n","        self.conv_S2_1  = conv(64 + 64, 128, stride=1, kernel_size=3, padding=1)\n","        self.conv_S2_2  = conv(128, 128, stride=1, kernel_size=3, padding=1)\n","        self.conv_D2    = conv(128, 64, stride=2)\n","        self.Pool2      = nn.MaxPool2d(4, 4)\n","\n","        self.conv_B3    = conv(128, 128, stride=1, kernel_size=3, padding=1)\n","        self.conv_S3_1  = conv(128 + 64, 128, stride=1, kernel_size=3, padding=1)\n","        self.conv_S3_2  = conv(128, 128, stride=1, kernel_size=3, padding=1)\n","        self.conv_D3    = conv(128, 64, stride=2)\n","        self.Pool3      = nn.MaxPool2d(2, 2)\n","\n","        self.conv_B4    = conv(256, 128, stride=1, kernel_size=3, padding=1)\n","        self.conv_S4_1  = conv(128 + 64, 128, stride=1, kernel_size=3, padding=1)\n","        self.conv_S4_2  = conv(128, 128, stride=1, kernel_size=3, padding=1)\n","\n","        # Motion feature\n","        self.conv_M = conv((64 + 128 + 128 + 128), 256, stride=1, kernel_size=3, padding=1)\n","\n","        # Motion reconstruction\n","        if self.args.couple:\n","            rec_in_size = [0, 64 + 64 + 2, 128 + 128 + 2, 128 + 196 + 2, 128 + 256]\n","        else:\n","            rec_in_size = [0, 64 + 2, 128 + 2, 196 + 2, 256]\n","\n","        self.conv_4     = conv(rec_in_size[4], 256)\n","        self.pred_flow4 = predict_flow(256)\n","        self.up_flow4   = deconv(2, 2)\n","        self.up_feat4   = deconv(256, 196)\n","\n","        self.conv_3     = conv(rec_in_size[3], 196)\n","        self.pred_flow3 = predict_flow(196)\n","        self.up_flow3   = deconv(2, 2)\n","        self.up_feat3   = deconv(196, 128)\n","\n","        self.conv_2     = conv(rec_in_size[2], 96)\n","        self.pred_flow2 = predict_flow(96)\n","        self.up_flow2   = conv(2, 2)\n","        self.up_feat2   = conv(96, 64)\n","\n","        self.conv_1     = conv(rec_in_size[1], 64)\n","        self.pred_flow1 = predict_flow(64)\n","\n","        self.dc_conv1 = conv(64, 64, kernel_size=3, stride=1, padding=1, dilation=1)\n","        self.dc_conv2 = conv(64, 64, kernel_size=3, stride=1, padding=2, dilation=2)\n","        self.dc_conv3 = conv(64, 64, kernel_size=3, stride=1, padding=4, dilation=4)\n","        self.dc_conv4 = conv(64, 64, kernel_size=3, stride=1, padding=8, dilation=8)\n","        self.dc_conv5 = conv(64, 64, kernel_size=3, stride=1, padding=16, dilation=16)\n","        self.dc_conv6 = conv(64, 32, kernel_size=3, stride=1, padding=1, dilation=1)\n","        self.dc_conv7 = predict_flow(32)\n","\n","        # for some reason, conv returns shape torch.Size([B*T, 3, 482, 642])\n","        self.reduce_channels = nn.Conv2d(4, 3, kernel_size=1)\n","\n","\n","    def forward(self, x):\n","\n","        if x.dim() == 6:    # (batch_size, K, snippet_len, channel, H, W)\n","            batch_size, K, snippet_len, channel, H, W = x.size()\n","        elif x.dim() == 5:  # (batch_size, snippet_len, channel, H, W)\n","            batch_size, snippet_len, channel, H, W = x.size()\n","            K = 1\n","        elif x.dim() == 4:  # (batch_size, channel * snippet_len, H, W)\n","            batch_size, _channels, H, W = x.size()\n","            K, channel = 1, 3\n","            snippet_len = _channels // channel\n","        else:\n","            raise RuntimeError('Input format not suppored!')\n","\n","        x = x.contiguous().view(-1, channel, H, W)\n","        if channel > 3:\n","            x = self.reduce_channels(x)\n","\n","        la1, la2, la3, la4, _ = self.feature_net(x)\n","\n","        la1 = la1.view((-1, snippet_len) + la1.size()[1:])\n","        la2 = la2.view((-1, snippet_len) + la2.size()[1:])\n","        la3 = la3.view((-1, snippet_len) + la3.size()[1:])\n","        la4 = la4.view((-1, snippet_len) + la4.size()[1:])\n","        # la5 = la5.view((-1, snippet_len) + la5.size()[1:])\n","\n","        h1, _ = self.clstm_encoder_1(la1)\n","        h2, _ = self.clstm_encoder_2(la2)\n","        h3, _ = self.clstm_encoder_3(la3)\n","        h4, _ = self.clstm_encoder_4(la4)\n","        # list for each step (batch_size * K, channel, H, W)\n","\n","        # (batch_size * K*(snippet_len -1), channel, H, W)\n","        h1 = torch.stack(h1[1:], 1).view((-1,) + h1[0].size()[-3:])\n","        h2 = torch.stack(h2[1:], 1).view((-1,) + h2[0].size()[-3:])\n","        h3 = torch.stack(h3[1:], 1).view((-1,) + h3[0].size()[-3:])\n","        h4 = torch.stack(h4[1:], 1).view((-1,) + h4[0].size()[-3:])\n","\n","        x1 = self.conv_B1(h1)\n","        x1 = self.conv_S1_2(self.conv_S1_1(x1))\n","\n","        x2 = torch.cat((self.conv_B2(h2), self.conv_D1(x1)), 1)\n","        x2 = self.conv_S2_2(self.conv_S2_1(x2))\n","\n","        x3 = torch.cat((self.conv_B3(h3), self.conv_D2(x2)), 1)\n","        x3 = self.conv_S3_2(self.conv_S3_1(x3))\n","\n","        x4 = torch.cat((self.conv_B4(h4), self.conv_D3(x3)), 1)\n","        x4 = self.conv_S4_2(self.conv_S4_1(x4))\n","\n","        xm = self.conv_M(torch.cat((self.Pool1(x1), self.Pool2(x2), self.Pool3(x3), x4), 1))\n","\n","        rec_x4 = torch.cat((x4, xm), 1) if self.args.couple else xm\n","        x = self.conv_4(rec_x4)\n","        flow4 = self.pred_flow4(x)\n","        up_flow4 = self.up_flow4(flow4)\n","        up_feat4 = self.up_feat4(x)\n","\n","        rec_x3 = torch.cat((x3, up_feat4, up_flow4), 1) if self.args.couple else torch.cat((up_feat4, up_flow4), 1)\n","        x = self.conv_3(rec_x3)\n","        flow3 = self.pred_flow3(x)\n","        up_flow3 = self.up_flow3(flow3)\n","        up_feat3 = self.up_feat3(x)\n","\n","        rec_x2 = torch.cat((x2, up_feat3, up_flow3), 1) if self.args.couple else torch.cat((up_feat3, up_flow3), 1)\n","        x = self.conv_2(rec_x2)\n","        flow2 = self.pred_flow2(x)\n","        up_flow2 = self.up_flow2(flow2)\n","        up_feat2 = self.up_feat2(x)\n","\n","        rec_x1 = torch.cat((x1, up_feat2, up_flow2), 1) if self.args.couple else torch.cat((up_feat2, up_flow2), 1)\n","        x = self.conv_1(rec_x1)\n","        flow1 = self.pred_flow1(x)\n","\n","        x = self.dc_conv4(self.dc_conv3(self.dc_conv2(self.dc_conv1(x))))\n","        flow1 += self.dc_conv7(self.dc_conv6(self.dc_conv5(x)))\n","\n","        re_dict = {\n","            'flow0': flow4,\n","            'flow1': flow3,\n","            'flow2': flow2,\n","            'flow3': flow1\n","        }\n","\n","        # output size: (batch_size, K, snippet_len -1 , C,H,W)\n","\n","#         flow_pyramid = [flo.view((batch_size, K, snippet_len - 1,) + flo.size()[-3:])\n","#                         for flo in [flow1, flow2, flow3, flow4]]\n","#         re_dict = {}\n","#         re_dict['flow_pyramid'] = flow_pyramid\n","\n","#         return re_dict\n","\n","        flow1 = flow1.view((batch_size, K, snippet_len - 1,) + flow1.size()[-3:])\n","        flow2 = flow2.view((batch_size, K, snippet_len - 1,) + flow2.size()[-3:])\n","        flow3 = flow3.view((batch_size, K, snippet_len - 1,) + flow3.size()[-3:])\n","        flow4 = flow4.view((batch_size, K, snippet_len - 1,) + flow4.size()[-3:])\n","\n","        flow1_arr = [in_f(flow1[:, :, i, :, :, :].squeeze(1).squeeze(1)) for i in range(flow1.size(2))]\n","        flow2_arr = [in_f(flow2[:, :, i, :, :, :].squeeze(1).squeeze(1)) for i in range(flow2.size(2))]\n","        flow3_arr = [in_f(flow3[:, :, i, :, :, :].squeeze(1).squeeze(1)) for i in range(flow3.size(2))]\n","        flow4_arr = [in_f(flow4[:, :, i, :, :, :].squeeze(1).squeeze(1)) for i in range(flow4.size(2))]\n","\n","        combined_flow = [torch.mean(torch.stack([f1, f2, f3, f4], dim=0), dim=0)\n","                             for f1, f2, f3, f4 in zip(flow1_arr, flow2_arr, flow3_arr, flow4_arr)]\n","        return re_dict, combined_flow"],"metadata":{"id":"yu7rm0heTLXj","execution":{"iopub.status.busy":"2024-07-16T12:26:48.644058Z","iopub.execute_input":"2024-07-16T12:26:48.644290Z","iopub.status.idle":"2024-07-16T12:26:48.679293Z","shell.execute_reply.started":"2024-07-16T12:26:48.644266Z","shell.execute_reply":"2024-07-16T12:26:48.678668Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1721136267987,"user_tz":-540,"elapsed":421,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["## Arguments & Definition"],"metadata":{"id":"u9miP9yJXFRV"}},{"cell_type":"code","source":["import argparse\n","args = argparse.Namespace(\n","    name='pclnet',\n","    snippet_len=3,\n","    backbone='resnet18',\n","    class_num=101,\n","    freeze_vgg=True,\n","    couple=False,\n","\n","    lr=0.01, # 2e-5\n","    num_steps=100000,\n","    batch_size=32, # default: 16\n","    image_size=[480, 640],\n","    mixed_precision=False,\n","    iters=12,\n","    wdecay=0.00005,\n","    epsilon=1e-8,\n","    clip=1.0,\n","    dropout=0.0,\n","    gamma=0.8,\n","    add_noise=False,\n","    seed=27,\n","    dataset_path='data/',\n",")"],"metadata":{"id":"QL51N2RHAqGh","execution":{"iopub.status.busy":"2024-07-16T12:26:48.680779Z","iopub.execute_input":"2024-07-16T12:26:48.681006Z","iopub.status.idle":"2024-07-16T12:26:48.694843Z","shell.execute_reply.started":"2024-07-16T12:26:48.680983Z","shell.execute_reply":"2024-07-16T12:26:48.693839Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1721136267987,"user_tz":-540,"elapsed":1,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["del model"],"metadata":{"id":"cFeveiNP0lbX","executionInfo":{"status":"ok","timestamp":1721138046150,"user_tz":-540,"elapsed":421,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["model = torch.nn.DataParallel(PCLNet(args).cpu())"],"metadata":{"id":"jObX__fv-JYD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b12a1788-b5f0-4113-c3ba-d98433f3329a","execution":{"iopub.status.busy":"2024-07-16T12:26:48.695689Z","iopub.execute_input":"2024-07-16T12:26:48.695930Z","iopub.status.idle":"2024-07-16T12:26:49.462876Z","shell.execute_reply.started":"2024-07-16T12:26:48.695905Z","shell.execute_reply":"2024-07-16T12:26:49.461787Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1721138054430,"user_tz":-540,"elapsed":969,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["RestNet checkpoint loaded: 100\n","[>>>> Feature head frozen.<<<<]\n"]}]},{"cell_type":"code","source":["model.load_state_dict(torch.load('models/pclnet_batch50.pth'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sMCIP26LtSAn","executionInfo":{"status":"ok","timestamp":1721138059972,"user_tz":-540,"elapsed":3993,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}},"outputId":"2d9cef46-9600-47e7-d22c-d60ab20334af"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","source":["## Image Preprocessing"],"metadata":{"id":"Asa6m7ccKl4E"}},{"cell_type":"code","source":["from sklearn.decomposition import PCA, SparsePCA, TruncatedSVD\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","\n","class DimensionalityReduction:\n","    def __init__(self, n_components, scaler=MinMaxScaler(), red_technique='pca'):\n","        self.scaler = scaler # if None, then doesn't perform any scaling\n","        self.n_components = n_components\n","\n","        match red_technique:\n","            case 'pca':\n","                self.technique = PCA(n_components=n_components)\n","            case 'sparsepca':\n","                self.technique = SparsePCA(n_components=n_components)\n","            case 'tsvd':\n","                self.technique = TruncatedSVD(n_components=n_components)\n","            case _:\n","                raise NotImplementedError\n","\n","    def __call__(self, image_tensor: torch.Tensor):\n","        # this is for a single image, not a batch of images\n","        C, H, W = image_tensor.shape\n","\n","        tensor_reshaped = image_tensor.cpu().numpy().reshape(C, -1).T\n","        if self.scaler is not None:\n","            tensor_reshaped = self.scaler.fit_transform(tensor_reshaped)\n","\n","        reduced_tensor_reshaped = self.technique.fit_transform(tensor_reshaped)\n","        reduced_tensor_reshaped = reduced_tensor_reshaped.T.reshape(self.n_components, H, W)\n","\n","        reduced_tensor = torch.tensor(reduced_tensor_reshaped, device=image_tensor.device)\n","        return reduced_tensor"],"metadata":{"id":"J3qppdpsKrZc","execution":{"iopub.status.busy":"2024-07-16T12:26:49.463986Z","iopub.execute_input":"2024-07-16T12:26:49.464257Z","iopub.status.idle":"2024-07-16T12:26:51.357078Z","shell.execute_reply.started":"2024-07-16T12:26:49.464219Z","shell.execute_reply":"2024-07-16T12:26:51.356091Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1721136369603,"user_tz":-540,"elapsed":2790,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["```python\n","a = torch.rand((4, 480, 640))\n","b = DimensionalityReduction(n_components=3, scaler=MinMaxScaler(), red_technique='tsvd')(a)\n","b.shape\n","```"],"metadata":{"id":"PGpEV1M23pyC"}},{"cell_type":"code","source":["# Needs to have 3 channels or color passed in\n","# runs into the problem of loss of information in dimensionality reduction\n","class HistogramEqualization:\n","    def __call__(self, img):\n","        return tf.functional.equalize(img)"],"metadata":{"id":"jt6kTo6xKip7","execution":{"iopub.status.busy":"2024-07-16T12:26:51.358171Z","iopub.execute_input":"2024-07-16T12:26:51.358703Z","iopub.status.idle":"2024-07-16T12:26:51.362792Z","shell.execute_reply.started":"2024-07-16T12:26:51.358673Z","shell.execute_reply":"2024-07-16T12:26:51.362020Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1721136369603,"user_tz":-540,"elapsed":2,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["class RandomCrop(object):\n","    def __init__(self, size):\n","        self.size = size\n","\n","    def __call__(self, clip):\n","        i, j, h, w = tf.RandomCrop.get_params(clip[0], output_size=self.size)\n","        return [F.crop(img, i, j, h, w) for img in clip]\n","\n","class RandomHorizontalFlip(object):\n","    def __call__(self, clip):\n","        if random.random() > 0.5:\n","            return [F.hflip(img) for img in clip]\n","        return clip\n","\n","class RandomVerticalFlip(object):\n","    def __call__(self, clip):\n","        if random.random() > 0.5:\n","            return [F.vflip(img) for img in clip]\n","        return clip\n","\n","class RandomRescale(object):\n","    def __init__(self, scale_range):\n","        self.scale_range = scale_range\n","\n","    def __call__(self, clip):\n","        scale_factor = random.uniform(*self.scale_range)\n","        return [F.resize(img, [int(img.shape[0] * scale_factor), int(img.shape[1] * scale_factor)]) for img in clip]\n","\n","class Resize(object):\n","    def __init__(self, size):\n","        self.size = size\n","\n","    def __call__(self, clip):\n","        return [F.resize(img, self.size) for img in clip]\n","\n","clip_transform = tf.Compose([\n","    RandomCrop((256, 256)),\n","    RandomHorizontalFlip(),\n","    RandomVerticalFlip(),\n","    RandomRescale((0.8, 1.2)),\n","    Resize((480, 640)),\n","])"],"metadata":{"execution":{"iopub.status.busy":"2024-07-16T12:26:51.363783Z","iopub.execute_input":"2024-07-16T12:26:51.364011Z","iopub.status.idle":"2024-07-16T12:26:51.380186Z","shell.execute_reply.started":"2024-07-16T12:26:51.363988Z","shell.execute_reply":"2024-07-16T12:26:51.379466Z"},"trusted":true,"id":"_xcaEaEBtBPO","executionInfo":{"status":"ok","timestamp":1721136369603,"user_tz":-540,"elapsed":2,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["class CombinedTransform:\n","    def __init__(self, transform=tf.Compose([ tf.ToTensor() ])):\n","        self.transform = transform\n","\n","    def __call__(self, flow_dict):\n","        seed = np.random.randint(2147483647)\n","\n","        # Just in case I decide to add more feature columns\n","        feaure_flow_columns = [c for c in flow_dict.keys() if 'event_volume' in c]\n","\n","        for col in feaure_flow_columns:\n","            torch.manual_seed(seed)\n","\n","            if type(flow_dict[col]) == list:\n","                flow_dict[col] = [ self.transform(img) for img in flow_dict[col] ]\n","            else:\n","                flow_dict[col] = self.transform(flow_dict[col])\n","\n","        return flow_dict\n","\n","combined_transform = CombinedTransform(\n","    transform=tf.Compose([\n","        tf.GaussianBlur(kernel_size=(5, 5)),\n","    ])\n","#     transform=clip_transform\n",")"],"metadata":{"id":"q2pCHbMaGb7T","execution":{"iopub.status.busy":"2024-07-16T12:26:51.381090Z","iopub.execute_input":"2024-07-16T12:26:51.381364Z","iopub.status.idle":"2024-07-16T12:26:51.390955Z","shell.execute_reply.started":"2024-07-16T12:26:51.381338Z","shell.execute_reply":"2024-07-16T12:26:51.390226Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1721136369603,"user_tz":-540,"elapsed":1,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["## `main.py`"],"metadata":{"id":"-W_HpclqGb7T"}},{"cell_type":"markdown","source":["Instead of changing the `Sequence`, just change how the data are loaded. Or iterate over the dataloader so that you use many images at once."],"metadata":{"id":"3lufwoSn32eN"}},{"cell_type":"markdown","source":["## Data Formatting"],"metadata":{"id":"4DiXjrXfuOUR"}},{"cell_type":"code","source":["class RepresentationType(Enum):\n","    VOXEL = auto()\n","    STEPAN = auto()\n","\n","def set_seed(seed):\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","\n","def compute_epe_error(pred_flow: torch.Tensor, gt_flow: torch.Tensor):\n","    '''\n","    end-point-error (ground truthと予測値の二乗誤差)を計算\n","    pred_flow: torch.Tensor, Shape: torch.Size([B, 2, 480, 640]) => 予測したオプティカルフローデータ\n","    gt_flow: torch.Tensor, Shape: torch.Size([B, 2, 480, 640]) => 正解のオプティカルフローデータ\n","    '''\n","    epe = torch.mean(torch.mean(torch.norm(pred_flow - gt_flow, p=2, dim=1), dim=(1, 2)), dim=0)\n","    return epe\n","\n","def save_optical_flow_to_npy(flow: torch.Tensor, file_name: str):\n","    '''\n","    optical flowをnpyファイルに保存\n","    flow: torch.Tensor, Shape: torch.Size([2, 480, 640]) => オプティカルフローデータ\n","    file_name: str => ファイル名\n","    '''\n","    np.save(f\"{file_name}.npy\", flow.cpu().numpy())"],"metadata":{"id":"k-O8nvDQGb7T","execution":{"iopub.status.busy":"2024-07-16T12:26:51.393473Z","iopub.execute_input":"2024-07-16T12:26:51.393749Z","iopub.status.idle":"2024-07-16T12:26:51.406802Z","shell.execute_reply.started":"2024-07-16T12:26:51.393703Z","shell.execute_reply":"2024-07-16T12:26:51.405936Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1721136395585,"user_tz":-540,"elapsed":431,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["set_seed(args.seed)\n","\n","'''\n","    ディレクトリ構造:\n","\n","    data\n","    ├─test\n","    |  ├─test_city\n","    |  |    ├─events_left\n","    |  |    |   ├─events.h5\n","    |  |    |   └─rectify_map.h5\n","    |  |    └─forward_timestamps.txt\n","    └─train\n","        ├─zurich_city_11_a\n","        |    ├─events_left\n","        |    |       ├─ events.h5\n","        |    |       └─ rectify_map.h5\n","        |    ├─ flow_forward\n","        |    |       ├─ 000134.png\n","        |    |       |.....\n","        |    └─ forward_timestamps.txt\n","        ├─zurich_city_11_b\n","        └─zurich_city_11_c\n","    '''\n","\n","# ------------------\n","#    Dataloader\n","# ------------------\n","\n","loader = DatasetProvider(\n","    dataset_path=Path(args.dataset_path),\n","    representation_type=RepresentationType.VOXEL,\n","    delta_t_ms=100,\n","    num_bins=4,\n","    transforms=combined_transform # Custom class\n",")\n","train_set = loader.get_train_dataset()\n","test_set = loader.get_test_dataset()\n","\n","# def split_train_valid(dataset):\n","#     train_indices = []\n","#     valid_indices = []\n","#     for idx in range(len(dataset)):\n","#         sample = dataset[idx]\n","#         if 'flow_gt_valid_mask' in sample and sample['flow_gt_valid_mask'].all():\n","#             valid_indices.append(idx)\n","#         else:\n","#             train_indices.append(idx)\n","#     train_subset = torch.utils.data.Subset(dataset, train_indices)\n","#     valid_subset = torch.utils.data.Subset(dataset, valid_indices)\n","#     return train_subset, valid_subset\n","\n","# train_set_split, valid_set_split = split_train_valid(train_set)\n","\n","collate_fn = train_collate\n","train_data = DataLoader(train_set, # train_set_split\n","                        batch_size=args.batch_size, #\n","                        shuffle=False,\n","                        collate_fn=collate_fn,\n","                        drop_last=False,\n","                        num_workers=os.cpu_count(),\n","                        pin_memory=True)\n","test_data = DataLoader(test_set,\n","                       batch_size=1,\n","                       shuffle=False,\n","                       collate_fn=collate_fn,\n","                       drop_last=False,\n","                       num_workers=os.cpu_count(),\n","                       pin_memory=True)\n","\n","'''\n","train data:\n","    Type of batch: Dict\n","    Key: seq_name, Type: list\n","    Key: event_volume, Type: torch.Tensor, Shape: torch.Size([Batch, 4, 480, 640]) => イベントデータのバッチ\n","    Key: flow_gt, Type: torch.Tensor, Shape: torch.Size([Batch, 2, 480, 640]) => オプティカルフローデータのバッチ\n","    Key: flow_gt_valid_mask, Type: torch.Tensor, Shape: torch.Size([Batch, 1, 480, 640]) => オプティカルフローデータのvalid. ベースラインでは使わない\n","\n","test data:\n","    Type of batch: Dict\n","    Key: seq_name, Type: list\n","    Key: event_volume, Type: torch.Tensor, Shape: torch.Size([Batch, 4, 480, 640]) => イベントデータのバッチ\n","'''"],"metadata":{"id":"2PL6GvxuGb7T","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"6cdf19e9-22f0-4c1e-e1fe-aef1bcad6f49","execution":{"iopub.status.busy":"2024-07-16T12:26:51.407825Z","iopub.execute_input":"2024-07-16T12:26:51.408069Z","iopub.status.idle":"2024-07-16T12:26:52.310048Z","shell.execute_reply.started":"2024-07-16T12:26:51.408046Z","shell.execute_reply":"2024-07-16T12:26:52.309262Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1721136438698,"user_tz":-540,"elapsed":42581,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ntrain data:\\n    Type of batch: Dict\\n    Key: seq_name, Type: list\\n    Key: event_volume, Type: torch.Tensor, Shape: torch.Size([Batch, 4, 480, 640]) => イベントデータのバッチ\\n    Key: flow_gt, Type: torch.Tensor, Shape: torch.Size([Batch, 2, 480, 640]) => オプティカルフローデータのバッチ\\n    Key: flow_gt_valid_mask, Type: torch.Tensor, Shape: torch.Size([Batch, 1, 480, 640]) => オプティカルフローデータのvalid. ベースラインでは使わない\\n\\ntest data:\\n    Type of batch: Dict\\n    Key: seq_name, Type: list\\n    Key: event_volume, Type: torch.Tensor, Shape: torch.Size([Batch, 4, 480, 640]) => イベントデータのバッチ\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"BITe7F2kuRTS"}},{"cell_type":"code","source":["# ------------------\n","#   optimizer\n","# ------------------\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.00002, weight_decay=args.wdecay, eps=args.epsilon)\n","scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.9)\n","\n","# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, args.lr, args.num_steps + 100,\n","#                                                 pct_start=0.05, cycle_momentum=False, anneal_strategy='linear')\n","# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, args.lr, args.num_steps + 100,\n","#                                                 pct_start=0.05, cycle_momentum=False, anneal_strategy='linear')\n","\n","loss_fn = TotalLoss(smoothness_weight=0.5)"],"metadata":{"id":"Nwxpx4RlGdQ4","execution":{"iopub.status.busy":"2024-07-16T12:26:52.311048Z","iopub.execute_input":"2024-07-16T12:26:52.311305Z","iopub.status.idle":"2024-07-16T12:26:52.317475Z","shell.execute_reply.started":"2024-07-16T12:26:52.311280Z","shell.execute_reply":"2024-07-16T12:26:52.316616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# num_epochs = args.train.epochs\n","num_epochs = 1\n","\n","epe_losses = [[] for _ in range(num_epochs)]\n","overall_losses = [[] for _ in range(num_epochs)]"],"metadata":{"id":"XOEcpMPHGb7T","execution":{"iopub.status.busy":"2024-07-16T12:26:52.318425Z","iopub.execute_input":"2024-07-16T12:26:52.318731Z","iopub.status.idle":"2024-07-16T12:26:52.329276Z","shell.execute_reply.started":"2024-07-16T12:26:52.318705Z","shell.execute_reply":"2024-07-16T12:26:52.328544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BATCH_CONCAT = args.snippet_len"],"metadata":{"id":"_aYlJLJa0Yqf","execution":{"iopub.status.busy":"2024-07-16T12:26:52.330219Z","iopub.execute_input":"2024-07-16T12:26:52.330462Z","iopub.status.idle":"2024-07-16T12:26:52.340816Z","shell.execute_reply.started":"2024-07-16T12:26:52.330438Z","shell.execute_reply":"2024-07-16T12:26:52.340085Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1721136460416,"user_tz":-540,"elapsed":509,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["def save_model(model, additional_string: str):\n","    current_time = time.strftime(\"%Y%m%d-%H%M%S\")\n","    model_path = f\"model_{current_time}_{additional_string}.pth\"\n","    torch.save(model.state_dict(), model_path)\n","    print(f\"Model saved to {model_path}\")"],"metadata":{"execution":{"iopub.status.busy":"2024-07-16T12:29:23.215292Z","iopub.execute_input":"2024-07-16T12:29:23.216056Z","iopub.status.idle":"2024-07-16T12:29:23.220178Z","shell.execute_reply.started":"2024-07-16T12:29:23.216024Z","shell.execute_reply":"2024-07-16T12:29:23.219467Z"},"trusted":true,"id":"TwV7tl-GtBPP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ------------------\n","#   Start training\n","# ------------------\n","model.train()\n","\n","for epoch in range(num_epochs):\n","\n","    total_loss = 0\n","    prev_event_volumes = [torch.zeros([args.batch_size, 4, 480, 640])] * BATCH_CONCAT # Acts as a queue\n","\n","    print(\"on epoch: {}\".format(epoch + 1))\n","    for i, batch in enumerate(tqdm(train_data)):\n","\n","        try:\n","            batch: Dict[str, Any]\n","\n","            event_image = batch[\"event_volume\"].to(device) # [B, 3, 480, 640]\n","            ground_truth_flow = batch[\"flow_gt\"].to(device) # [B, 2, 480, 640]\n","\n","            prev_event_volumes.append(event_image)\n","\n","            prev_ground_truth_flow = batch['prev_flow_gt'].to(device) # [B, 2, 480, 640]\n","            next_ground_truth_flow = batch['next_flow_gt'].to(device) # [B, 2, 480, 640]\n","\n","            input_tensor = torch.stack(prev_event_volumes[-BATCH_CONCAT:], dim=1)\n","            _, flows = model(input_tensor) # [B, 3, 480, 640]\n","\n","            # Overall loss requires flow0, ..., flow3 so we don't implement it here\n","            # What if you created flow_dict from flow0, ..., flow11 (n=12) to and then use overall loss?\n","\n","            for j, flow in enumerate(flows):\n","                print(f'batch {i} | flow #{j + 1} | EPE LOSS:', compute_epe_error(flow, ground_truth_flow).item())\n","\n","            avg_flow = torch.mean(torch.stack(flows, dim=0), dim=0)\n","            epe_loss: torch.Tensor = compute_epe_error(avg_flow, ground_truth_flow)\n","#             overall_loss: loss_fn(flow_dict,\n","#                                  prev_ground_truth_flow,\n","#                                  next_ground_truth_flow,\n","#                                  model)\n","\n","            print(f\"batch {i} average EPE LOSS: {epe_loss.item()}\")\n","            epe_losses[epoch].append(epe_loss.item())\n","\n","#             print(f'batch {i} OVERALL LOSS: {loss_fn()}')\n","\n","            optimizer.zero_grad()\n","\n","            epe_loss.backward() # Change this to which loss function is to be updated\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","            optimizer.step()\n","\n","            total_loss += epe_loss.item() # This too\n","\n","            if len(prev_event_volumes) >= BATCH_CONCAT:\n","                prev_event_volumes.pop(0) # Remove first element\n","\n","            if (i + 1) % 10 == 0:\n","                save_model(model, f'epoch{i + 1}')\n","\n","        except KeyboardInterrupt:\n","            save_model(model)\n","            raise SystemExit(\"KeyboardInterrupt\")\n","\n","    scheduler.step()\n","\n","    print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_data)}')"],"metadata":{"id":"YbeX9fnmGb7U","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"e976b3ef-311f-4b54-eb5e-2ba69ac20bc6","execution":{"iopub.status.busy":"2024-07-16T12:30:26.576196Z","iopub.execute_input":"2024-07-16T12:30:26.576956Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"on epoch: 1\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/63 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"batch 0 | flow #1 | EPE LOSS: 1.8396062090700058\nbatch 0 | flow #2 | EPE LOSS: 1.8395508744573315\nbatch 0 average EPE LOSS: 1.8395785293693376\n","output_type":"stream"},{"name":"stderr","text":"  2%|▏         | 1/63 [02:30<2:35:34, 150.56s/it]","output_type":"stream"},{"name":"stdout","text":"batch 1 | flow #1 | EPE LOSS: 1.3619467749821583\nbatch 1 | flow #2 | EPE LOSS: 1.3619275544724172\nbatch 1 average EPE LOSS: 1.3619371590125076\n","output_type":"stream"},{"name":"stderr","text":"  3%|▎         | 2/63 [03:58<1:55:38, 113.75s/it]","output_type":"stream"},{"name":"stdout","text":"batch 2 | flow #1 | EPE LOSS: 7.162377360312281\nbatch 2 | flow #2 | EPE LOSS: 7.162346349042931\nbatch 2 average EPE LOSS: 7.162361847920057\n","output_type":"stream"},{"name":"stderr","text":"  5%|▍         | 3/63 [05:27<1:42:38, 102.64s/it]","output_type":"stream"},{"name":"stdout","text":"batch 3 | flow #1 | EPE LOSS: 6.344814803053367\nbatch 3 | flow #2 | EPE LOSS: 6.344752112039278\nbatch 3 average EPE LOSS: 6.3447834534162215\n","output_type":"stream"},{"name":"stderr","text":"  6%|▋         | 4/63 [06:56<1:35:26, 97.06s/it] ","output_type":"stream"},{"name":"stdout","text":"batch 4 | flow #1 | EPE LOSS: 3.7186164226350416\nbatch 4 | flow #2 | EPE LOSS: 3.7185795124431653\nbatch 4 average EPE LOSS: 3.7185979638303523\n","output_type":"stream"},{"name":"stderr","text":"  8%|▊         | 5/63 [08:24<1:30:39, 93.79s/it]","output_type":"stream"},{"name":"stdout","text":"batch 5 | flow #1 | EPE LOSS: 2.825004593217984\nbatch 5 | flow #2 | EPE LOSS: 2.8249310917934443\nbatch 5 average EPE LOSS: 2.824967837594634\n","output_type":"stream"},{"name":"stderr","text":" 10%|▉         | 6/63 [09:53<1:27:28, 92.09s/it]","output_type":"stream"},{"name":"stdout","text":"batch 6 | flow #1 | EPE LOSS: 2.29672946530145\nbatch 6 | flow #2 | EPE LOSS: 2.296680076328348\nbatch 6 average EPE LOSS: 2.2967047661321702\n","output_type":"stream"},{"name":"stderr","text":" 11%|█         | 7/63 [11:22<1:25:02, 91.12s/it]","output_type":"stream"},{"name":"stdout","text":"batch 7 | flow #1 | EPE LOSS: 2.873589348851873\nbatch 7 | flow #2 | EPE LOSS: 2.8735631131565\nbatch 7 average EPE LOSS: 2.8735762265105746\n","output_type":"stream"},{"name":"stderr","text":" 13%|█▎        | 8/63 [12:50<1:22:36, 90.12s/it]","output_type":"stream"},{"name":"stdout","text":"batch 8 | flow #1 | EPE LOSS: 3.0382382613548553\nbatch 8 | flow #2 | EPE LOSS: 3.038146791772329\nbatch 8 average EPE LOSS: 3.0381925215022534\n","output_type":"stream"},{"name":"stderr","text":" 14%|█▍        | 9/63 [14:19<1:20:44, 89.71s/it]","output_type":"stream"},{"name":"stdout","text":"batch 9 | flow #1 | EPE LOSS: 2.7668836037184334\nbatch 9 | flow #2 | EPE LOSS: 2.7667894609405552\nbatch 9 average EPE LOSS: 2.7668365219706255\n","output_type":"stream"},{"name":"stderr","text":" 16%|█▌        | 10/63 [15:48<1:19:16, 89.74s/it]","output_type":"stream"},{"name":"stdout","text":"Model saved to model_20240716-124615_epoch10.pth\nbatch 10 | flow #1 | EPE LOSS: 2.4285279740776127\nbatch 10 | flow #2 | EPE LOSS: 2.428396687670258\nbatch 10 average EPE LOSS: 2.428462301353687\n","output_type":"stream"},{"name":"stderr","text":" 17%|█▋        | 11/63 [17:17<1:17:35, 89.52s/it]","output_type":"stream"},{"name":"stdout","text":"batch 11 | flow #1 | EPE LOSS: 2.4646249365988258\nbatch 11 | flow #2 | EPE LOSS: 2.464461195391217\nbatch 11 average EPE LOSS: 2.4645429721152787\n","output_type":"stream"},{"name":"stderr","text":" 19%|█▉        | 12/63 [18:47<1:16:03, 89.49s/it]","output_type":"stream"},{"name":"stdout","text":"batch 12 | flow #1 | EPE LOSS: 2.6449084530256943\nbatch 12 | flow #2 | EPE LOSS: 2.6447026475947277\nbatch 12 average EPE LOSS: 2.6448049849686837\n","output_type":"stream"},{"name":"stderr","text":" 21%|██        | 13/63 [20:15<1:14:17, 89.15s/it]","output_type":"stream"},{"name":"stdout","text":"batch 13 | flow #1 | EPE LOSS: 2.5406600661570344\nbatch 13 | flow #2 | EPE LOSS: 2.540498659477098\nbatch 13 average EPE LOSS: 2.5405734615211246\n","output_type":"stream"},{"name":"stderr","text":" 22%|██▏       | 14/63 [21:44<1:12:49, 89.17s/it]","output_type":"stream"},{"name":"stdout","text":"batch 14 | flow #1 | EPE LOSS: 2.845440700293924\nbatch 14 | flow #2 | EPE LOSS: 2.845453338795262\nbatch 14 average EPE LOSS: 2.8454354812949845\n","output_type":"stream"},{"name":"stderr","text":" 24%|██▍       | 15/63 [23:13<1:11:11, 88.99s/it]","output_type":"stream"},{"name":"stdout","text":"batch 15 | flow #1 | EPE LOSS: 2.4299786496068334\nbatch 15 | flow #2 | EPE LOSS: 2.4303375249572765\nbatch 15 average EPE LOSS: 2.4301547064369675\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▌       | 16/63 [24:42<1:09:36, 88.86s/it]","output_type":"stream"},{"name":"stdout","text":"batch 16 | flow #1 | EPE LOSS: 2.246387872758973\nbatch 16 | flow #2 | EPE LOSS: 2.2467333557478177\nbatch 16 average EPE LOSS: 2.246559123212028\n","output_type":"stream"},{"name":"stderr","text":" 27%|██▋       | 17/63 [26:10<1:08:05, 88.82s/it]","output_type":"stream"},{"name":"stdout","text":"batch 17 | flow #1 | EPE LOSS: 2.5308735030063403\nbatch 17 | flow #2 | EPE LOSS: 2.5312205530276684\nbatch 17 average EPE LOSS: 2.5310459425780794\n","output_type":"stream"},{"name":"stderr","text":" 29%|██▊       | 18/63 [27:40<1:06:54, 89.22s/it]","output_type":"stream"},{"name":"stdout","text":"batch 18 | flow #1 | EPE LOSS: 2.6085754483270445\nbatch 18 | flow #2 | EPE LOSS: 2.608953130093144\nbatch 18 average EPE LOSS: 2.608763362078948\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 19/63 [29:11<1:05:41, 89.59s/it]","output_type":"stream"},{"name":"stdout","text":"batch 19 | flow #1 | EPE LOSS: 2.7964723043875646\nbatch 19 | flow #2 | EPE LOSS: 2.796720692607935\nbatch 19 average EPE LOSS: 2.796595274667605\n","output_type":"stream"},{"name":"stderr","text":" 32%|███▏      | 20/63 [30:41<1:04:17, 89.72s/it]","output_type":"stream"},{"name":"stdout","text":"Model saved to model_20240716-130107_epoch20.pth\nbatch 20 | flow #1 | EPE LOSS: 2.658587000239665\nbatch 20 | flow #2 | EPE LOSS: 2.658736168698733\nbatch 20 average EPE LOSS: 2.6586607164649547\n","output_type":"stream"},{"name":"stderr","text":" 33%|███▎      | 21/63 [32:10<1:02:40, 89.54s/it]","output_type":"stream"},{"name":"stdout","text":"batch 21 | flow #1 | EPE LOSS: 2.1000249757144136\nbatch 21 | flow #2 | EPE LOSS: 2.1002681021195277\nbatch 21 average EPE LOSS: 2.1001455203687085\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▍      | 22/63 [33:39<1:01:02, 89.33s/it]","output_type":"stream"},{"name":"stdout","text":"batch 22 | flow #1 | EPE LOSS: 2.487792712297309\nbatch 22 | flow #2 | EPE LOSS: 2.488061061618193\nbatch 22 average EPE LOSS: 2.4879255761767896\n","output_type":"stream"},{"name":"stderr","text":" 37%|███▋      | 23/63 [35:07<59:14, 88.86s/it]  ","output_type":"stream"},{"name":"stdout","text":"batch 23 | flow #1 | EPE LOSS: 2.459311224791461\nbatch 23 | flow #2 | EPE LOSS: 2.4594535534925623\nbatch 23 average EPE LOSS: 2.4593815602586395\n","output_type":"stream"},{"name":"stderr","text":" 38%|███▊      | 24/63 [36:37<57:58, 89.19s/it]","output_type":"stream"},{"name":"stdout","text":"batch 24 | flow #1 | EPE LOSS: 2.236986798157563\nbatch 24 | flow #2 | EPE LOSS: 2.2371097897588332\nbatch 24 average EPE LOSS: 2.2370468408859074\n","output_type":"stream"},{"name":"stderr","text":" 40%|███▉      | 25/63 [38:05<56:24, 89.06s/it]","output_type":"stream"},{"name":"stdout","text":"batch 25 | flow #1 | EPE LOSS: 2.663109831764366\nbatch 25 | flow #2 | EPE LOSS: 2.6632135171499245\nbatch 25 average EPE LOSS: 2.6631576587533967\n","output_type":"stream"},{"name":"stderr","text":" 41%|████▏     | 26/63 [39:35<55:00, 89.19s/it]","output_type":"stream"},{"name":"stdout","text":"batch 26 | flow #1 | EPE LOSS: 2.1505382074575206\nbatch 26 | flow #2 | EPE LOSS: 2.150443526728732\nbatch 26 average EPE LOSS: 2.1504873017924155\n","output_type":"stream"},{"name":"stderr","text":" 43%|████▎     | 27/63 [41:04<53:31, 89.21s/it]","output_type":"stream"},{"name":"stdout","text":"batch 27 | flow #1 | EPE LOSS: 3.2348084167526894\nbatch 27 | flow #2 | EPE LOSS: 3.2346694565968526\nbatch 27 average EPE LOSS: 3.234738607624671\n","output_type":"stream"},{"name":"stderr","text":" 44%|████▍     | 28/63 [42:34<52:06, 89.32s/it]","output_type":"stream"},{"name":"stdout","text":"batch 28 | flow #1 | EPE LOSS: 6.961979225804903\nbatch 28 | flow #2 | EPE LOSS: 6.961799806928618\nbatch 28 average EPE LOSS: 6.961889392112606\n","output_type":"stream"},{"name":"stderr","text":" 46%|████▌     | 29/63 [44:03<50:39, 89.40s/it]","output_type":"stream"},{"name":"stdout","text":"batch 29 | flow #1 | EPE LOSS: 2.4598365694186803\nbatch 29 | flow #2 | EPE LOSS: 2.4597832064928298\nbatch 29 average EPE LOSS: 2.459809769515961\n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 30/63 [45:34<49:19, 89.70s/it]","output_type":"stream"},{"name":"stdout","text":"Model saved to model_20240716-131600_epoch30.pth\nbatch 30 | flow #1 | EPE LOSS: 2.2468038624075533\nbatch 30 | flow #2 | EPE LOSS: 2.246761215456463\nbatch 30 average EPE LOSS: 2.2467823458239353\n","output_type":"stream"},{"name":"stderr","text":" 49%|████▉     | 31/63 [47:03<47:47, 89.61s/it]","output_type":"stream"},{"name":"stdout","text":"batch 31 | flow #1 | EPE LOSS: 2.4344783179527894\nbatch 31 | flow #2 | EPE LOSS: 2.4344390852540503\nbatch 31 average EPE LOSS: 2.434458374238545\n","output_type":"stream"}]},{"cell_type":"code","source":["save_model(model)"],"metadata":{"trusted":true,"id":"v-eAMBAHtBPP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# epe_losses = list(map(lambda x: x.item(), epe_losses[0]))\n","# overall_losses = list(map(lambda x: x.item(), overall_losses[0]))\n","\n","plt.figure(figsize=(16, 9))\n","len_x = min(len(epe_losses), len(overall_losses))\n","\n","plt.plot(epe_losses[:len_x])\n","plt.plot(overall_losses[:len_x])\n","\n","plt.xlabel('Batch Number')\n","plt.ylabel('Loss')\n","\n","plt.grid()\n","plt.legend()\n","\n","plt.show()"],"metadata":{"id":"lc4Q3acvGb7U","execution":{"iopub.status.busy":"2024-07-16T12:27:34.590052Z","iopub.status.idle":"2024-07-16T12:27:34.590434Z","shell.execute_reply.started":"2024-07-16T12:27:34.590256Z","shell.execute_reply":"2024-07-16T12:27:34.590275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","# Create the directory if it doesn't exist\n","# if not os.path.exists('checkpoints'):\n","#     os.makedirs('checkpoints')\n","\n","current_time = time.strftime(\"%Y%m%d-%H%M%S\")\n","model_path = f\"models/pclnet_model_{current_time}_epoch1.pth\"\n","torch.save(model.state_dict(), model_path)\n","print(f\"Model saved to {model_path}\")"],"metadata":{"id":"FtltB8OsGb7U","execution":{"iopub.status.busy":"2024-07-16T12:27:34.591517Z","iopub.status.idle":"2024-07-16T12:27:34.591808Z","shell.execute_reply.started":"2024-07-16T12:27:34.591662Z","shell.execute_reply":"2024-07-16T12:27:34.591676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","flow: torch.Tensor = torch.tensor([]).to(device)\n","\n","prev_event_volumes = [torch.zeros([1, 4, 480, 640])] * BATCH_CONCAT # Acts as a queue\n","\n","with torch.no_grad():\n","    print(\"start test\")\n","    for batch in tqdm(test_data):\n","        batch: Dict[str, Any]\n","\n","        event_image = batch[\"event_volume\"].to(device)\n","        prev_event_volumes.append(event_image)\n","\n","        input_tensor = torch.stack(prev_event_volumes[-BATCH_CONCAT:], dim=1)\n","        _, flows = model(input_tensor)\n","\n","        batch_flow = torch.mean(torch.stack(flows, dim=0), dim=0)\n","        flow = torch.cat((flow, batch_flow), dim=0)  # [N, 2, 480, 640]\n","\n","        if len(prev_event_volumes) >= BATCH_CONCAT:\n","            prev_event_volumes.pop(0)\n","\n","    print(\"test done\")"],"metadata":{"id":"LdYOQdyRGb7U","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2e68ae19-d99a-4a5f-b3cc-9902eb41409b","execution":{"iopub.status.busy":"2024-07-16T12:27:34.592752Z","iopub.status.idle":"2024-07-16T12:27:34.593035Z","shell.execute_reply.started":"2024-07-16T12:27:34.592899Z","shell.execute_reply":"2024-07-16T12:27:34.592913Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1721139134589,"user_tz":-540,"elapsed":1061910,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["start test\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 97/97 [17:41<00:00, 10.95s/it]"]},{"output_type":"stream","name":"stdout","text":["test done\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# ------------------\n","#  save submission\n","# ------------------\n","current_time = time.strftime(\"%Y%m%d-%H%M%S\")\n","save_optical_flow_to_npy(flow, f'submissions/submission_pclnet_{current_time}')"],"metadata":{"id":"U-D4ukOay9kB","executionInfo":{"status":"ok","timestamp":1721139135622,"user_tz":-540,"elapsed":1053,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":36,"outputs":[]}]}