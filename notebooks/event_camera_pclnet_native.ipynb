{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8921598,"sourceType":"datasetVersion","datasetId":5366031},{"sourceId":8936349,"sourceType":"datasetVersion","datasetId":5376452}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[],"collapsed_sections":["tpYel0PeGb7O","iNELPnjTGb7Q","Asa6m7ccKl4E"]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","project_dir = '/content/drive/My Drive/Colab Notebooks/matsuoo/dl/event_camera_repo'\n","%cd {project_dir}"],"metadata":{"id":"QTx90vTeGl3_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721112071198,"user_tz":-540,"elapsed":38482,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}},"outputId":"0b9d16a1-ecb7-4ab4-caf0-1c691bb8f386"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/Colab Notebooks/matsuoo/dl/event_camera_repo\n"]}]},{"cell_type":"code","source":["!pip install hydra-core omegaconf hdf5plugin h5py numba imageio imageio-ffmpeg tqdm torchvision --quiet"],"metadata":{"trusted":true,"id":"fdfquW_iGb7K","executionInfo":{"status":"ok","timestamp":1721112160030,"user_tz":-540,"elapsed":88835,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"21223f2f-7234-43a0-a3c9-59d6bed38311"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["import torch\n","import hydra\n","from omegaconf import DictConfig\n","from torch.utils.data import DataLoader\n","import random\n","import numpy as np\n","from enum import Enum, auto\n","from tqdm import tqdm\n","from pathlib import Path\n","from typing import Dict, Any\n","import os\n","import time\n","\n","import math\n","from pathlib import PurePath\n","from typing import Tuple\n","import cv2\n","import hdf5plugin\n","import h5py\n","from numba import jit\n","import imageio\n","imageio.plugins.freeimage.download()\n","import imageio.v3 as iio\n","from torchvision.transforms import RandomCrop\n","from torchvision import transforms as tf\n","from torch.utils.checkpoint import checkpoint\n","from torch.utils.data import Dataset\n","import torchvision.transforms.functional as F\n","\n","from torch import nn"],"metadata":{"trusted":true,"id":"tAW9ff9RGb7M","executionInfo":{"status":"ok","timestamp":1721112176185,"user_tz":-540,"elapsed":16159,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ce3675cc-15c3-4b36-fbef-4664b1451a5d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Imageio: 'libfreeimage-3.16.0-linux64.so' was not found on your computer; downloading it now.\n","Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/freeimage/libfreeimage-3.16.0-linux64.so (4.6 MB)\n","Downloading: 8192/4830080 bytes (0.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b4830080/4830080 bytes (100.0%)\n","  Done\n","File saved as /root/.imageio/freeimage/libfreeimage-3.16.0-linux64.so.\n"]}]},{"cell_type":"code","source":["!pip install einops\n","from einops.layers.torch import Rearrange\n","from einops import rearrange"],"metadata":{"trusted":true,"id":"EjOf_ZjXGb7N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721112266633,"user_tz":-540,"elapsed":10551,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}},"outputId":"931f2f05-31d7-4063-ed22-a207dd256fe0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting einops\n","  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m832.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.8.0\n"]}]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"ZGFPKK5nCkDE","executionInfo":{"status":"ok","timestamp":1721112266633,"user_tz":-540,"elapsed":3,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## `utils.py`"],"metadata":{"id":"tpYel0PeGb7O"}},{"cell_type":"code","source":["def set_seed(seed: int = 0) -> None:\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","class RepresentationType(Enum):\n","    VOXEL = auto()\n","    STEPAN = auto()\n","\n","\n","class EventRepresentation:\n","    def __init__(self):\n","        pass\n","\n","    def convert(self, events):\n","        raise NotImplementedError\n","\n","\n","class VoxelGrid(EventRepresentation):\n","    def __init__(self, input_size: tuple, normalize: bool):\n","        assert len(input_size) == 3\n","        self.voxel_grid = torch.zeros(\n","            (input_size), dtype=torch.float, requires_grad=False)\n","        self.nb_channels = input_size[0]\n","        self.normalize = normalize\n","\n","    def convert(self, events):\n","        C, H, W = self.voxel_grid.shape\n","        with torch.no_grad():\n","            self.voxel_grid = self.voxel_grid.to(events['p'].device)\n","            voxel_grid = self.voxel_grid.clone()\n","\n","            t_norm = events['t']\n","            t_norm = (C - 1) * (t_norm-t_norm[0]) / (t_norm[-1]-t_norm[0])\n","\n","            x0 = events['x'].int()\n","            y0 = events['y'].int()\n","            t0 = t_norm.int()\n","\n","            value = 2*events['p']-1\n","            #start_t = time()\n","            for xlim in [x0, x0+1]:\n","                for ylim in [y0, y0+1]:\n","                    for tlim in [t0, t0+1]:\n","\n","                        mask = (xlim < W) & (xlim >= 0) & (ylim < H) & (\n","                            ylim >= 0) & (tlim >= 0) & (tlim < self.nb_channels)\n","                        interp_weights = value * (1 - (xlim-events['x']).abs()) * (\n","                            1 - (ylim-events['y']).abs()) * (1 - (tlim - t_norm).abs())\n","                        index = H * W * tlim.long() + \\\n","                            W * ylim.long() + \\\n","                            xlim.long()\n","\n","                        voxel_grid.put_(\n","                            index[mask], interp_weights[mask], accumulate=True)\n","\n","            if self.normalize:\n","                mask = torch.nonzero(voxel_grid, as_tuple=True)\n","                if mask[0].size()[0] > 0:\n","                    mean = voxel_grid[mask].mean()\n","                    std = voxel_grid[mask].std()\n","                    if std > 0:\n","                        voxel_grid[mask] = (voxel_grid[mask] - mean) / std\n","                    else:\n","                        voxel_grid[mask] = voxel_grid[mask] - mean\n","\n","        return voxel_grid\n","\n","\n","class PolarityCount(EventRepresentation):\n","    def __init__(self, input_size: tuple):\n","        assert len(input_size) == 3\n","        self.voxel_grid = torch.zeros(\n","            (input_size), dtype=torch.float, requires_grad=False)\n","        self.nb_channels = input_size[0]\n","\n","    def convert(self, events):\n","        C, H, W = self.voxel_grid.shape\n","        with torch.no_grad():\n","            self.voxel_grid = self.voxel_grid.to(events['p'].device)\n","            voxel_grid = self.voxel_grid.clone()\n","\n","            x0 = events['x'].int()\n","            y0 = events['y'].int()\n","\n","            #start_t = time()\n","            for xlim in [x0, x0+1]:\n","                for ylim in [y0, y0+1]:\n","                    mask = (xlim < W) & (xlim >= 0) & (ylim < H) & (\n","                        ylim >= 0)\n","                    interp_weights = (1 - (xlim-events['x']).abs()) * (\n","                        1 - (ylim-events['y']).abs())\n","                    index = H * W * events['p'].long() + \\\n","                        W * ylim.long() + \\\n","                        xlim.long()\n","\n","                    voxel_grid.put_(\n","                        index[mask], interp_weights[mask], accumulate=True)\n","\n","        return voxel_grid\n","\n","\n","def flow_16bit_to_float(flow_16bit: np.ndarray):\n","    assert flow_16bit.dtype == np.uint16\n","    assert flow_16bit.ndim == 3\n","    h, w, c = flow_16bit.shape\n","    assert c == 3\n","\n","    valid2D = flow_16bit[..., 2] == 1\n","    assert valid2D.shape == (h, w)\n","    assert np.all(flow_16bit[~valid2D, -1] == 0)\n","    valid_map = np.where(valid2D)\n","\n","    # to actually compute something useful:\n","    flow_16bit = flow_16bit.astype('float')\n","\n","    flow_map = np.zeros((h, w, 2))\n","    flow_map[valid_map[0], valid_map[1], 0] = (\n","        flow_16bit[valid_map[0], valid_map[1], 0] - 2 ** 15) / 128\n","    flow_map[valid_map[0], valid_map[1], 1] = (\n","        flow_16bit[valid_map[0], valid_map[1], 1] - 2 ** 15) / 128\n","    return flow_map, valid2D"],"metadata":{"trusted":true,"id":"8z3ZgnkfGb7P","executionInfo":{"status":"ok","timestamp":1721113047126,"user_tz":-540,"elapsed":275,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def warp_images_with_flow(images, flow):\n","    dim3 = 0\n","    if images.dim() == 3:\n","        dim3 = 1\n","        images = images.unsqueeze(0)\n","        flow = flow.unsqueeze(0)\n","    height = images.shape[2]\n","    width = images.shape[3]\n","    flow_x,flow_y = flow[:,0,...],flow[:,1,...]\n","    coord_x, coord_y = torch.meshgrid(torch.arange(height), torch.arange(width))\n","\n","    if torch.cuda.is_available():\n","        pos_x = coord_x.reshape(height,width).type(torch.float32).cuda() + flow_x\n","        pos_y = coord_y.reshape(height,width).type(torch.float32).cuda() + flow_y\n","    else: # Troubleshoot without cuda\n","        pos_x = coord_x.reshape(height,width).type(torch.float32) + flow_x\n","        pos_y = coord_y.reshape(height,width).type(torch.float32) + flow_y\n","    pos_x = (pos_x-(height-1)/2)/((height-1)/2)\n","    pos_y = (pos_y-(width-1)/2)/((width-1)/2)\n","\n","    pos = torch.stack((pos_y,pos_x),3).type(torch.float32)\n","    result = torch.nn.functional.grid_sample(images, pos, mode='bilinear', padding_mode='zeros')\n","    if dim3 == 1:\n","        result = result.squeeze()\n","\n","    return result\n","\n","def charbonnier_loss(delta, alpha=0.45, epsilon=1e-3):\n","        loss = torch.mean(torch.pow((delta ** 2 + epsilon ** 2), alpha))\n","        return loss\n","\n","def compute_smoothness_loss(flow):\n","\n","    flow_ucrop = flow[..., 1:]\n","    flow_dcrop = flow[..., :-1]\n","    flow_lcrop = flow[..., 1:, :]\n","    flow_rcrop = flow[..., :-1, :]\n","\n","    flow_ulcrop = flow[..., 1:, 1:]\n","    flow_drcrop = flow[..., :-1, :-1]\n","    flow_dlcrop = flow[..., :-1, 1:]\n","    flow_urcrop = flow[..., 1:, :-1]\n","\n","    smoothness_loss = charbonnier_loss(flow_lcrop - flow_rcrop) +\\\n","                      charbonnier_loss(flow_ucrop - flow_dcrop) +\\\n","                      charbonnier_loss(flow_ulcrop - flow_drcrop) +\\\n","                      charbonnier_loss(flow_dlcrop - flow_urcrop)\n","    smoothness_loss /= 4.\n","\n","    return smoothness_loss\n","\n","def compute_photometric_loss(prev_images, next_images, flow_dict):\n","    total_photometric_loss = 0.\n","    loss_weight_sum = 0.\n","    for i in range(len(flow_dict)):\n","        for image_num in range(prev_images.shape[0]):\n","            flow = flow_dict[\"flow{}\".format(i)][image_num]\n","            height = flow.shape[1]\n","            width = flow.shape[2]\n","\n","            prev_images_resize = F.to_tensor(F.resize(F.to_pil_image(prev_images[image_num].cpu()),\n","                                                    [height, width]))\n","            next_images_resize = F.to_tensor(F.resize(F.to_pil_image(next_images[image_num].cpu()),\n","                                                    [height, width]))\n","\n","            if torch.cuda.is_available():\n","                prev_images_resize = prev_images_resize.cuda()\n","                next_images_resize = next_images_resize.cuda()\n","\n","            next_images_warped = warp_images_with_flow(next_images_resize, flow)\n","\n","            distance = next_images_warped - prev_images_resize\n","            photometric_loss = charbonnier_loss(distance)\n","            total_photometric_loss += photometric_loss\n","        loss_weight_sum += 1.\n","    total_photometric_loss /= loss_weight_sum\n","\n","    return total_photometric_loss\n","\n","\n","class TotalLoss(torch.nn.Module):\n","    def __init__(self, smoothness_weight, weight_decay_weight=1e-4):\n","        super(TotalLoss, self).__init__()\n","        self._smoothness_weight = smoothness_weight\n","        self._weight_decay_weight = weight_decay_weight\n","\n","    def forward(self, flow_dict, prev_image, next_image, EVFlowNet_model):\n","        # weight decay loss\n","        weight_decay_loss = 0\n","        for i in EVFlowNet_model.parameters():\n","            weight_decay_loss += torch.sum(i ** 2) / 2 * self._weight_decay_weight\n","\n","        # smoothness loss\n","        smoothness_loss = 0\n","        for i in range(len(flow_dict)):\n","            smoothness_loss += compute_smoothness_loss(flow_dict[\"flow{}\".format(i)])\n","        smoothness_loss *= self._smoothness_weight / 4.\n","\n","        # Photometric loss.\n","        photometric_loss = compute_photometric_loss(prev_image,\n","                                                    next_image,\n","                                                    flow_dict)\n","\n","        # Warped next image for debugging.\n","        #next_image_warped = warp_images_with_flow(next_image,\n","        #                                          flow_dict['flow3'])\n","\n","        loss = weight_decay_loss + photometric_loss + smoothness_loss\n","\n","        return loss"],"metadata":{"trusted":true,"id":"72oMFIZCGb7P","executionInfo":{"status":"ok","timestamp":1721113047362,"user_tz":-540,"elapsed":2,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["## Logging"],"metadata":{"id":"4HVuxiIAMXY7"}},{"cell_type":"markdown","source":["```python\n","import sys\n","import logging\n","import datetime\n","import time\n","\n","class StreamToLogger:\n","    def __init__(self, logger, log_level=logging.INFO):\n","        self.logger = logger\n","        self.log_level = log_level\n","        self.linebuf = ''\n","\n","    def write(self, buf):\n","        for line in buf.rstrip().splitlines():\n","            self.logger.log(self.log_level, line.rstrip())\n","            sys.__stdout__.write(line + '\\n')\n","\n","    def flush(self):\n","        pass\n","\n","# Create logger\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","logger.setLevel(logging.INFO)\n","\n","# Create file handler which logs even debug messages\n","current_time = time.strftime(\"%Y%m%d-%H%M%S\")\n","logfile = f'../../logs/log_{current_time}.log'\n","fh = logging.FileHandler(logfile)\n","fh.setLevel(logging.DEBUG)\n","\n","# Create console handler with a higher log level\n","ch = logging.StreamHandler(sys.__stdout__)\n","ch.setLevel(logging.INFO)\n","\n","# Create formatter and add it to the handlers\n","formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n","fh.setFormatter(formatter)\n","ch.setFormatter(formatter)\n","\n","# Add the handlers to the logger\n","logger.addHandler(fh)\n","logger.addHandler(ch)\n","\n","# Ensure the log file is created\n","with open(logfile, 'w') as file:\n","    file.write('')  # Create an empty log file\n","\n","# Redirect stdout and stderr to the logger\n","sys.stdout = StreamToLogger(logger, logging.INFO)\n","sys.stderr = StreamToLogger(logger, logging.ERROR)\n","```"],"metadata":{"id":"l1Giqrq2Qwm4"}},{"cell_type":"markdown","source":["## `datasets.py`"],"metadata":{"id":"iNELPnjTGb7Q"}},{"cell_type":"code","source":["VISU_INDEX = 1\n","\n","\n","class EventSlicer:\n","    def __init__(self, h5f: h5py.File):\n","        self.h5f = h5f\n","\n","        self.events = dict()\n","        for dset_str in ['p', 'x', 'y', 't']:\n","            self.events[dset_str] = self.h5f['events/{}'.format(dset_str)]\n","\n","        # This is the mapping from milliseconds to event index:\n","        # It is defined such that\n","        # (1) t[ms_to_idx[ms]] >= ms*1000\n","        # (2) t[ms_to_idx[ms] - 1] < ms*1000\n","        # ,where 'ms' is the time in milliseconds and 't' the event timestamps in microseconds.\n","        #\n","        # As an example, given 't' and 'ms':\n","        # t:    0     500    2100    5000    5000    7100    7200    7200    8100    9000\n","        # ms:   0       1       2       3       4       5       6       7       8       9\n","        #\n","        # we get\n","        #\n","        # ms_to_idx:\n","        #       0       2       2       3       3       3       5       5       8       9\n","        self.ms_to_idx = np.asarray(self.h5f['ms_to_idx'], dtype='int64')\n","\n","        self.t_offset = int(h5f['t_offset'][()])\n","        self.t_final = int(self.events['t'][-1]) + self.t_offset\n","\n","    def get_final_time_us(self):\n","        return self.t_final\n","\n","    def get_events(self, t_start_us: int, t_end_us: int) -> Dict[str, np.ndarray]:\n","        \"\"\"Get events (p, x, y, t) within the specified time window\n","        Parameters\n","        ----------\n","        t_start_us: start time in microseconds\n","        t_end_us: end time in microseconds\n","        Returns\n","        -------\n","        events: dictionary of (p, x, y, t) or None if the time window cannot be retrieved\n","        \"\"\"\n","        assert t_start_us < t_end_us\n","\n","        # We assume that the times are top-off-day, hence subtract offset:\n","        t_start_us -= self.t_offset\n","        t_end_us -= self.t_offset\n","\n","        t_start_ms, t_end_ms = self.get_conservative_window_ms(\n","            t_start_us, t_end_us)\n","        t_start_ms_idx = self.ms2idx(t_start_ms)\n","        t_end_ms_idx = self.ms2idx(t_end_ms)\n","        if t_start_ms_idx is None or t_end_ms_idx is None:\n","            print('Error', 'start', t_start_us, 'end', t_end_us)\n","            # Cannot guarantee window size anymore\n","            return None\n","\n","        events = dict()\n","        time_array_conservative = np.asarray(\n","            self.events['t'][t_start_ms_idx:t_end_ms_idx])\n","        idx_start_offset, idx_end_offset = self.get_time_indices_offsets(\n","            time_array_conservative, t_start_us, t_end_us)\n","        t_start_us_idx = t_start_ms_idx + idx_start_offset\n","        t_end_us_idx = t_start_ms_idx + idx_end_offset\n","        # Again add t_offset to get gps time\n","        events['t'] = time_array_conservative[idx_start_offset:idx_end_offset] + self.t_offset\n","        for dset_str in ['p', 'x', 'y']:\n","            events[dset_str] = np.asarray(\n","                self.events[dset_str][t_start_us_idx:t_end_us_idx])\n","            assert events[dset_str].size == events['t'].size\n","        return events\n","\n","    @staticmethod\n","    def get_conservative_window_ms(ts_start_us: int, ts_end_us) -> Tuple[int, int]:\n","        \"\"\"Compute a conservative time window of time with millisecond resolution.\n","        We have a time to index mapping for each millisecond. Hence, we need\n","        to compute the lower and upper millisecond to retrieve events.\n","        Parameters\n","        ----------\n","        ts_start_us:    start time in microseconds\n","        ts_end_us:      end time in microseconds\n","        Returns\n","        -------\n","        window_start_ms:    conservative start time in milliseconds\n","        window_end_ms:      conservative end time in milliseconds\n","        \"\"\"\n","        assert ts_end_us > ts_start_us\n","        window_start_ms = math.floor(ts_start_us/1000)\n","        window_end_ms = math.ceil(ts_end_us/1000)\n","        return window_start_ms, window_end_ms\n","\n","    @staticmethod\n","    @jit(nopython=True)\n","    def get_time_indices_offsets(\n","            time_array: np.ndarray,\n","            time_start_us: int,\n","            time_end_us: int) -> Tuple[int, int]:\n","        \"\"\"Compute index offset of start and end timestamps in microseconds\n","        Parameters\n","        ----------\n","        time_array:     timestamps (in us) of the events\n","        time_start_us:  start timestamp (in us)\n","        time_end_us:    end timestamp (in us)\n","        Returns\n","        -------\n","        idx_start:  Index within this array corresponding to time_start_us\n","        idx_end:    Index within this array corresponding to time_end_us\n","        such that (in non-edge cases)\n","        time_array[idx_start] >= time_start_us\n","        time_array[idx_end] >= time_end_us\n","        time_array[idx_start - 1] < time_start_us\n","        time_array[idx_end - 1] < time_end_us\n","        this means that\n","        time_start_us <= time_array[idx_start:idx_end] < time_end_us\n","        \"\"\"\n","\n","        assert time_array.ndim == 1\n","\n","        idx_start = -1\n","        if time_array[-1] < time_start_us:\n","\n","            # Return same index twice: array[x:x] is empty.\n","            return time_array.size, time_array.size\n","        else:\n","            for idx_from_start in range(0, time_array.size, 1):\n","                if time_array[idx_from_start] >= time_start_us:\n","                    idx_start = idx_from_start\n","                    break\n","        assert idx_start >= 0\n","\n","        idx_end = time_array.size\n","        for idx_from_end in range(time_array.size - 1, -1, -1):\n","            if time_array[idx_from_end] >= time_end_us:\n","                idx_end = idx_from_end\n","            else:\n","                break\n","\n","        assert time_array[idx_start] >= time_start_us\n","        if idx_end < time_array.size:\n","            assert time_array[idx_end] >= time_end_us\n","        if idx_start > 0:\n","            assert time_array[idx_start - 1] < time_start_us\n","        if idx_end > 0:\n","            assert time_array[idx_end - 1] < time_end_us\n","        return idx_start, idx_end\n","\n","    def ms2idx(self, time_ms: int) -> int:\n","        assert time_ms >= 0\n","        if time_ms >= self.ms_to_idx.size:\n","            return None\n","        return self.ms_to_idx[time_ms]\n","\n","\n","class Sequence(Dataset):\n","    def __init__(self, seq_path: Path, representation_type: RepresentationType, mode: str = 'test', delta_t_ms: int = 100,\n","                 num_bins: int = 4, transforms=[], name_idx=0, visualize=False, load_gt=False):\n","        assert num_bins >= 1\n","        assert delta_t_ms == 100\n","        assert seq_path.is_dir(), seq_path\n","        assert mode in {'train', 'test'}\n","        assert representation_type is not None\n","        '''\n","        ディレクトリ構造:\n","\n","        data\n","        ├─test\n","        |  ├─seq_1\n","        |  |    ├─events_left\n","        |  |    |   ├─events.h5\n","        |  |    |   └─rectify_map.h5\n","        |  |    └─forward_timestamps.txt\n","        └─train\n","            ├─seq_1\n","            |    ├─events_left\n","            |    |       ├─ events.h5\n","            |    |       └─ rectify_map.h5\n","            |    ├─flow_forward\n","            |    |       ├─ 000134.png\n","            |    |       |.....\n","            |    └─forward_timestamps.txt\n","            ├─seq_2\n","            └─seq_3\n","        '''\n","        self.seq_name = PurePath(seq_path).name\n","        self.mode = mode\n","        self.name_idx = name_idx\n","        self.visualize_samples = visualize\n","        self.load_gt = load_gt\n","        self.transforms = transforms\n","        if self.mode == \"test\":\n","            assert load_gt == False\n","            # Get Test Timestamp File\n","            ev_dir_location = seq_path / 'events_left'\n","            timestamp_file = seq_path / 'forward_timestamps.txt'\n","            flow_path = seq_path / 'flow_forward'\n","            timestamps_flow = np.loadtxt(\n","                seq_path / 'forward_timestamps.txt', delimiter=',', dtype='int64')\n","            self.indices = np.arange(len(timestamps_flow))\n","            self.timestamps_flow = timestamps_flow[:, 0]\n","\n","        elif self.mode == \"train\":\n","            ev_dir_location = seq_path / 'events_left'\n","            flow_path = seq_path / 'flow_forward'\n","            timestamp_file = seq_path / 'forward_timestamps.txt'\n","            self.flow_png = [Path(os.path.join(flow_path, img)) for img in sorted(\n","                os.listdir(flow_path))]\n","            timestamps_flow = np.loadtxt(\n","                seq_path / 'forward_timestamps.txt', delimiter=',', dtype='int64')\n","            self.indices = np.arange(len(timestamps_flow))\n","            self.timestamps_flow = timestamps_flow[:, 0]\n","        else:\n","            pass\n","        assert timestamp_file.is_file()\n","\n","        file = np.genfromtxt(\n","            timestamp_file,\n","            delimiter=','\n","        )\n","\n","        self.idx_to_visualize = file[:, 2] if file.shape[1] == 3 else []\n","\n","        # Save output dimensions\n","        self.height = 480\n","        self.width = 640\n","        self.num_bins = num_bins\n","\n","\n","        # Set event representation\n","        self.voxel_grid = VoxelGrid(\n","                (self.num_bins, self.height, self.width), normalize=True)\n","        self.delta_t_us = delta_t_ms * 1000\n","\n","        # Left events only\n","        ev_data_file = ev_dir_location / 'events.h5'\n","        ev_rect_file = ev_dir_location / 'rectify_map.h5'\n","\n","        h5f_location = h5py.File(str(ev_data_file), 'r')\n","        self.h5f = h5f_location\n","        self.event_slicer = EventSlicer(h5f_location)\n","\n","        self.h5rect = h5py.File(str(ev_rect_file), 'r')\n","        self.rectify_ev_map = self.h5rect['rectify_map'][()]\n","\n","\n","    def events_to_voxel_grid(self, p, t, x, y, device: str = 'cpu'):\n","        t = (t - t[0]).astype('float32')\n","        t = (t/t[-1])\n","        x = x.astype('float32')\n","        y = y.astype('float32')\n","        pol = p.astype('float32')\n","        event_data_torch = {\n","            'p': torch.from_numpy(pol),\n","            't': torch.from_numpy(t),\n","            'x': torch.from_numpy(x),\n","            'y': torch.from_numpy(y),\n","        }\n","        return self.voxel_grid.convert(event_data_torch)\n","\n","    def getHeightAndWidth(self):\n","        return self.height, self.width\n","\n","    @staticmethod\n","    def get_disparity_map(filepath: Path):\n","        assert filepath.is_file()\n","        disp_16bit = cv2.imread(str(filepath), cv2.IMREAD_ANYDEPTH)\n","        return disp_16bit.astype('float32')/256\n","\n","    @staticmethod\n","    def load_flow(flowfile: Path):\n","        assert flowfile.exists()\n","        assert flowfile.suffix == '.png'\n","        flow_16bit = iio.imread(str(flowfile), plugin='PNG-FI')\n","        flow, valid2D = flow_16bit_to_float(flow_16bit)\n","        return flow, valid2D\n","\n","    @staticmethod\n","    def close_callback(h5f):\n","        h5f.close()\n","\n","    def get_image_width_height(self):\n","        return self.height, self.width\n","\n","    def __len__(self):\n","        # Ignore the first and last images as their own\n","        return len(self.timestamps_flow) # - 2\n","\n","    def rectify_events(self, x: np.ndarray, y: np.ndarray):\n","        # assert location in self.locations\n","        # From distorted to undistorted\n","        rectify_map = self.rectify_ev_map\n","        assert rectify_map.shape == (\n","            self.height, self.width, 2), rectify_map.shape\n","        assert x.max() < self.width\n","        assert y.max() < self.height\n","        return rectify_map[y, x]\n","\n","    def get_data(self, index) -> Dict[str, any]:\n","        # Adjust index to skip the first element\n","#         index += 1\n","\n","        ts_start: int = self.timestamps_flow[index] - self.delta_t_us\n","        ts_end: int = self.timestamps_flow[index]\n","\n","        file_index = self.indices[index]\n","\n","        output = {\n","            'file_index': file_index,\n","            'timestamp': self.timestamps_flow[index],\n","            'seq_name': self.seq_name\n","        }\n","        # Save sample for benchmark submission\n","        output['save_submission'] = file_index in self.idx_to_visualize\n","        output['visualize'] = self.visualize_samples\n","        event_data = self.event_slicer.get_events(\n","            ts_start, ts_end)\n","        p = event_data['p']\n","        t = event_data['t']\n","        x = event_data['x']\n","        y = event_data['y']\n","\n","        xy_rect = self.rectify_events(x, y)\n","        x_rect = xy_rect[:, 0]\n","        y_rect = xy_rect[:, 1]\n","\n","        if self.voxel_grid is None:\n","            raise NotImplementedError\n","        else:\n","            event_representation = self.events_to_voxel_grid(\n","                p, t, x_rect, y_rect)\n","            output['event_volume'] = event_representation\n","        output['name_map'] = self.name_idx\n","\n","        if self.load_gt:\n","            output['flow_gt'] = [torch.tensor(x) for x in self.load_flow(self.flow_png[index])]\n","            output['flow_gt'][0] = torch.moveaxis(output['flow_gt'][0], -1, 0)\n","            output['flow_gt'][1] = torch.unsqueeze(output['flow_gt'][1], 0)\n","\n","            flow_gt_shape = [tensor.shape for tensor in output['flow_gt']]\n","            zero_flow_gt = [torch.zeros_like(tensor) for tensor in output['flow_gt']]\n","\n","            # Load previous image\n","            if index > 0:\n","                output['prev_flow_gt'] = [torch.tensor(x) for x in self.load_flow(self.flow_png[index - 1])]\n","                output['prev_flow_gt'][0] = torch.moveaxis(output['prev_flow_gt'][0], -1, 0)\n","                output['prev_flow_gt'][1] = torch.unsqueeze(output['prev_flow_gt'][1], 0)\n","            else:\n","                output['prev_flow_gt'] = zero_flow_gt\n","\n","            # Load next image\n","            if index < len(self.timestamps_flow) - 1:\n","                output['next_flow_gt'] = [torch.tensor(x) for x in self.load_flow(self.flow_png[index + 1])]\n","                output['next_flow_gt'][0] = torch.moveaxis(output['next_flow_gt'][0], -1, 0)\n","                output['next_flow_gt'][1] = torch.unsqueeze(output['next_flow_gt'][1], 0)\n","            else:\n","                output['next_flow_gt'] = zero_flow_gt\n","\n","        return output\n","\n","    def __getitem__(self, idx):\n","        # Adjust index to skip the first element\n","        sample = self.get_data(idx) # idx + 1\n","\n","        if self.transforms:\n","            sample = self.transforms(sample)\n","\n","        return sample\n","\n","    def get_voxel_grid(self, idx):\n","\n","        if idx == 0:\n","            event_data = self.event_slicer.get_events(\n","                self.timestamps_flow[0] - self.delta_t_us, self.timestamps_flow[0])\n","        elif idx > 0 and idx <= self.__len__():\n","            event_data = self.event_slicer.get_events(\n","                self.timestamps_flow[idx-1], self.timestamps_flow[idx-1] + self.delta_t_us)\n","        else:\n","            raise IndexError\n","\n","        p = event_data['p']\n","        t = event_data['t']\n","        x = event_data['x']\n","        y = event_data['y']\n","\n","        xy_rect = self.rectify_events(x, y)\n","        x_rect = xy_rect[:, 0]\n","        y_rect = xy_rect[:, 1]\n","        return self.events_to_voxel_grid(p, t, x_rect, y_rect)\n","\n","    def get_event_count_image(self, ts_start, ts_end, num_bins, normalize=True):\n","        assert ts_end > ts_start\n","        delta_t_bin = (ts_end - ts_start) / num_bins\n","        ts_start_bin = np.linspace(\n","            ts_start, ts_end, num=num_bins, endpoint=False)\n","        ts_end_bin = ts_start_bin + delta_t_bin\n","        assert abs(ts_end_bin[-1] - ts_end) < 10.\n","        ts_end_bin[-1] = ts_end\n","\n","        event_count = torch.zeros(\n","            (num_bins, self.height, self.width), dtype=torch.float, requires_grad=False)\n","\n","        for i in range(num_bins):\n","            event_data = self.event_slicer.get_events(\n","                ts_start_bin[i], ts_end_bin[i])\n","            p = event_data['p']\n","            t = event_data['t']\n","            x = event_data['x']\n","            y = event_data['y']\n","\n","            t = (t - t[0]).astype('float32')\n","            t = (t/t[-1])\n","            x = x.astype('float32')\n","            y = y.astype('float32')\n","            pol = p.astype('float32')\n","            event_data_torch = {\n","                'p': torch.from_numpy(pol),\n","                't': torch.from_numpy(t),\n","                'x': torch.from_numpy(x),\n","                'y': torch.from_numpy(y),\n","            }\n","            x = event_data_torch['x']\n","            y = event_data_torch['y']\n","            xy_rect = self.rectify_events(x.int(), y.int())\n","            x_rect = torch.from_numpy(xy_rect[:, 0]).long()\n","            y_rect = torch.from_numpy(xy_rect[:, 1]).long()\n","            value = 2*event_data_torch['p']-1\n","            index = self.width*y_rect + x_rect\n","            mask = (x_rect < self.width) & (y_rect < self.height)\n","            event_count[i].put_(index[mask], value[mask], accumulate=True)\n","\n","        return event_count\n","\n","    @staticmethod\n","    def normalize_tensor(event_count):\n","        mask = torch.nonzero(event_count, as_tuple=True)\n","        if mask[0].size()[0] > 0:\n","            mean = event_count[mask].mean()\n","            std = event_count[mask].std()\n","            if std > 0:\n","                event_count[mask] = (event_count[mask] - mean) / std\n","            else:\n","                event_count[mask] = event_count[mask] - mean\n","        return event_count\n","\n","\n","class SequenceRecurrent(Sequence):\n","    def __init__(self, seq_path: Path, representation_type: RepresentationType, mode: str = 'test', delta_t_ms: int = 100,\n","                 num_bins: int = 15, transforms=None, sequence_length=1, name_idx=0, visualize=False, load_gt=False):\n","        super(SequenceRecurrent, self).__init__(seq_path, representation_type, mode, delta_t_ms, transforms=transforms,\n","                                                name_idx=name_idx, visualize=visualize, load_gt=load_gt)\n","        self.crop_size = self.transforms['randomcrop'] if 'randomcrop' in self.transforms else None\n","        self.sequence_length = sequence_length\n","        self.valid_indices = self.get_continuous_sequences()\n","\n","    def get_continuous_sequences(self):\n","        continuous_seq_idcs = []\n","        if self.sequence_length > 1:\n","            for i in range(len(self.timestamps_flow)-self.sequence_length+1):\n","                diff = self.timestamps_flow[i + self.sequence_length-1] - self.timestamps_flow[i]\n","                if diff < np.max([100000 * (self.sequence_length-1) + 1000, 101000]):\n","                    continuous_seq_idcs.append(i)\n","        else:\n","            for i in range(len(self.timestamps_flow)-1):\n","                diff = self.timestamps_flow[i+1] - self.timestamps_flow[i]\n","                if diff < np.max([100000 * (self.sequence_length-1) + 1000, 101000]):\n","                    continuous_seq_idcs.append(i)\n","        return continuous_seq_idcs\n","\n","    def __len__(self):\n","        return len(self.valid_indices)\n","\n","    def __getitem__(self, idx):\n","        assert idx >= 0\n","        assert idx < len(self)\n","\n","        # Valid index is the actual index we want to load, which guarantees a continuous sequence length\n","        valid_idx = self.valid_indices[idx]\n","\n","        sequence = []\n","        j = valid_idx\n","\n","        ts_cur = self.timestamps_flow[j]\n","        # Add first sample\n","        sample = self.get_data_sample(j)\n","        sequence.append(sample)\n","\n","        # Data augmentation according to first sample\n","        crop_window = None\n","        flip = None\n","        if 'crop_window' in sample.keys():\n","            crop_window = sample['crop_window']\n","        if 'flipped' in sample.keys():\n","            flip = sample['flipped']\n","\n","        for i in range(self.sequence_length-1):\n","            j += 1\n","            ts_old = ts_cur\n","            ts_cur = self.timestamps_flow[j]\n","            assert(ts_cur-ts_old < 100000 + 1000)\n","            sample = self.get_data_sample(\n","                j, crop_window=crop_window, flip=flip)\n","            sequence.append(sample)\n","\n","        # Check if the current sample is the first sample of a continuous sequence\n","        if idx == 0 or self.valid_indices[idx]-self.valid_indices[idx-1] != 1:\n","            sequence[0]['new_sequence'] = 1\n","            print(\"Timestamp {} is the first one of the next seq!\".format(\n","                self.timestamps_flow[self.valid_indices[idx]]))\n","        else:\n","            sequence[0]['new_sequence'] = 0\n","\n","        # random crop\n","        if self.crop_size is not None:\n","            i, j, h, w = RandomCrop.get_params(\n","                sample[\"event_volume_old\"], output_size=self.crop_size)\n","            keys_to_crop = [\"event_volume_old\", \"event_volume_new\",\n","                            \"flow_gt_event_volume_old\", \"flow_gt_event_volume_new\",\n","                            \"flow_gt_next\",]\n","\n","            for sample in sequence:\n","                for key, value in sample.items():\n","                    if key in keys_to_crop:\n","                        if isinstance(value, torch.Tensor):\n","                            sample[key] = tf.functional.crop(value, i, j, h, w)\n","                        elif isinstance(value, list) or isinstance(value, tuple):\n","                            sample[key] = [tf.functional.crop(v, i, j, h, w) for v in value]\n","        return sequence\n","\n","\n","class DatasetProvider:\n","    def __init__(self, dataset_path: Path, representation_type: RepresentationType, delta_t_ms: int = 100, num_bins=4, config=None, visualize=False, transforms=None):\n","        test_path = Path(os.path.join(dataset_path, 'test'))\n","        train_path = Path(os.path.join(dataset_path, 'train'))\n","        assert dataset_path.is_dir(), str(dataset_path)\n","        assert test_path.is_dir(), str(test_path)\n","        assert delta_t_ms == 100\n","        self.config = config\n","        self.name_mapper_test = []\n","\n","        if transforms:\n","            self.transforms = transforms\n","        else:\n","            self.transforms = tf.Compose([\n","                transforms.ToTensor(),  # Convert image to PyTorch tensor\n","            ])\n","\n","        # Assemble test sequences\n","        test_sequences = list()\n","        for child in test_path.iterdir():\n","            self.name_mapper_test.append(str(child).split(\"/\")[-1])\n","            test_sequences.append(Sequence(child, representation_type, 'test', delta_t_ms, num_bins,\n","                                               name_idx=len(self.name_mapper_test) - 1,\n","                                               visualize=visualize,\n","                                               transforms=self.transforms))\n","\n","        self.test_dataset = torch.utils.data.ConcatDataset(test_sequences)\n","\n","        # Assemble train sequences\n","        available_seqs = os.listdir(train_path)\n","\n","        seqs = available_seqs\n","\n","        train_sequences: list[Sequence] = []\n","        for seq in seqs:\n","            extra_arg = dict()\n","            train_sequences.append(Sequence(Path(train_path) / seq,\n","                                   representation_type=representation_type, mode=\"train\",\n","                                   load_gt=True, **extra_arg, transforms=self.transforms))\n","            self.train_dataset: torch.utils.data.ConcatDataset[Sequence] = torch.utils.data.ConcatDataset(train_sequences)\n","\n","    def get_test_dataset(self):\n","        return self.test_dataset\n","\n","    def get_train_dataset(self):\n","        return self.train_dataset\n","\n","    def get_name_mapping_test(self):\n","        return self.name_mapper_test\n","\n","    def summary(self, logger):\n","        logger.write_line(\n","            \"================================== Dataloader Summary ====================================\", True)\n","        logger.write_line(\"Loader Type:\\t\\t\" + self.__class__.__name__, True)\n","        logger.write_line(\"Number of Voxel Bins: {}\".format(\n","            self.test_dataset.datasets[0].num_bins), True)\n","        logger.write_line(\"Number of Train Sequences: {}\".format(\n","            len(self.train_dataset)), True)\n","\n","def train_collate(sample_list):\n","    batch = dict()\n","    for field_name in sample_list[0]:\n","        if field_name == 'timestamp':\n","            batch['timestamp'] = [sample[field_name] for sample in sample_list]\n","        if field_name == 'seq_name':\n","            batch['seq_name'] = [sample[field_name] for sample in sample_list]\n","        if field_name == 'new_sequence':\n","            batch['new_sequence'] = [sample[field_name]\n","                                     for sample in sample_list]\n","        if field_name.startswith(\"event_volume\"):\n","            batch[field_name] = torch.stack(\n","                [sample[field_name] for sample in sample_list])\n","        if field_name.startswith(\"flow_gt\") or field_name.startswith('prev_flow_gt') or field_name.startswith('next_flow_gt'):\n","            if all(field_name in x for x in sample_list):\n","                batch[field_name] = torch.stack(\n","                    [sample[field_name][0] for sample in sample_list])\n","                batch[field_name + '_valid_mask'] = torch.stack(\n","                    [sample[field_name][1] for sample in sample_list])\n","\n","    return batch\n","\n","\n","def rec_train_collate(sample_list):\n","    seq_length = len(sample_list[0])\n","    seq_of_batch = []\n","    for i in range(seq_length):\n","        seq_of_batch.append(train_collate(\n","            [sample[i] for sample in sample_list]))\n","    return seq_of_batch"],"metadata":{"trusted":true,"id":"BMr6cf3sGb7Q","executionInfo":{"status":"ok","timestamp":1721113048088,"user_tz":-540,"elapsed":2,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["## PCLNet"],"metadata":{"id":"5MpHeit75O-h"}},{"cell_type":"markdown","source":["## From the github"],"metadata":{"id":"W-FQAttFXDKX"}},{"cell_type":"code","source":["import torch.utils.model_zoo as model_zoo\n","\n","__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n","           'resnet152']\n","\n","model_urls = {\n","    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n","    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n","    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n","    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n","    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n","}\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=1, bias=False)\n","\n","\n","def conv1x1(in_planes, out_planes, stride=1):\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = conv1x1(inplanes, planes)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = conv1x1(planes, planes * self.expansion)\n","        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","\n","    def __init__(self, block, layers, num_classes=1000):\n","        super(ResNet, self).__init__()\n","        self.inplanes = 64\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n","                               bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 64, layers[0])\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n","        # self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        # self.fc = nn.Linear(512 * block.expansion, num_classes)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","    def _make_layer(self, block, planes, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                conv1x1(self.inplanes, planes * block.expansion, stride),\n","                nn.BatchNorm2d(planes * block.expansion),\n","            )\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride, downsample))\n","        self.inplanes = planes * block.expansion\n","        for _ in range(1, blocks):\n","            layers.append(block(self.inplanes, planes))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        endpoint = []\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        endpoint.append(x) # output here\n","\n","        x = self.maxpool(x)\n","        x = self.layer1(x)\n","        endpoint.append(x)\n","        x = self.layer2(x)\n","        endpoint.append(x)\n","        x = self.layer3(x)\n","        endpoint.append(x)\n","        x = self.layer4(x)\n","        endpoint.append(x)\n","\n","        return endpoint\n","\n","\n","def resnet18(pretrained=False, **kwargs):\n","    \"\"\"Constructs a ResNet-18 model.\n","\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","    \"\"\"\n","    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n","    if pretrained:\n","        state = model.state_dict()\n","        state_ckp = model_zoo.load_url(model_urls['resnet18'])\n","        cnt = 0\n","        for k, val in state_ckp.items():\n","            if k in state.keys():\n","                state[k] = val\n","                cnt += 1\n","        model.load_state_dict(state)\n","        print (\"RestNet checkpoint loaded: %d\" % cnt)\n","    return model\n","\n","\n","def resnet34(pretrained=False, **kwargs):\n","    \"\"\"Constructs a ResNet-34 model.\n","\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","    \"\"\"\n","    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n","    if pretrained:\n","        state = model.state_dict()\n","        state_ckp = model_zoo.load_url(model_urls['resnet34'])\n","        cnt = 0\n","        for k, val in state_ckp.items():\n","            if k in state.keys():\n","                state[k] = val\n","                cnt += 1\n","        model.load_state_dict(state)\n","        print (\"RestNet checkpoint loaded: %d\" % cnt)\n","    return model\n","\n","\n","def resnet50(pretrained=False, **kwargs):\n","    \"\"\"Constructs a ResNet-50 model.\n","\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","    \"\"\"\n","    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n","    if pretrained:\n","        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n","    return model\n","\n","\n","def resnet101(pretrained=False, **kwargs):\n","    \"\"\"Constructs a ResNet-101 model.\n","\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","    \"\"\"\n","    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n","    if pretrained:\n","        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n","    return model\n","\n","\n","def resnet152(pretrained=False, **kwargs):\n","    \"\"\"Constructs a ResNet-152 model.\n","\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","    \"\"\"\n","    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n","    if pretrained:\n","        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n","    return model"],"metadata":{"id":"8R-omgjZS8n0","executionInfo":{"status":"ok","timestamp":1721113055425,"user_tz":-540,"elapsed":265,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["from torch.autograd import Variable\n","\n","class ConvLSTMCell(nn.Module):\n","    def __init__(self, input_channels, hidden_channels, kernel_size, bias=True):\n","        super(ConvLSTMCell, self).__init__()\n","\n","        assert hidden_channels % 2 == 0\n","\n","        self.input_channels = input_channels\n","        self.hidden_channels = hidden_channels\n","        self.bias = bias\n","        self.kernel_size = kernel_size\n","\n","        self.padding = int((kernel_size - 1) / 2)\n","        self.Gates = nn.Conv2d(self.input_channels + self.hidden_channels , 4*self.hidden_channels,\n","                self.kernel_size, 1, self.padding, bias=True)\n","\n","    def forward(self, x, h, c):\n","\n","        stacked_inputs = torch.cat((x, h), 1)\n","        gates = self.Gates(stacked_inputs)\n","\n","        # chunk across the channel dimension\n","        xi, xf, xo, xg = gates.chunk(4, 1)\n","\n","        # apply sigmoid non linearity\n","        xi = torch.sigmoid(xi)\n","        xf = torch.sigmoid(xf)\n","        xo = torch.sigmoid(xo)\n","        xg = torch.tanh(xg)\n","\n","        # compute current cell and hidden state\n","        c = (xf * c) + (xi * xg)\n","        h = xo * torch.tanh(c)\n","\n","        return h, c\n","\n","    def init_hidden(self, batch_size, hidden, shape):\n","        return (Variable(torch.zeros(batch_size, hidden, shape[0], shape[1])),\n","                Variable(torch.zeros(batch_size, hidden, shape[0], shape[1])))\n","\n","\n","class ConvLSTM(nn.Module):\n","    def __init__(self, input_channels, hidden_channels, kernel_size, step=1, effective_step=[1], bias=True):\n","        super(ConvLSTM, self).__init__()\n","        self.input_channels = [input_channels] + hidden_channels\n","        self.hidden_channels = hidden_channels\n","        self.kernel_size = kernel_size\n","        self.num_layers = len(hidden_channels)\n","        self.step = step\n","        self.bias = bias\n","        self.effective_step = effective_step\n","        self._all_layers = []\n","        for i in range(self.num_layers):\n","            name = 'cell{}'.format(i)\n","            cell = ConvLSTMCell(self.input_channels[i], self.hidden_channels[i], self.kernel_size, self.bias)\n","            setattr(self, name, cell)\n","            self._all_layers.append(cell)\n","\n","    def forward(self, input):\n","        #input : (num, seq_len, channel, H,W)\n","        internal_state = []\n","        outputs = []\n","        for step in range(self.step):\n","            x = input[:, step, :,:,:]\n","            for i in range(self.num_layers):\n","                # all cells are initialized in the first step\n","                name = 'cell{}'.format(i)\n","                if step == 0:\n","                    bsize, _, height, width = x.size()\n","                    (h, c) = getattr(self, name).init_hidden(batch_size=bsize, hidden=self.hidden_channels[i],\n","                            shape=(height, width))\n","                    internal_state.append((h, c))\n","\n","                # do forward\n","                (h, c) = internal_state[i]\n","                x, new_c = getattr(self, name)(x, h, c)\n","                internal_state[i] = (x, new_c)\n","            # only record effective steps\n","            if step in self.effective_step:\n","                outputs.append(x)\n","        return outputs, (x, new_c)"],"metadata":{"id":"yD2nx5HATAgK","executionInfo":{"status":"ok","timestamp":1721113056772,"user_tz":-540,"elapsed":257,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1):\n","    if type(in_planes) == np.int64:\n","        in_planes = np.asscalar(in_planes)\n","    return nn.Sequential(\n","        nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n","                  padding=padding, dilation=dilation, bias=True),\n","        nn.LeakyReLU(0.1))\n","\n","\n","def predict_flow(in_planes):\n","    if type(in_planes) == np.int64:\n","        in_planes = np.asscalar(in_planes)\n","    return nn.Conv2d(in_planes, 2, kernel_size=3, stride=1, padding=1, bias=True)\n","\n","\n","def deconv(in_planes, out_planes, kernel_size=4, stride=2, padding=1):\n","    if type(in_planes) == np.int64:\n","        in_planes = np.asscalar(in_planes)\n","    return nn.ConvTranspose2d(in_planes, out_planes, kernel_size, stride, padding, bias=True)\n","\n","\n","class PCLNet(nn.Module):\n","    \"\"\"\n","    PCLNet: Unsupervised Learning for Optical Flow Estimation Using Pyramid Convolution LSTM\n","    Author: Shuosen Guan\n","    \"\"\"\n","\n","    def __init__(self, args):\n","\n","        super(PCLNet, self).__init__()\n","        self.args = args\n","\n","        snippet_len = args.snippet_len\n","        self.feature_net = eval(args.backbone)(pretrained=True, num_classes=args.class_num)\n","        if args.freeze_vgg:\n","            for p in self.feature_net.parameters():\n","                p.required_grad = False\n","            print(\"[>>>> Feature head frozen.<<<<]\")\n","\n","        # Motion Encoding\n","        # in_size: 1/2\n","        self.clstm_encoder_1 = ConvLSTM(input_channels=64, hidden_channels=[64],\n","                                        kernel_size=3, step=snippet_len, effective_step=list(range(snippet_len)))\n","        # in_size: 1/4\n","        self.clstm_encoder_2 = ConvLSTM(input_channels=64, hidden_channels=[64],\n","                                        kernel_size=3, step=snippet_len, effective_step=list(range(snippet_len)))\n","        # in_size: 1/8\n","        self.clstm_encoder_3 = ConvLSTM(input_channels=128, hidden_channels=[128],\n","                                        kernel_size=3, step=snippet_len, effective_step=list(range(snippet_len)))\n","        # in_size: 1/16\n","        self.clstm_encoder_4 = ConvLSTM(input_channels=256, hidden_channels=[256],\n","                                        kernel_size=3, step=snippet_len, effective_step=list(range(snippet_len)))\n","\n","        self.conv_B1    = conv(64, 64, stride=1, kernel_size=3, padding=1)\n","        self.conv_S1_1  = conv(64, 64, stride=1, kernel_size=3, padding=1)\n","        self.conv_S1_2  = conv(64, 64, stride=1, kernel_size=3, padding=1)\n","        self.conv_D1    = conv(64, 64, stride=2)\n","        self.Pool1      = nn.MaxPool2d(8, 8)\n","\n","        self.conv_B2    = conv(64, 64, stride=1, kernel_size=3, padding=1)\n","        self.conv_S2_1  = conv(64 + 64, 128, stride=1, kernel_size=3, padding=1)\n","        self.conv_S2_2  = conv(128, 128, stride=1, kernel_size=3, padding=1)\n","        self.conv_D2    = conv(128, 64, stride=2)\n","        self.Pool2      = nn.MaxPool2d(4, 4)\n","\n","        self.conv_B3    = conv(128, 128, stride=1, kernel_size=3, padding=1)\n","        self.conv_S3_1  = conv(128 + 64, 128, stride=1, kernel_size=3, padding=1)\n","        self.conv_S3_2  = conv(128, 128, stride=1, kernel_size=3, padding=1)\n","        self.conv_D3    = conv(128, 64, stride=2)\n","        self.Pool3      = nn.MaxPool2d(2, 2)\n","\n","        self.conv_B4    = conv(256, 128, stride=1, kernel_size=3, padding=1)\n","        self.conv_S4_1  = conv(128 + 64, 128, stride=1, kernel_size=3, padding=1)\n","        self.conv_S4_2  = conv(128, 128, stride=1, kernel_size=3, padding=1)\n","\n","        # Motion feature\n","        self.conv_M = conv((64 + 128 + 128 + 128), 256, stride=1, kernel_size=3, padding=1)\n","\n","        # Motion reconstruction\n","        if self.args.couple:\n","            rec_in_size = [0, 64 + 64 + 2, 128 + 128 + 2, 128 + 196 + 2, 128 + 256]\n","        else:\n","            rec_in_size = [0, 64 + 2, 128 + 2, 196 + 2, 256]\n","\n","        self.conv_4     = conv(rec_in_size[4], 256)\n","        self.pred_flow4 = predict_flow(256)\n","        self.up_flow4   = deconv(2, 2)\n","        self.up_feat4   = deconv(256, 196)\n","\n","        self.conv_3     = conv(rec_in_size[3], 196)\n","        self.pred_flow3 = predict_flow(196)\n","        self.up_flow3   = deconv(2, 2)\n","        self.up_feat3   = deconv(196, 128)\n","\n","        self.conv_2     = conv(rec_in_size[2], 96)\n","        self.pred_flow2 = predict_flow(96)\n","        self.up_flow2   = conv(2, 2)\n","        self.up_feat2   = conv(96, 64)\n","\n","        self.conv_1     = conv(rec_in_size[1], 64)\n","        self.pred_flow1 = predict_flow(64)\n","\n","        self.dc_conv1 = conv(64, 64, kernel_size=3, stride=1, padding=1, dilation=1)\n","        self.dc_conv2 = conv(64, 64, kernel_size=3, stride=1, padding=2, dilation=2)\n","        self.dc_conv3 = conv(64, 64, kernel_size=3, stride=1, padding=4, dilation=4)\n","        self.dc_conv4 = conv(64, 64, kernel_size=3, stride=1, padding=8, dilation=8)\n","        self.dc_conv5 = conv(64, 64, kernel_size=3, stride=1, padding=16, dilation=16)\n","        self.dc_conv6 = conv(64, 32, kernel_size=3, stride=1, padding=1, dilation=1)\n","        self.dc_conv7 = predict_flow(32)\n","\n","\n","    def forward(self, x):\n","\n","        if x.dim() == 6:    # (batch_size, K, snippet_len, channel, H, W)\n","            batch_size, K, snippet_len, channel, H, W = x.size()\n","        elif x.dim() == 5:  # (batch_size, snippet_len, channel, H, W)\n","            batch_size, snippet_len, channel, H, W = x.size()\n","            K = 1\n","        elif x.dim() == 4:  # (batch_size, channel * snippet_len, H, W)\n","            batch_size, _channels, H, W = x.size()\n","            K, channel = 1, 3\n","            snippet_len = _channels // channel\n","        else:\n","            raise RuntimeError('Input format not suppored!')\n","\n","        x = x.contiguous().view(-1, channel, H, W)\n","\n","        la1, la2, la3, la4, _ = self.feature_net(x)\n","\n","        la1 = la1.view((-1, snippet_len) + la1.size()[1:])\n","        la2 = la2.view((-1, snippet_len) + la2.size()[1:])\n","        la3 = la3.view((-1, snippet_len) + la3.size()[1:])\n","        la4 = la4.view((-1, snippet_len) + la4.size()[1:])\n","        # la5 = la5.view((-1, snippet_len) + la5.size()[1:])\n","\n","        h1, _ = self.clstm_encoder_1(la1)\n","        h2, _ = self.clstm_encoder_2(la2)\n","        h3, _ = self.clstm_encoder_3(la3)\n","        h4, _ = self.clstm_encoder_4(la4)\n","        # list for each step (batch_size * K, channel, H, W)\n","\n","        # (batch_size * K*(snippet_len -1), channel, H, W)\n","        h1 = torch.stack(h1[1:], 1).view((-1,) + h1[0].size()[-3:])\n","        h2 = torch.stack(h2[1:], 1).view((-1,) + h2[0].size()[-3:])\n","        h3 = torch.stack(h3[1:], 1).view((-1,) + h3[0].size()[-3:])\n","        h4 = torch.stack(h4[1:], 1).view((-1,) + h4[0].size()[-3:])\n","\n","        x1 = self.conv_B1(h1)\n","        x1 = self.conv_S1_2(self.conv_S1_1(x1))\n","\n","        x2 = torch.cat((self.conv_B2(h2), self.conv_D1(x1)), 1)\n","        x2 = self.conv_S2_2(self.conv_S2_1(x2))\n","\n","        x3 = torch.cat((self.conv_B3(h3), self.conv_D2(x2)), 1)\n","        x3 = self.conv_S3_2(self.conv_S3_1(x3))\n","\n","        x4 = torch.cat((self.conv_B4(h4), self.conv_D3(x3)), 1)\n","        x4 = self.conv_S4_2(self.conv_S4_1(x4))\n","\n","        xm = self.conv_M(torch.cat((self.Pool1(x1), self.Pool2(x2), self.Pool3(x3), x4), 1))\n","\n","        rec_x4 = torch.cat((x4, xm), 1) if self.args.couple else xm\n","        x = self.conv_4(rec_x4)\n","        flow4 = self.pred_flow4(x)\n","        up_flow4 = self.up_flow4(flow4)\n","        up_feat4 = self.up_feat4(x)\n","\n","        rec_x3 = torch.cat((x3, up_feat4, up_flow4), 1) if self.args.couple else torch.cat((up_feat4, up_flow4), 1)\n","        x = self.conv_3(rec_x3)\n","        flow3 = self.pred_flow3(x)\n","        up_flow3 = self.up_flow3(flow3)\n","        up_feat3 = self.up_feat3(x)\n","\n","        rec_x2 = torch.cat((x2, up_feat3, up_flow3), 1) if self.args.couple else torch.cat((up_feat3, up_flow3), 1)\n","        x = self.conv_2(rec_x2)\n","        flow2 = self.pred_flow2(x)\n","        up_flow2 = self.up_flow2(flow2)\n","        up_feat2 = self.up_feat2(x)\n","\n","        rec_x1 = torch.cat((x1, up_feat2, up_flow2), 1) if self.args.couple else torch.cat((up_feat2, up_flow2), 1)\n","        x = self.conv_1(rec_x1)\n","        flow1 = self.pred_flow1(x)\n","\n","        x = self.dc_conv4(self.dc_conv3(self.dc_conv2(self.dc_conv1(x))))\n","        flow1 += self.dc_conv7(self.dc_conv6(self.dc_conv5(x)))\n","\n","        # output size: (batch_size, K, snippet_len -1 , C,H,W)\n","        flow_pyramid = [flo.view((batch_size, K, snippet_len - 1,) + flo.size()[-3:])\n","                        for flo in [flow1, flow2, flow3, flow4]]\n","        re_dict = {}\n","        re_dict['flow_pyramid'] = flow_pyramid\n","\n","        return re_dict"],"metadata":{"id":"yu7rm0heTLXj","executionInfo":{"status":"ok","timestamp":1721113182961,"user_tz":-540,"elapsed":286,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["## Arguments & Definition"],"metadata":{"id":"u9miP9yJXFRV"}},{"cell_type":"code","source":["import argparse\n","args = argparse.Namespace(\n","    name='pclnet',\n","    snippet_len=2,\n","    backbone='resnet18',\n","    class_num=101,\n","    freeze_vgg=True,\n","    couple=False,\n","\n","    lr=0.01, # 2e-5\n","    num_steps=100000,\n","    batch_size=1,\n","    image_size=[480, 640],\n","    mixed_precision=False,\n","    iters=12,\n","    wdecay=0.00005,\n","    epsilon=1e-8,\n","    clip=1.0,\n","    dropout=0.0,\n","    gamma=0.8,\n","    add_noise=False,\n","    seed=27,\n","    dataset_path='data/',\n",")"],"metadata":{"id":"QL51N2RHAqGh","executionInfo":{"status":"ok","timestamp":1721113550988,"user_tz":-540,"elapsed":258,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["model = torch.nn.DataParallel(PCLNet(args).cpu())"],"metadata":{"id":"jObX__fv-JYD","executionInfo":{"status":"ok","timestamp":1721113555744,"user_tz":-540,"elapsed":953,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b218abd2-7508-4717-a2ff-4316970be9a7"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["RestNet checkpoint loaded: 100\n","[>>>> Feature head frozen.<<<<]\n"]}]},{"cell_type":"code","source":["input_data = torch.randn(16, 4, 3, 480, 640)"],"metadata":{"id":"ttE1S3q_Zpse","executionInfo":{"status":"ok","timestamp":1721114208383,"user_tz":-540,"elapsed":1820,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["batch_size, snippet_len, channel, H, W = input_data.size()"],"metadata":{"id":"6mso19DdZqEe","executionInfo":{"status":"ok","timestamp":1721114216185,"user_tz":-540,"elapsed":292,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["input_data = input_data.permute(0, 2, 1, 3, 4).contiguous().view(batch_size, snippet_len, channel, H, W)\n","input_data.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DZzRBRo2ZsXh","executionInfo":{"status":"ok","timestamp":1721114226398,"user_tz":-540,"elapsed":716,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}},"outputId":"ac6afa0d-72bc-436c-ff6c-7b91043c007c"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([16, 4, 3, 480, 640])"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["output = model(input_data)"],"metadata":{"id":"gTkQUhbqZyNK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Image Preprocessing"],"metadata":{"id":"Asa6m7ccKl4E"}},{"cell_type":"code","source":["from sklearn.decomposition import PCA, SparsePCA, TruncatedSVD\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","\n","class DimensionalityReduction:\n","    def __init__(self, n_components, scaler=MinMaxScaler(), red_technique='pca'):\n","        self.scaler = scaler # if None, then doesn't perform any scaling\n","        self.n_components = n_components\n","\n","        match red_technique:\n","            case 'pca':\n","                self.technique = PCA(n_components=n_components)\n","            case 'sparsepca':\n","                self.technique = SparsePCA(n_components=n_components)\n","            case 'tsvd':\n","                self.technique = TruncatedSVD(n_components=n_components)\n","            case _:\n","                raise NotImplementedError\n","\n","    def __call__(self, image_tensor: torch.Tensor):\n","        # this is for a single image, not a batch of images\n","        C, H, W = image_tensor.shape\n","\n","        tensor_reshaped = image_tensor.cpu().numpy().reshape(C, -1).T\n","        if self.scaler is not None:\n","            tensor_reshaped = self.scaler.fit_transform(tensor_reshaped)\n","\n","        reduced_tensor_reshaped = self.technique.fit_transform(tensor_reshaped)\n","        reduced_tensor_reshaped = reduced_tensor_reshaped.T.reshape(self.n_components, H, W)\n","\n","        reduced_tensor = torch.tensor(reduced_tensor_reshaped, device=image_tensor.device)\n","        return reduced_tensor"],"metadata":{"id":"J3qppdpsKrZc","executionInfo":{"status":"ok","timestamp":1721113475220,"user_tz":-540,"elapsed":3224,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["```python\n","a = torch.rand((4, 480, 640))\n","b = DimensionalityReduction(n_components=3, scaler=MinMaxScaler(), red_technique='tsvd')(a)\n","b.shape\n","```"],"metadata":{"id":"PGpEV1M23pyC"}},{"cell_type":"code","source":["# Needs to have 3 channels or color passed in\n","# runs into the problem of loss of information in dimensionality reduction\n","class HistogramEqualization:\n","    def __call__(self, img):\n","        return tf.functional.equalize(img)"],"metadata":{"id":"jt6kTo6xKip7","executionInfo":{"status":"ok","timestamp":1721113475220,"user_tz":-540,"elapsed":2,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["class CombinedTransform:\n","    def __init__(self, transform=tf.Compose([ tf.ToTensor() ])):\n","        self.transform = transform\n","\n","    def __call__(self, flow_dict):\n","        seed = np.random.randint(2147483647)\n","\n","        # Just in case I decide to add more feature columns\n","        feaure_flow_columns = [c for c in flow_dict.keys() if 'event_volume' in c]\n","\n","        for col in feaure_flow_columns:\n","            torch.manual_seed(seed)\n","\n","            if type(flow_dict[col]) == list:\n","                flow_dict[col] = [ self.transform(img) for img in flow_dict[col] ]\n","            else:\n","                flow_dict[col] = self.transform(flow_dict[col])\n","\n","        return flow_dict\n","\n","combined_transform = CombinedTransform(\n","    transform=tf.Compose([\n","        tf.GaussianBlur(kernel_size=(5, 5)),\n","    ])\n",")"],"metadata":{"trusted":true,"id":"q2pCHbMaGb7T","executionInfo":{"status":"ok","timestamp":1721113475220,"user_tz":-540,"elapsed":1,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["## `main.py`"],"metadata":{"id":"-W_HpclqGb7T"}},{"cell_type":"markdown","source":["Instead of changing the `Sequence`, just change how the data are loaded. Or iterate over the dataloader so that you use many images at once."],"metadata":{"id":"3lufwoSn32eN"}},{"cell_type":"code","source":["class RepresentationType(Enum):\n","    VOXEL = auto()\n","    STEPAN = auto()\n","\n","def set_seed(seed):\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","\n","def compute_epe_error(pred_flow: torch.Tensor, gt_flow: torch.Tensor):\n","    '''\n","    end-point-error (ground truthと予測値の二乗誤差)を計算\n","    pred_flow: torch.Tensor, Shape: torch.Size([B, 2, 480, 640]) => 予測したオプティカルフローデータ\n","    gt_flow: torch.Tensor, Shape: torch.Size([B, 2, 480, 640]) => 正解のオプティカルフローデータ\n","    '''\n","    epe = torch.mean(torch.mean(torch.norm(pred_flow - gt_flow, p=2, dim=1), dim=(1, 2)), dim=0)\n","    return epe\n","\n","def save_optical_flow_to_npy(flow: torch.Tensor, file_name: str):\n","    '''\n","    optical flowをnpyファイルに保存\n","    flow: torch.Tensor, Shape: torch.Size([2, 480, 640]) => オプティカルフローデータ\n","    file_name: str => ファイル名\n","    '''\n","    np.save(f\"{file_name}.npy\", flow.cpu().numpy())"],"metadata":{"trusted":true,"id":"k-O8nvDQGb7T","executionInfo":{"status":"ok","timestamp":1721113560812,"user_tz":-540,"elapsed":249,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["set_seed(args.seed)\n","\n","'''\n","    ディレクトリ構造:\n","\n","    data\n","    ├─test\n","    |  ├─test_city\n","    |  |    ├─events_left\n","    |  |    |   ├─events.h5\n","    |  |    |   └─rectify_map.h5\n","    |  |    └─forward_timestamps.txt\n","    └─train\n","        ├─zurich_city_11_a\n","        |    ├─events_left\n","        |    |       ├─ events.h5\n","        |    |       └─ rectify_map.h5\n","        |    ├─ flow_forward\n","        |    |       ├─ 000134.png\n","        |    |       |.....\n","        |    └─ forward_timestamps.txt\n","        ├─zurich_city_11_b\n","        └─zurich_city_11_c\n","    '''\n","\n","# ------------------\n","#    Dataloader\n","# ------------------\n","\n","loader = DatasetProvider(\n","    dataset_path=Path(args.dataset_path),\n","    representation_type=RepresentationType.VOXEL,\n","    delta_t_ms=100,\n","    num_bins=4,\n","    transforms=combined_transform # Custom class\n",")\n","train_set = loader.get_train_dataset()\n","test_set = loader.get_test_dataset()\n","\n","# def split_train_valid(dataset):\n","#     train_indices = []\n","#     valid_indices = []\n","#     for idx in range(len(dataset)):\n","#         sample = dataset[idx]\n","#         if 'flow_gt_valid_mask' in sample and sample['flow_gt_valid_mask'].all():\n","#             valid_indices.append(idx)\n","#         else:\n","#             train_indices.append(idx)\n","#     train_subset = torch.utils.data.Subset(dataset, train_indices)\n","#     valid_subset = torch.utils.data.Subset(dataset, valid_indices)\n","#     return train_subset, valid_subset\n","\n","# train_set_split, valid_set_split = split_train_valid(train_set)\n","\n","collate_fn = train_collate\n","train_data = DataLoader(train_set, # train_set_split\n","                        batch_size=16, #\n","                        shuffle=False,\n","                        collate_fn=collate_fn,\n","                        drop_last=False,\n","                        num_workers=os.cpu_count(),\n","                        pin_memory=True)\n","test_data = DataLoader(test_set,\n","                       batch_size=1,\n","                       shuffle=False,\n","                       collate_fn=collate_fn,\n","                       drop_last=False,\n","                       num_workers=os.cpu_count(),\n","                       pin_memory=True)\n","\n","'''\n","train data:\n","    Type of batch: Dict\n","    Key: seq_name, Type: list\n","    Key: event_volume, Type: torch.Tensor, Shape: torch.Size([Batch, 4, 480, 640]) => イベントデータのバッチ\n","    Key: flow_gt, Type: torch.Tensor, Shape: torch.Size([Batch, 2, 480, 640]) => オプティカルフローデータのバッチ\n","    Key: flow_gt_valid_mask, Type: torch.Tensor, Shape: torch.Size([Batch, 1, 480, 640]) => オプティカルフローデータのvalid. ベースラインでは使わない\n","\n","test data:\n","    Type of batch: Dict\n","    Key: seq_name, Type: list\n","    Key: event_volume, Type: torch.Tensor, Shape: torch.Size([Batch, 4, 480, 640]) => イベントデータのバッチ\n","'''"],"metadata":{"trusted":true,"id":"2PL6GvxuGb7T","executionInfo":{"status":"ok","timestamp":1721113804050,"user_tz":-540,"elapsed":510,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}},"colab":{"base_uri":"https://localhost:8080/","height":107},"outputId":"74b528f7-b0d0-44a0-ae88-5323f2c0510a"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ntrain data:\\n    Type of batch: Dict\\n    Key: seq_name, Type: list\\n    Key: event_volume, Type: torch.Tensor, Shape: torch.Size([Batch, 4, 480, 640]) => イベントデータのバッチ\\n    Key: flow_gt, Type: torch.Tensor, Shape: torch.Size([Batch, 2, 480, 640]) => オプティカルフローデータのバッチ\\n    Key: flow_gt_valid_mask, Type: torch.Tensor, Shape: torch.Size([Batch, 1, 480, 640]) => オプティカルフローデータのvalid. ベースラインでは使わない\\n\\ntest data:\\n    Type of batch: Dict\\n    Key: seq_name, Type: list\\n    Key: event_volume, Type: torch.Tensor, Shape: torch.Size([Batch, 4, 480, 640]) => イベントデータのバッチ\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["# ------------------\n","#   optimizer\n","# ------------------\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.01, weight_decay=args.wdecay, eps=args.epsilon)\n","scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.9)\n","# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, args.lr, args.num_steps + 100,\n","#                                                 pct_start=0.05, cycle_momentum=False, anneal_strategy='linear')\n","\n","loss_fn = TotalLoss(smoothness_weight=0.5)"],"metadata":{"id":"Nwxpx4RlGdQ4","executionInfo":{"status":"ok","timestamp":1721113858220,"user_tz":-540,"elapsed":294,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["# num_epochs = args.train.epochs\n","num_epochs = 1\n","\n","epe_losses = [[] for _ in range(num_epochs)]\n","overall_losses = [[] for _ in range(num_epochs)]"],"metadata":{"trusted":true,"id":"XOEcpMPHGb7T","executionInfo":{"status":"ok","timestamp":1721113859391,"user_tz":-540,"elapsed":0,"user":{"displayName":"Not Applicable","userId":"10607765742985119765"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["BATCH_CONCAT = 2"],"metadata":{"id":"_aYlJLJa0Yqf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def reduce_batch_dim(input_tensor: torch.Tensor, dim: int):\n","    B, C, H, W = input_tensor.shape\n","    tensor_reshaped = input_tensor.permute(0, 2, 3, 1).reshape(B * H * W, C)\n","\n","    tsvd = TruncatedSVD(n_components=dim)\n","    reduced_tensor_reshaped = tsvd.fit_transform(tensor_reshaped)\n","\n","    reduced_tensor = torch.tensor(reduced_tensor_reshaped, device=input_tensor.device).reshape(B, H, W, dim).permute(0, 3, 1, 2)\n","\n","    return reduced_tensor"],"metadata":{"id":"9Brph0qa_hj2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ------------------\n","#   Start training\n","# ------------------\n","model.train()\n","\n","for epoch in range(num_epochs):\n","\n","    total_loss = 0\n","    prev_event_volumes = [torch.zeros([args.batch_size, 3, 480, 640])] # Acts as a queue\n","\n","    print(\"on epoch: {}\".format(epoch + 1))\n","    for i, batch in enumerate(tqdm(train_data)):\n","\n","        try:\n","            batch: Dict[str, Any]\n","\n","            event_image = batch[\"event_volume\"].to(device) # [B, 3, 480, 640]\n","            ground_truth_flow = batch[\"flow_gt\"].to(device) # [B, 2, 480, 640]\n","\n","            prev_event_volumes.append(event_image)\n","\n","            prev_ground_truth_flow = batch['prev_flow_gt'].to(device) # [B, 2, 480, 640]\n","            next_ground_truth_flow = batch['next_flow_gt'].to(device) # [B, 2, 480, 640]\n","\n","            flows = model(prev_event_volumes[0],\n","                         prev_event_volumes[1],\n","                         iters=args.iters) # [B, 3, 480, 640]\n","\n","            # Overall loss requires flow0, ..., flow3 so we don't implement it here\n","            # What if you created flow_dict from flow0, ..., flow11 (n=12) to and then use overall loss?\n","\n","            print()\n","            for j, flow in enumerate(flows):\n","                print(f'batch {i} | flow #{j + 1} | EPE LOSS:', compute_epe_error(flow, ground_truth_flow).item())\n","\n","            avg_flow = torch.mean(torch.stack(flows, dim=0), dim=0)\n","            epe_loss: torch.Tensor = compute_epe_error(avg_flow, ground_truth_flow)\n","\n","            print(f\"batch {i} average EPE LOSS: {epe_loss.item()}\")\n","            epe_losses[epoch].append(epe_loss.item())\n","\n","            optimizer.zero_grad()\n","            epe_loss.backward() # Change this to which loss function is to be updated\n","            optimizer.step()\n","\n","            total_loss += epe_loss.item() # This too\n","\n","            if len(prev_event_volumes) >= BATCH_CONCAT:\n","                prev_event_volumes.pop(0) # Remove first element\n","\n","        except KeyboardInterrupt:\n","            current_time = time.strftime(\"%Y%m%d-%H%M%S\")\n","            model_path = f\"../../models/model_{current_time}.pth\"\n","            torch.save(model.state_dict(), model_path)\n","            print(f\"Model saved to {model_path}\")\n","\n","            raise SystemExit(\"KeyboardInterrupt\")\n","\n","    scheduler.step()\n","\n","    print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_data)}')"],"metadata":{"trusted":true,"id":"YbeX9fnmGb7U","executionInfo":{"status":"error","timestamp":1721017110775,"user_tz":-540,"elapsed":3436000,"user":{"displayName":"Ariya Narayanasamy","userId":"11235920312618626542"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"e976b3ef-311f-4b54-eb5e-2ba69ac20bc6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["on epoch: 1\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/336 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"]},{"output_type":"stream","name":"stdout","text":["batch 0 | flow #1 | EPE LOSS: 8.665224853998806\n","batch 0 | flow #2 | EPE LOSS: 8.655190191741282\n","batch 0 | flow #3 | EPE LOSS: 7.620202816595744\n","batch 0 | flow #4 | EPE LOSS: 6.592811228633178\n","batch 0 | flow #5 | EPE LOSS: 6.015046058273764\n","batch 0 | flow #6 | EPE LOSS: 5.615552434148813\n","batch 0 | flow #7 | EPE LOSS: 5.35362252625018\n","batch 0 | flow #8 | EPE LOSS: 5.050601268962979\n","batch 0 | flow #9 | EPE LOSS: 4.617178101805345\n","batch 0 | flow #10 | EPE LOSS: 4.127171727828501\n","batch 0 | flow #11 | EPE LOSS: 3.7053065108698235\n","batch 0 | flow #12 | EPE LOSS: 3.3722322695325073\n","batch 0 average EPE LOSS: 4.8400707484085475\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 1/336 [05:01<28:04:33, 301.71s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 1 | flow #1 | EPE LOSS: 19.14768372354231\n","batch 1 | flow #2 | EPE LOSS: 19.589359778340427\n","batch 1 | flow #3 | EPE LOSS: 21.62149647547366\n","batch 1 | flow #4 | EPE LOSS: 24.295078936330096\n","batch 1 | flow #5 | EPE LOSS: 26.398977671458738\n","batch 1 | flow #6 | EPE LOSS: 27.583953103598787\n","batch 1 | flow #7 | EPE LOSS: 28.201270189883903\n","batch 1 | flow #8 | EPE LOSS: 28.740880142867905\n","batch 1 | flow #9 | EPE LOSS: 29.16502199973344\n","batch 1 | flow #10 | EPE LOSS: 29.51449314723368\n","batch 1 | flow #11 | EPE LOSS: 29.788746403239944\n","batch 1 | flow #12 | EPE LOSS: 29.993694205263115\n","batch 1 average EPE LOSS: 25.142931643099118\n"]},{"output_type":"stream","name":"stderr","text":["\r  1%|          | 2/336 [05:20<12:32:55, 135.26s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 2 | flow #1 | EPE LOSS: 21.487205259667366\n","batch 2 | flow #2 | EPE LOSS: 22.848478155866605\n","batch 2 | flow #3 | EPE LOSS: 24.17999642029018\n","batch 2 | flow #4 | EPE LOSS: 25.42630798496165\n","batch 2 | flow #5 | EPE LOSS: 26.10369248688848\n","batch 2 | flow #6 | EPE LOSS: 25.966448073942633\n","batch 2 | flow #7 | EPE LOSS: 25.873290215861477\n","batch 2 | flow #8 | EPE LOSS: 25.918628033867876\n","batch 2 | flow #9 | EPE LOSS: 25.927862147490284\n","batch 2 | flow #10 | EPE LOSS: 25.889152441184432\n","batch 2 | flow #11 | EPE LOSS: 25.769820787020084\n","batch 2 | flow #12 | EPE LOSS: 25.551100401415454\n","batch 2 average EPE LOSS: 23.49068306209148\n"]},{"output_type":"stream","name":"stderr","text":["\r  1%|          | 3/336 [05:37<7:31:09, 81.29s/it]  "]},{"output_type":"stream","name":"stdout","text":["batch 3 | flow #1 | EPE LOSS: 19.678770558393467\n","batch 3 | flow #2 | EPE LOSS: 19.283080646158542\n","batch 3 | flow #3 | EPE LOSS: 17.935461874104348\n","batch 3 | flow #4 | EPE LOSS: 16.6370024894197\n","batch 3 | flow #5 | EPE LOSS: 15.365397239613422\n","batch 3 | flow #6 | EPE LOSS: 14.350191487901368\n","batch 3 | flow #7 | EPE LOSS: 13.958713375175693\n","batch 3 | flow #8 | EPE LOSS: 13.813237108320136\n","batch 3 | flow #9 | EPE LOSS: 13.73043774507358\n","batch 3 | flow #10 | EPE LOSS: 13.765154430134979\n","batch 3 | flow #11 | EPE LOSS: 14.027499537807316\n","batch 3 | flow #12 | EPE LOSS: 14.490882762533431\n","batch 3 average EPE LOSS: 14.491976477672937\n"]},{"output_type":"stream","name":"stderr","text":["\r  1%|          | 4/336 [05:53<5:07:32, 55.58s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 4 | flow #1 | EPE LOSS: 18.12176586685005\n","batch 4 | flow #2 | EPE LOSS: 17.926760625373053\n","batch 4 | flow #3 | EPE LOSS: 17.266352892755815\n","batch 4 | flow #4 | EPE LOSS: 16.106711483353887\n","batch 4 | flow #5 | EPE LOSS: 14.854495309778306\n","batch 4 | flow #6 | EPE LOSS: 13.965126952274877\n","batch 4 | flow #7 | EPE LOSS: 13.3610890635397\n","batch 4 | flow #8 | EPE LOSS: 12.839371978338663\n","batch 4 | flow #9 | EPE LOSS: 12.538987868893068\n","batch 4 | flow #10 | EPE LOSS: 12.575433399519406\n","batch 4 | flow #11 | EPE LOSS: 12.529230079671052\n","batch 4 | flow #12 | EPE LOSS: 12.399996397378487\n","batch 4 average EPE LOSS: 13.351043370579559\n"]},{"output_type":"stream","name":"stderr","text":["\r  1%|▏         | 5/336 [06:09<3:48:16, 41.38s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 5 | flow #1 | EPE LOSS: 16.977657152223248\n","batch 5 | flow #2 | EPE LOSS: 16.27805675936979\n","batch 5 | flow #3 | EPE LOSS: 15.66456240014899\n","batch 5 | flow #4 | EPE LOSS: 15.382181280312082\n","batch 5 | flow #5 | EPE LOSS: 14.440655631184192\n","batch 5 | flow #6 | EPE LOSS: 13.818195721889651\n","batch 5 | flow #7 | EPE LOSS: 13.472031703723827\n","batch 5 | flow #8 | EPE LOSS: 13.022061212105507\n","batch 5 | flow #9 | EPE LOSS: 12.925613059066619\n","batch 5 | flow #10 | EPE LOSS: 12.714408230045464\n","batch 5 | flow #11 | EPE LOSS: 12.4032054774763\n","batch 5 | flow #12 | EPE LOSS: 12.062864067090246\n","batch 5 average EPE LOSS: 13.148727720074982\n"]},{"output_type":"stream","name":"stderr","text":["\r  2%|▏         | 6/336 [06:24<2:57:11, 32.22s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 6 | flow #1 | EPE LOSS: 16.47676090623903\n","batch 6 | flow #2 | EPE LOSS: 16.341286780608915\n","batch 6 | flow #3 | EPE LOSS: 15.870328307030649\n","batch 6 | flow #4 | EPE LOSS: 15.517908471544045\n","batch 6 | flow #5 | EPE LOSS: 15.142666845364829\n","batch 6 | flow #6 | EPE LOSS: 14.474561829154402\n","batch 6 | flow #7 | EPE LOSS: 13.806530344710184\n","batch 6 | flow #8 | EPE LOSS: 13.230430601479911\n","batch 6 | flow #9 | EPE LOSS: 12.777028646143217\n","batch 6 | flow #10 | EPE LOSS: 12.497818082195655\n","batch 6 | flow #11 | EPE LOSS: 12.420785769955307\n","batch 6 | flow #12 | EPE LOSS: 12.37375965980589\n","batch 6 average EPE LOSS: 12.766502186843498\n"]},{"output_type":"stream","name":"stderr","text":["\r  2%|▏         | 7/336 [06:39<2:26:03, 26.64s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 7 | flow #1 | EPE LOSS: 17.354480958467114\n","batch 7 | flow #2 | EPE LOSS: 16.3537000737302\n","batch 7 | flow #3 | EPE LOSS: 15.673290015238619\n","batch 7 | flow #4 | EPE LOSS: 15.204156269512737\n","batch 7 | flow #5 | EPE LOSS: 14.346827347653724\n","batch 7 | flow #6 | EPE LOSS: 13.162530423178941\n","batch 7 | flow #7 | EPE LOSS: 12.398162246453063\n","batch 7 | flow #8 | EPE LOSS: 11.788209703141275\n","batch 7 | flow #9 | EPE LOSS: 11.15474854058093\n","batch 7 | flow #10 | EPE LOSS: 10.830101729612743\n","batch 7 | flow #11 | EPE LOSS: 10.704940545073583\n","batch 7 | flow #12 | EPE LOSS: 10.788186706736388\n","batch 7 average EPE LOSS: 11.967847228225176\n"]},{"output_type":"stream","name":"stderr","text":["\r  2%|▏         | 8/336 [06:55<2:07:09, 23.26s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 8 | flow #1 | EPE LOSS: 19.798549941340315\n","batch 8 | flow #2 | EPE LOSS: 20.643182017333743\n","batch 8 | flow #3 | EPE LOSS: 19.905144864244445\n","batch 8 | flow #4 | EPE LOSS: 17.530565929095957\n","batch 8 | flow #5 | EPE LOSS: 15.483597765425571\n","batch 8 | flow #6 | EPE LOSS: 13.675225512322001\n","batch 8 | flow #7 | EPE LOSS: 12.66176007637875\n","batch 8 | flow #8 | EPE LOSS: 12.293918087404778\n","batch 8 | flow #9 | EPE LOSS: 12.382642766447333\n","batch 8 | flow #10 | EPE LOSS: 12.604099612183239\n","batch 8 | flow #11 | EPE LOSS: 12.840409726701276\n","batch 8 | flow #12 | EPE LOSS: 12.821380365833926\n","batch 8 average EPE LOSS: 13.997337670808877\n"]},{"output_type":"stream","name":"stderr","text":["\r  3%|▎         | 9/336 [07:11<1:54:16, 20.97s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 9 | flow #1 | EPE LOSS: 20.249531168449955\n","batch 9 | flow #2 | EPE LOSS: 20.413805990754074\n","batch 9 | flow #3 | EPE LOSS: 20.91864788503099\n","batch 9 | flow #4 | EPE LOSS: 21.619955252628653\n","batch 9 | flow #5 | EPE LOSS: 20.57282900041292\n","batch 9 | flow #6 | EPE LOSS: 19.714625304360876\n","batch 9 | flow #7 | EPE LOSS: 18.922039942272423\n","batch 9 | flow #8 | EPE LOSS: 18.392541551751194\n","batch 9 | flow #9 | EPE LOSS: 17.836194354649642\n","batch 9 | flow #10 | EPE LOSS: 17.521733119610474\n","batch 9 | flow #11 | EPE LOSS: 17.25877432577415\n","batch 9 | flow #12 | EPE LOSS: 17.165357721847215\n","batch 9 average EPE LOSS: 18.13996493073127\n"]},{"output_type":"stream","name":"stderr","text":["\r  3%|▎         | 10/336 [07:27<1:45:16, 19.38s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 10 | flow #1 | EPE LOSS: 23.128785304278875\n","batch 10 | flow #2 | EPE LOSS: 23.515255452024974\n","batch 10 | flow #3 | EPE LOSS: 22.781289228901088\n","batch 10 | flow #4 | EPE LOSS: 22.356761710853647\n","batch 10 | flow #5 | EPE LOSS: 22.27108440862212\n","batch 10 | flow #6 | EPE LOSS: 22.580159375464664\n","batch 10 | flow #7 | EPE LOSS: 22.49548728946507\n","batch 10 | flow #8 | EPE LOSS: 22.66606329531128\n","batch 10 | flow #9 | EPE LOSS: 22.50638350302805\n","batch 10 | flow #10 | EPE LOSS: 22.340989530252063\n","batch 10 | flow #11 | EPE LOSS: 22.02003978596777\n","batch 10 | flow #12 | EPE LOSS: 21.70608578312489\n","batch 10 average EPE LOSS: 20.125113329035916\n"]},{"output_type":"stream","name":"stderr","text":["\r  3%|▎         | 11/336 [07:43<1:39:11, 18.31s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 11 | flow #1 | EPE LOSS: 35.683846217093226\n","batch 11 | flow #2 | EPE LOSS: 42.85059951385707\n","batch 11 | flow #3 | EPE LOSS: 46.66381080205118\n","batch 11 | flow #4 | EPE LOSS: 51.74809666406899\n","batch 11 | flow #5 | EPE LOSS: 56.00431769774678\n","batch 11 | flow #6 | EPE LOSS: 59.78120058027705\n","batch 11 | flow #7 | EPE LOSS: 62.069971149648715\n","batch 11 | flow #8 | EPE LOSS: 63.35199764445591\n","batch 11 | flow #9 | EPE LOSS: 64.50119762666846\n","batch 11 | flow #10 | EPE LOSS: 65.54812229948897\n","batch 11 | flow #11 | EPE LOSS: 67.24388209571094\n","batch 11 | flow #12 | EPE LOSS: 68.7782744437892\n","batch 11 average EPE LOSS: 52.76043350447426\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▎         | 12/336 [07:58<1:34:01, 17.41s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 12 | flow #1 | EPE LOSS: 37.118117409662254\n","batch 12 | flow #2 | EPE LOSS: 53.28457589525758\n","batch 12 | flow #3 | EPE LOSS: 68.84538730169221\n","batch 12 | flow #4 | EPE LOSS: 82.3500984804208\n","batch 12 | flow #5 | EPE LOSS: 95.18012600320849\n","batch 12 | flow #6 | EPE LOSS: 104.66894738287995\n","batch 12 | flow #7 | EPE LOSS: 110.7521472117801\n","batch 12 | flow #8 | EPE LOSS: 116.7549052681439\n","batch 12 | flow #9 | EPE LOSS: 120.6951645327161\n","batch 12 | flow #10 | EPE LOSS: 123.53440095304448\n","batch 12 | flow #11 | EPE LOSS: 125.14818685331623\n","batch 12 | flow #12 | EPE LOSS: 126.20862592720029\n","batch 12 average EPE LOSS: 92.52017254276758\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 13/336 [08:13<1:30:02, 16.73s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 13 | flow #1 | EPE LOSS: 34.967036226853054\n","batch 13 | flow #2 | EPE LOSS: 46.718343295547726\n","batch 13 | flow #3 | EPE LOSS: 62.42167276935266\n","batch 13 | flow #4 | EPE LOSS: 76.93740077684515\n","batch 13 | flow #5 | EPE LOSS: 92.90545119034148\n","batch 13 | flow #6 | EPE LOSS: 107.43666260332743\n","batch 13 | flow #7 | EPE LOSS: 122.23779235079549\n","batch 13 | flow #8 | EPE LOSS: 136.0882056677656\n","batch 13 | flow #9 | EPE LOSS: 148.22952021636206\n","batch 13 | flow #10 | EPE LOSS: 157.0387230324708\n","batch 13 | flow #11 | EPE LOSS: 162.07200412654547\n","batch 13 | flow #12 | EPE LOSS: 165.0570471847437\n","batch 13 average EPE LOSS: 98.39835701833813\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 14/336 [08:28<1:27:14, 16.26s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 14 | flow #1 | EPE LOSS: 33.539081086434685\n","batch 14 | flow #2 | EPE LOSS: 40.205754357832966\n","batch 14 | flow #3 | EPE LOSS: 49.78139073486639\n","batch 14 | flow #4 | EPE LOSS: 62.07553878438981\n","batch 14 | flow #5 | EPE LOSS: 76.34562434155721\n","batch 14 | flow #6 | EPE LOSS: 88.9010109355931\n","batch 14 | flow #7 | EPE LOSS: 97.35034991407099\n","batch 14 | flow #8 | EPE LOSS: 105.35929668640671\n","batch 14 | flow #9 | EPE LOSS: 112.3960156770255\n","batch 14 | flow #10 | EPE LOSS: 120.52230204151117\n","batch 14 | flow #11 | EPE LOSS: 128.30238523192193\n","batch 14 | flow #12 | EPE LOSS: 135.0713226894228\n","batch 14 average EPE LOSS: 76.14218105223189\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 15/336 [08:43<1:24:57, 15.88s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 15 | flow #1 | EPE LOSS: 34.4441567152794\n","batch 15 | flow #2 | EPE LOSS: 44.64189561641612\n","batch 15 | flow #3 | EPE LOSS: 54.06858632724269\n","batch 15 | flow #4 | EPE LOSS: 63.34601961857823\n","batch 15 | flow #5 | EPE LOSS: 73.68823471669198\n","batch 15 | flow #6 | EPE LOSS: 83.19465303672432\n","batch 15 | flow #7 | EPE LOSS: 92.28742900829002\n","batch 15 | flow #8 | EPE LOSS: 101.6815675225542\n","batch 15 | flow #9 | EPE LOSS: 112.01976724117459\n","batch 15 | flow #10 | EPE LOSS: 118.62739322289256\n","batch 15 | flow #11 | EPE LOSS: 123.7391756292315\n","batch 15 | flow #12 | EPE LOSS: 127.1665087029116\n","batch 15 average EPE LOSS: 78.50084386381565\n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▍         | 16/336 [08:58<1:23:28, 15.65s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 16 | flow #1 | EPE LOSS: 30.581355955717992\n","batch 16 | flow #2 | EPE LOSS: 33.690340677073955\n","batch 16 | flow #3 | EPE LOSS: 35.490936991460636\n","batch 16 | flow #4 | EPE LOSS: 37.586058908430495\n","batch 16 | flow #5 | EPE LOSS: 39.233043797324875\n","batch 16 | flow #6 | EPE LOSS: 40.16209029256155\n","batch 16 | flow #7 | EPE LOSS: 40.91554784911959\n","batch 16 | flow #8 | EPE LOSS: 41.82756991391078\n","batch 16 | flow #9 | EPE LOSS: 42.46938841276047\n","batch 16 | flow #10 | EPE LOSS: 42.67721481345217\n","batch 16 | flow #11 | EPE LOSS: 42.77792649326077\n","batch 16 | flow #12 | EPE LOSS: 42.69820459958462\n","batch 16 average EPE LOSS: 37.08350188978365\n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▌         | 17/336 [09:14<1:22:36, 15.54s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 17 | flow #1 | EPE LOSS: 30.11202320102538\n","batch 17 | flow #2 | EPE LOSS: 34.049129265270075\n","batch 17 | flow #3 | EPE LOSS: 40.22780124321468\n","batch 17 | flow #4 | EPE LOSS: 46.698283498966084\n","batch 17 | flow #5 | EPE LOSS: 52.32284280465143\n","batch 17 | flow #6 | EPE LOSS: 56.306873443090744\n","batch 17 | flow #7 | EPE LOSS: 59.49492923360511\n","batch 17 | flow #8 | EPE LOSS: 62.87756258997416\n","batch 17 | flow #9 | EPE LOSS: 65.08931987075367\n","batch 17 | flow #10 | EPE LOSS: 65.50433431427574\n","batch 17 | flow #11 | EPE LOSS: 65.2522257190127\n","batch 17 | flow #12 | EPE LOSS: 65.34994189118304\n","batch 17 average EPE LOSS: 49.31348241472293\n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▌         | 18/336 [09:28<1:21:08, 15.31s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 18 | flow #1 | EPE LOSS: 31.777558396346894\n","batch 18 | flow #2 | EPE LOSS: 38.46827570602444\n","batch 18 | flow #3 | EPE LOSS: 45.58900518605489\n","batch 18 | flow #4 | EPE LOSS: 54.9858236828995\n","batch 18 | flow #5 | EPE LOSS: 67.49895165338616\n","batch 18 | flow #6 | EPE LOSS: 77.1836113770172\n","batch 18 | flow #7 | EPE LOSS: 85.36445735606196\n","batch 18 | flow #8 | EPE LOSS: 94.18032814083209\n","batch 18 | flow #9 | EPE LOSS: 101.61205442915677\n","batch 18 | flow #10 | EPE LOSS: 109.43809512724044\n","batch 18 | flow #11 | EPE LOSS: 117.45013892706652\n","batch 18 | flow #12 | EPE LOSS: 122.54838352349238\n","batch 18 average EPE LOSS: 73.02310009967906\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 19/336 [09:43<1:19:59, 15.14s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 19 | flow #1 | EPE LOSS: 33.891446152781086\n","batch 19 | flow #2 | EPE LOSS: 41.54402492203045\n","batch 19 | flow #3 | EPE LOSS: 51.12166413139397\n","batch 19 | flow #4 | EPE LOSS: 60.46128286364893\n","batch 19 | flow #5 | EPE LOSS: 71.66019875498382\n","batch 19 | flow #6 | EPE LOSS: 84.12663301041242\n","batch 19 | flow #7 | EPE LOSS: 94.34424063280596\n","batch 19 | flow #8 | EPE LOSS: 104.0971707786113\n","batch 19 | flow #9 | EPE LOSS: 113.18127326531105\n","batch 19 | flow #10 | EPE LOSS: 121.29758874995207\n","batch 19 | flow #11 | EPE LOSS: 127.75131944190927\n","batch 19 | flow #12 | EPE LOSS: 133.4087629903268\n","batch 19 average EPE LOSS: 76.3903786186826\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 20/336 [09:59<1:20:19, 15.25s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 20 | flow #1 | EPE LOSS: 38.17141601574056\n","batch 20 | flow #2 | EPE LOSS: 50.21924398484371\n","batch 20 | flow #3 | EPE LOSS: 63.463898584609495\n","batch 20 | flow #4 | EPE LOSS: 82.40391917111056\n","batch 20 | flow #5 | EPE LOSS: 99.21723122691162\n","batch 20 | flow #6 | EPE LOSS: 113.65779671895301\n","batch 20 | flow #7 | EPE LOSS: 127.05250550306103\n","batch 20 | flow #8 | EPE LOSS: 138.2936984296737\n","batch 20 | flow #9 | EPE LOSS: 146.00072571562356\n","batch 20 | flow #10 | EPE LOSS: 151.1372365083861\n","batch 20 | flow #11 | EPE LOSS: 154.72885392821624\n","batch 20 | flow #12 | EPE LOSS: 157.74922495024674\n","batch 20 average EPE LOSS: 103.61704385866385\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▋         | 21/336 [10:14<1:19:43, 15.19s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 21 | flow #1 | EPE LOSS: 44.818419304755224\n","batch 21 | flow #2 | EPE LOSS: 60.98012295769836\n","batch 21 | flow #3 | EPE LOSS: 74.62695791969261\n","batch 21 | flow #4 | EPE LOSS: 87.96277837005914\n","batch 21 | flow #5 | EPE LOSS: 102.66397468421043\n","batch 21 | flow #6 | EPE LOSS: 115.40100378105767\n","batch 21 | flow #7 | EPE LOSS: 124.77890081671679\n","batch 21 | flow #8 | EPE LOSS: 131.34680092074834\n","batch 21 | flow #9 | EPE LOSS: 135.80617622396338\n","batch 21 | flow #10 | EPE LOSS: 139.64806597889887\n","batch 21 | flow #11 | EPE LOSS: 143.39045159186216\n","batch 21 | flow #12 | EPE LOSS: 146.6270362331552\n","batch 21 average EPE LOSS: 102.79937296548616\n"]},{"output_type":"stream","name":"stderr","text":["\r  7%|▋         | 22/336 [10:29<1:20:02, 15.30s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 22 | flow #1 | EPE LOSS: 42.05490201875782\n","batch 22 | flow #2 | EPE LOSS: 56.577908469121915\n","batch 22 | flow #3 | EPE LOSS: 71.12478743177194\n","batch 22 | flow #4 | EPE LOSS: 87.00406325187477\n","batch 22 | flow #5 | EPE LOSS: 101.2346973129129\n","batch 22 | flow #6 | EPE LOSS: 110.95361216557751\n","batch 22 | flow #7 | EPE LOSS: 115.95673672715401\n","batch 22 | flow #8 | EPE LOSS: 119.0281602900814\n","batch 22 | flow #9 | EPE LOSS: 122.1739137531343\n","batch 22 | flow #10 | EPE LOSS: 124.63304235398705\n","batch 22 | flow #11 | EPE LOSS: 125.40175137802716\n","batch 22 | flow #12 | EPE LOSS: 125.2789308300449\n","batch 22 average EPE LOSS: 94.5562692543246\n"]},{"output_type":"stream","name":"stderr","text":["\r  7%|▋         | 23/336 [10:45<1:21:01, 15.53s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 23 | flow #1 | EPE LOSS: 42.43414641004946\n","batch 23 | flow #2 | EPE LOSS: 51.877118872173234\n","batch 23 | flow #3 | EPE LOSS: 61.52166440973762\n","batch 23 | flow #4 | EPE LOSS: 69.0682767328753\n","batch 23 | flow #5 | EPE LOSS: 74.88837296656553\n","batch 23 | flow #6 | EPE LOSS: 78.34184099801448\n","batch 23 | flow #7 | EPE LOSS: 80.0570990422122\n","batch 23 | flow #8 | EPE LOSS: 80.66020405511475\n","batch 23 | flow #9 | EPE LOSS: 82.12745625526046\n","batch 23 | flow #10 | EPE LOSS: 83.62583789785208\n","batch 23 | flow #11 | EPE LOSS: 84.04719415029588\n","batch 23 | flow #12 | EPE LOSS: 83.73611453363834\n","batch 23 average EPE LOSS: 69.99035044495918\n"]},{"output_type":"stream","name":"stderr","text":["\r  7%|▋         | 24/336 [11:01<1:20:23, 15.46s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 24 | flow #1 | EPE LOSS: 35.10809817120677\n","batch 24 | flow #2 | EPE LOSS: 40.28742961397427\n","batch 24 | flow #3 | EPE LOSS: 41.622842411959006\n","batch 24 | flow #4 | EPE LOSS: 42.281318899001036\n","batch 24 | flow #5 | EPE LOSS: 43.542145713559826\n","batch 24 | flow #6 | EPE LOSS: 45.30006085550483\n","batch 24 | flow #7 | EPE LOSS: 47.11025895241567\n","batch 24 | flow #8 | EPE LOSS: 48.53571770605735\n","batch 24 | flow #9 | EPE LOSS: 48.91191613667047\n","batch 24 | flow #10 | EPE LOSS: 48.853761689480415\n","batch 24 | flow #11 | EPE LOSS: 48.899100707113256\n","batch 24 | flow #12 | EPE LOSS: 48.5520649234326\n","batch 24 average EPE LOSS: 41.46808538071351\n"]},{"output_type":"stream","name":"stderr","text":["\r  7%|▋         | 25/336 [11:17<1:20:45, 15.58s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 25 | flow #1 | EPE LOSS: 27.10262196894503\n","batch 25 | flow #2 | EPE LOSS: 28.43472473588949\n","batch 25 | flow #3 | EPE LOSS: 26.907151534083795\n","batch 25 | flow #4 | EPE LOSS: 26.087727246618968\n","batch 25 | flow #5 | EPE LOSS: 25.628343420196277\n","batch 25 | flow #6 | EPE LOSS: 26.178457538579682\n","batch 25 | flow #7 | EPE LOSS: 27.60887322251075\n","batch 25 | flow #8 | EPE LOSS: 29.14588687076282\n","batch 25 | flow #9 | EPE LOSS: 30.856415766328187\n","batch 25 | flow #10 | EPE LOSS: 32.34918629190005\n","batch 25 | flow #11 | EPE LOSS: 33.45382021238121\n","batch 25 | flow #12 | EPE LOSS: 34.32852923386486\n","batch 25 average EPE LOSS: 26.566693510816105\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 26/336 [11:31<1:19:14, 15.34s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 26 | flow #1 | EPE LOSS: 29.61552833579235\n","batch 26 | flow #2 | EPE LOSS: 31.131869134702175\n","batch 26 | flow #3 | EPE LOSS: 32.12280029452504\n","batch 26 | flow #4 | EPE LOSS: 34.04214650121033\n","batch 26 | flow #5 | EPE LOSS: 35.96438098586658\n","batch 26 | flow #6 | EPE LOSS: 37.65705064451722\n","batch 26 | flow #7 | EPE LOSS: 39.44799306857794\n","batch 26 | flow #8 | EPE LOSS: 41.01714066309774\n","batch 26 | flow #9 | EPE LOSS: 42.51833816730854\n","batch 26 | flow #10 | EPE LOSS: 44.26824497230399\n","batch 26 | flow #11 | EPE LOSS: 45.770745170861154\n","batch 26 | flow #12 | EPE LOSS: 46.89991460164335\n","batch 26 average EPE LOSS: 36.07132516970518\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 27/336 [11:47<1:19:16, 15.39s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 27 | flow #1 | EPE LOSS: 30.88608377584984\n","batch 27 | flow #2 | EPE LOSS: 34.187533658990056\n","batch 27 | flow #3 | EPE LOSS: 36.46495027153416\n","batch 27 | flow #4 | EPE LOSS: 37.160501981924575\n","batch 27 | flow #5 | EPE LOSS: 37.93367887235129\n","batch 27 | flow #6 | EPE LOSS: 39.3505340827107\n","batch 27 | flow #7 | EPE LOSS: 40.55492702807748\n","batch 27 | flow #8 | EPE LOSS: 41.59143494486441\n","batch 27 | flow #9 | EPE LOSS: 42.68757658817316\n","batch 27 | flow #10 | EPE LOSS: 43.62619583822313\n","batch 27 | flow #11 | EPE LOSS: 43.905392754347076\n","batch 27 | flow #12 | EPE LOSS: 44.38141020880588\n","batch 27 average EPE LOSS: 36.13092090341111\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 28/336 [12:02<1:19:05, 15.41s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 28 | flow #1 | EPE LOSS: 33.08623211007672\n","batch 28 | flow #2 | EPE LOSS: 36.98577924896653\n","batch 28 | flow #3 | EPE LOSS: 36.27509652242221\n","batch 28 | flow #4 | EPE LOSS: 35.77796104689552\n","batch 28 | flow #5 | EPE LOSS: 35.33507533860028\n","batch 28 | flow #6 | EPE LOSS: 35.168694530481154\n","batch 28 | flow #7 | EPE LOSS: 35.02974604821093\n","batch 28 | flow #8 | EPE LOSS: 34.43317821379035\n","batch 28 | flow #9 | EPE LOSS: 33.77684201942065\n","batch 28 | flow #10 | EPE LOSS: 33.08314753708584\n","batch 28 | flow #11 | EPE LOSS: 32.870258742034615\n","batch 28 | flow #12 | EPE LOSS: 32.51199935634164\n","batch 28 average EPE LOSS: 29.31363108492803\n"]},{"output_type":"stream","name":"stderr","text":["\r  9%|▊         | 29/336 [12:17<1:18:28, 15.34s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 29 | flow #1 | EPE LOSS: 34.153405942619386\n","batch 29 | flow #2 | EPE LOSS: 37.77529111644209\n","batch 29 | flow #3 | EPE LOSS: 37.425833318968024\n","batch 29 | flow #4 | EPE LOSS: 37.16874192558124\n","batch 29 | flow #5 | EPE LOSS: 35.96908694261435\n","batch 29 | flow #6 | EPE LOSS: 34.20384410010238\n","batch 29 | flow #7 | EPE LOSS: 31.969626133378217\n","batch 29 | flow #8 | EPE LOSS: 30.731391239814553\n","batch 29 | flow #9 | EPE LOSS: 28.839830742646186\n","batch 29 | flow #10 | EPE LOSS: 26.358107042120537\n","batch 29 | flow #11 | EPE LOSS: 25.44658733875137\n","batch 29 | flow #12 | EPE LOSS: 25.260742347744085\n","batch 29 average EPE LOSS: 24.326788885602454\n"]},{"output_type":"stream","name":"stderr","text":["\r  9%|▉         | 30/336 [12:35<1:20:53, 15.86s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 30 | flow #1 | EPE LOSS: 30.98739240830318\n","batch 30 | flow #2 | EPE LOSS: 33.09059767480462\n","batch 30 | flow #3 | EPE LOSS: 33.23370836017933\n","batch 30 | flow #4 | EPE LOSS: 33.63979921880942\n","batch 30 | flow #5 | EPE LOSS: 33.96316039533718\n","batch 30 | flow #6 | EPE LOSS: 33.025843157162775\n","batch 30 | flow #7 | EPE LOSS: 32.77198347051762\n","batch 30 | flow #8 | EPE LOSS: 33.34165626868856\n","batch 30 | flow #9 | EPE LOSS: 33.18809752520162\n","batch 30 | flow #10 | EPE LOSS: 34.01231602040008\n","batch 30 | flow #11 | EPE LOSS: 33.53369057152942\n","batch 30 | flow #12 | EPE LOSS: 32.52900716533103\n","batch 30 average EPE LOSS: 23.60243604153638\n"]},{"output_type":"stream","name":"stderr","text":["\r  9%|▉         | 31/336 [12:50<1:20:34, 15.85s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 31 | flow #1 | EPE LOSS: 31.932537849288224\n","batch 31 | flow #2 | EPE LOSS: 35.5004338017549\n","batch 31 | flow #3 | EPE LOSS: 37.82031970894358\n","batch 31 | flow #4 | EPE LOSS: 41.45566390014606\n","batch 31 | flow #5 | EPE LOSS: 45.156873076894435\n","batch 31 | flow #6 | EPE LOSS: 46.944345268568746\n","batch 31 | flow #7 | EPE LOSS: 48.2340299717892\n","batch 31 | flow #8 | EPE LOSS: 50.878809297600064\n","batch 31 | flow #9 | EPE LOSS: 54.380447713187515\n","batch 31 | flow #10 | EPE LOSS: 57.7482029278189\n","batch 31 | flow #11 | EPE LOSS: 60.36182313049249\n","batch 31 | flow #12 | EPE LOSS: 61.81170553370001\n","batch 31 average EPE LOSS: 44.888782787854176\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|▉         | 32/336 [13:05<1:18:54, 15.57s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 32 | flow #1 | EPE LOSS: 32.422273095892656\n","batch 32 | flow #2 | EPE LOSS: 35.639014839885256\n","batch 32 | flow #3 | EPE LOSS: 36.99726458472296\n","batch 32 | flow #4 | EPE LOSS: 40.383563023593304\n","batch 32 | flow #5 | EPE LOSS: 43.90342467792126\n","batch 32 | flow #6 | EPE LOSS: 44.30874720499011\n","batch 32 | flow #7 | EPE LOSS: 43.42761697904289\n","batch 32 | flow #8 | EPE LOSS: 44.21521583087415\n","batch 32 | flow #9 | EPE LOSS: 45.433845861787965\n","batch 32 | flow #10 | EPE LOSS: 46.95714617256754\n","batch 32 | flow #11 | EPE LOSS: 47.846435028150175\n","batch 32 | flow #12 | EPE LOSS: 48.15722325978167\n","batch 32 average EPE LOSS: 36.853496386610004\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|▉         | 33/336 [13:21<1:18:41, 15.58s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 33 | flow #1 | EPE LOSS: 34.64214422285481\n","batch 33 | flow #2 | EPE LOSS: 39.88101940679896\n","batch 33 | flow #3 | EPE LOSS: 44.99965790399943\n","batch 33 | flow #4 | EPE LOSS: 50.35996367304519\n","batch 33 | flow #5 | EPE LOSS: 55.18486958939875\n","batch 33 | flow #6 | EPE LOSS: 58.02175803696446\n","batch 33 | flow #7 | EPE LOSS: 60.014589572419005\n","batch 33 | flow #8 | EPE LOSS: 63.17119250574367\n","batch 33 | flow #9 | EPE LOSS: 66.9356031419134\n","batch 33 | flow #10 | EPE LOSS: 69.93930704081157\n","batch 33 | flow #11 | EPE LOSS: 71.40239102804307\n","batch 33 | flow #12 | EPE LOSS: 71.74027686567457\n","batch 33 average EPE LOSS: 53.317806723321915\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 34/336 [13:37<1:18:31, 15.60s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 34 | flow #1 | EPE LOSS: 29.840121114347692\n","batch 34 | flow #2 | EPE LOSS: 30.29106559807894\n","batch 34 | flow #3 | EPE LOSS: 29.330445987052144\n","batch 34 | flow #4 | EPE LOSS: 27.731506095773323\n","batch 34 | flow #5 | EPE LOSS: 27.198672897216337\n","batch 34 | flow #6 | EPE LOSS: 27.415269302193682\n","batch 34 | flow #7 | EPE LOSS: 27.973920937053098\n","batch 34 | flow #8 | EPE LOSS: 29.64333090954364\n","batch 34 | flow #9 | EPE LOSS: 31.201592778048038\n","batch 34 | flow #10 | EPE LOSS: 32.98498690930487\n","batch 34 | flow #11 | EPE LOSS: 34.28033554758388\n","batch 34 | flow #12 | EPE LOSS: 35.15505178300314\n","batch 34 average EPE LOSS: 25.157551171562375\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 35/336 [13:52<1:18:12, 15.59s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 35 | flow #1 | EPE LOSS: 31.373538538263727\n","batch 35 | flow #2 | EPE LOSS: 31.79241725685\n","batch 35 | flow #3 | EPE LOSS: 30.60612169286634\n","batch 35 | flow #4 | EPE LOSS: 27.088430343100978\n","batch 35 | flow #5 | EPE LOSS: 24.077895320187665\n","batch 35 | flow #6 | EPE LOSS: 22.332153522718315\n","batch 35 | flow #7 | EPE LOSS: 20.83674176736206\n","batch 35 | flow #8 | EPE LOSS: 20.121614360785\n","batch 35 | flow #9 | EPE LOSS: 19.318942294530036\n","batch 35 | flow #10 | EPE LOSS: 18.930102072995954\n","batch 35 | flow #11 | EPE LOSS: 18.48795671998553\n","batch 35 | flow #12 | EPE LOSS: 18.10023456031295\n","batch 35 average EPE LOSS: 19.758317061836664\n"]},{"output_type":"stream","name":"stderr","text":["\r 11%|█         | 36/336 [14:09<1:19:16, 15.86s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 36 | flow #1 | EPE LOSS: 29.007195698368744\n","batch 36 | flow #2 | EPE LOSS: 32.13306816467197\n","batch 36 | flow #3 | EPE LOSS: 30.443904769033125\n","batch 36 | flow #4 | EPE LOSS: 29.020147428380266\n","batch 36 | flow #5 | EPE LOSS: 27.973783083619043\n","batch 36 | flow #6 | EPE LOSS: 28.10438842047182\n","batch 36 | flow #7 | EPE LOSS: 29.176708143365953\n","batch 36 | flow #8 | EPE LOSS: 30.112508255693985\n","batch 36 | flow #9 | EPE LOSS: 30.531174882019076\n","batch 36 | flow #10 | EPE LOSS: 30.804798227116425\n","batch 36 | flow #11 | EPE LOSS: 31.394986142130197\n","batch 36 | flow #12 | EPE LOSS: 32.07559101220567\n","batch 36 average EPE LOSS: 26.469677250428315\n"]},{"output_type":"stream","name":"stderr","text":["\r 11%|█         | 37/336 [14:25<1:19:21, 15.92s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 37 | flow #1 | EPE LOSS: 25.00386906775435\n","batch 37 | flow #2 | EPE LOSS: 24.627843475598954\n","batch 37 | flow #3 | EPE LOSS: 23.984777154242646\n","batch 37 | flow #4 | EPE LOSS: 22.232638551286303\n","batch 37 | flow #5 | EPE LOSS: 20.504971075121873\n","batch 37 | flow #6 | EPE LOSS: 19.24769134073593\n","batch 37 | flow #7 | EPE LOSS: 18.466513534977675\n","batch 37 | flow #8 | EPE LOSS: 17.805056343384532\n","batch 37 | flow #9 | EPE LOSS: 17.695406638042936\n","batch 37 | flow #10 | EPE LOSS: 18.13316002963071\n","batch 37 | flow #11 | EPE LOSS: 18.958649469930048\n","batch 37 | flow #12 | EPE LOSS: 19.462124683593082\n","batch 37 average EPE LOSS: 15.92181739314774\n"]},{"output_type":"stream","name":"stderr","text":["\r 11%|█▏        | 38/336 [14:40<1:18:34, 15.82s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 38 | flow #1 | EPE LOSS: 24.83616540912099\n","batch 38 | flow #2 | EPE LOSS: 24.465691620189975\n","batch 38 | flow #3 | EPE LOSS: 24.281341837847616\n","batch 38 | flow #4 | EPE LOSS: 23.113865999138838\n","batch 38 | flow #5 | EPE LOSS: 21.542247484346433\n","batch 38 | flow #6 | EPE LOSS: 20.117592879750124\n","batch 38 | flow #7 | EPE LOSS: 19.255531869592662\n","batch 38 | flow #8 | EPE LOSS: 19.438802225073402\n","batch 38 | flow #9 | EPE LOSS: 19.818768412901523\n","batch 38 | flow #10 | EPE LOSS: 20.11878337355878\n","batch 38 | flow #11 | EPE LOSS: 20.107570294348296\n","batch 38 | flow #12 | EPE LOSS: 19.864548047724337\n","batch 38 average EPE LOSS: 18.44698086083581\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 39/336 [14:56<1:18:21, 15.83s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 39 | flow #1 | EPE LOSS: 26.147019501786257\n","batch 39 | flow #2 | EPE LOSS: 25.235159439741636\n","batch 39 | flow #3 | EPE LOSS: 24.532573242538735\n","batch 39 | flow #4 | EPE LOSS: 24.697108051554192\n","batch 39 | flow #5 | EPE LOSS: 24.80523266624307\n","batch 39 | flow #6 | EPE LOSS: 25.072898817469845\n","batch 39 | flow #7 | EPE LOSS: 25.389684803758303\n","batch 39 | flow #8 | EPE LOSS: 25.942667833781893\n","batch 39 | flow #9 | EPE LOSS: 26.556056505139292\n","batch 39 | flow #10 | EPE LOSS: 26.92355071250792\n","batch 39 | flow #11 | EPE LOSS: 27.373593355940795\n","batch 39 | flow #12 | EPE LOSS: 27.698778037979267\n","batch 39 average EPE LOSS: 24.308970089641008\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 40/336 [15:12<1:17:44, 15.76s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 40 | flow #1 | EPE LOSS: 25.353594027649653\n","batch 40 | flow #2 | EPE LOSS: 26.07770921486637\n","batch 40 | flow #3 | EPE LOSS: 27.262812125160625\n","batch 40 | flow #4 | EPE LOSS: 28.826361319363105\n","batch 40 | flow #5 | EPE LOSS: 30.301806659102127\n","batch 40 | flow #6 | EPE LOSS: 31.80764161175371\n","batch 40 | flow #7 | EPE LOSS: 33.359150144693736\n","batch 40 | flow #8 | EPE LOSS: 34.62607927446848\n","batch 40 | flow #9 | EPE LOSS: 35.67076697282166\n","batch 40 | flow #10 | EPE LOSS: 36.88108993507901\n","batch 40 | flow #11 | EPE LOSS: 38.28950481112454\n","batch 40 | flow #12 | EPE LOSS: 39.372316412795584\n","batch 40 average EPE LOSS: 31.00234480381022\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 41/336 [15:27<1:16:53, 15.64s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 41 | flow #1 | EPE LOSS: 28.551961274766043\n","batch 41 | flow #2 | EPE LOSS: 28.33395181853015\n","batch 41 | flow #3 | EPE LOSS: 28.50229181918805\n","batch 41 | flow #4 | EPE LOSS: 29.506912340362334\n","batch 41 | flow #5 | EPE LOSS: 30.3053647153066\n","batch 41 | flow #6 | EPE LOSS: 30.23710352232288\n","batch 41 | flow #7 | EPE LOSS: 30.16281344941802\n","batch 41 | flow #8 | EPE LOSS: 30.857415342368185\n","batch 41 | flow #9 | EPE LOSS: 31.704580826704817\n","batch 41 | flow #10 | EPE LOSS: 32.69926880062901\n","batch 41 | flow #11 | EPE LOSS: 33.47031136843427\n","batch 41 | flow #12 | EPE LOSS: 34.1646620684056\n","batch 41 average EPE LOSS: 27.71206142448456\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▎        | 42/336 [15:44<1:18:40, 16.06s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 42 | flow #1 | EPE LOSS: 27.468983125035724\n","batch 42 | flow #2 | EPE LOSS: 25.14099860296777\n","batch 42 | flow #3 | EPE LOSS: 24.24249729560273\n","batch 42 | flow #4 | EPE LOSS: 23.97238564300441\n","batch 42 | flow #5 | EPE LOSS: 23.34989083729997\n","batch 42 | flow #6 | EPE LOSS: 22.805645492721528\n","batch 42 | flow #7 | EPE LOSS: 22.67197613601272\n","batch 42 | flow #8 | EPE LOSS: 22.587085834488963\n","batch 42 | flow #9 | EPE LOSS: 22.50941323436582\n","batch 42 | flow #10 | EPE LOSS: 22.65111976809179\n","batch 42 | flow #11 | EPE LOSS: 22.729578614518257\n","batch 42 | flow #12 | EPE LOSS: 22.721225002801386\n","batch 42 average EPE LOSS: 20.43212136408061\n"]},{"output_type":"stream","name":"stderr","text":["\r 13%|█▎        | 43/336 [16:00<1:18:17, 16.03s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 43 | flow #1 | EPE LOSS: 26.930416436695413\n","batch 43 | flow #2 | EPE LOSS: 26.263792874766747\n","batch 43 | flow #3 | EPE LOSS: 24.61466900671257\n","batch 43 | flow #4 | EPE LOSS: 22.791759730833306\n","batch 43 | flow #5 | EPE LOSS: 21.091469749365505\n","batch 43 | flow #6 | EPE LOSS: 19.465514132357033\n","batch 43 | flow #7 | EPE LOSS: 19.043551226535552\n","batch 43 | flow #8 | EPE LOSS: 18.849215673707175\n","batch 43 | flow #9 | EPE LOSS: 18.690315889811924\n","batch 43 | flow #10 | EPE LOSS: 18.92452735612253\n","batch 43 | flow #11 | EPE LOSS: 19.076110333906943\n","batch 43 | flow #12 | EPE LOSS: 19.275587732441736\n","batch 43 average EPE LOSS: 18.35330951205272\n"]},{"output_type":"stream","name":"stderr","text":["\r 13%|█▎        | 44/336 [16:16<1:17:26, 15.91s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 44 | flow #1 | EPE LOSS: 23.680802037010263\n","batch 44 | flow #2 | EPE LOSS: 23.199855506913867\n","batch 44 | flow #3 | EPE LOSS: 22.210800970638513\n","batch 44 | flow #4 | EPE LOSS: 20.347722484277174\n","batch 44 | flow #5 | EPE LOSS: 19.416766885987258\n","batch 44 | flow #6 | EPE LOSS: 18.963720945764596\n","batch 44 | flow #7 | EPE LOSS: 18.786596393882515\n","batch 44 | flow #8 | EPE LOSS: 18.700647615431304\n","batch 44 | flow #9 | EPE LOSS: 18.958333374243125\n","batch 44 | flow #10 | EPE LOSS: 19.19790396950428\n","batch 44 | flow #11 | EPE LOSS: 19.619471050444798\n","batch 44 | flow #12 | EPE LOSS: 19.94976803298591\n","batch 44 average EPE LOSS: 18.11432633905067\n"]},{"output_type":"stream","name":"stderr","text":["\r 13%|█▎        | 45/336 [16:32<1:17:32, 15.99s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 45 | flow #1 | EPE LOSS: 25.462234938094465\n","batch 45 | flow #2 | EPE LOSS: 25.658180021426322\n","batch 45 | flow #3 | EPE LOSS: 23.777226215678525\n","batch 45 | flow #4 | EPE LOSS: 22.25648923705803\n","batch 45 | flow #5 | EPE LOSS: 22.482169694486245\n","batch 45 | flow #6 | EPE LOSS: 22.612836260845125\n","batch 45 | flow #7 | EPE LOSS: 22.296740523645763\n","batch 45 | flow #8 | EPE LOSS: 22.108600842747336\n","batch 45 | flow #9 | EPE LOSS: 22.001579538647785\n","batch 45 | flow #10 | EPE LOSS: 22.065795098200827\n","batch 45 | flow #11 | EPE LOSS: 22.38720429933422\n","batch 45 | flow #12 | EPE LOSS: 22.902023785744632\n","batch 45 average EPE LOSS: 20.742427788249422\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▎        | 46/336 [16:47<1:16:43, 15.87s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 46 | flow #1 | EPE LOSS: 25.221648859783212\n","batch 46 | flow #2 | EPE LOSS: 25.763023042396195\n","batch 46 | flow #3 | EPE LOSS: 25.324623838936727\n","batch 46 | flow #4 | EPE LOSS: 25.25990402360127\n","batch 46 | flow #5 | EPE LOSS: 25.473885882368915\n","batch 46 | flow #6 | EPE LOSS: 25.97441158282338\n","batch 46 | flow #7 | EPE LOSS: 26.787716803206422\n","batch 46 | flow #8 | EPE LOSS: 27.5778623937715\n","batch 46 | flow #9 | EPE LOSS: 28.340333493649407\n","batch 46 | flow #10 | EPE LOSS: 29.52795328484228\n","batch 46 | flow #11 | EPE LOSS: 30.540368910952107\n","batch 46 | flow #12 | EPE LOSS: 31.30321845373693\n","batch 46 average EPE LOSS: 25.149380960600098\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 47/336 [17:03<1:16:37, 15.91s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 47 | flow #1 | EPE LOSS: 25.45713174897453\n","batch 47 | flow #2 | EPE LOSS: 25.74189335906017\n","batch 47 | flow #3 | EPE LOSS: 24.891974769375494\n","batch 47 | flow #4 | EPE LOSS: 25.247760948025014\n","batch 47 | flow #5 | EPE LOSS: 25.729872966823365\n","batch 47 | flow #6 | EPE LOSS: 26.80110095635277\n","batch 47 | flow #7 | EPE LOSS: 28.447108283032566\n","batch 47 | flow #8 | EPE LOSS: 30.335834685790644\n","batch 47 | flow #9 | EPE LOSS: 31.73809505163352\n","batch 47 | flow #10 | EPE LOSS: 33.09483718074235\n","batch 47 | flow #11 | EPE LOSS: 34.32684711337394\n","batch 47 | flow #12 | EPE LOSS: 35.298613529206214\n","batch 47 average EPE LOSS: 27.011694698440408\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 48/336 [17:20<1:16:53, 16.02s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 48 | flow #1 | EPE LOSS: 26.23146043563919\n","batch 48 | flow #2 | EPE LOSS: 26.314316965719954\n","batch 48 | flow #3 | EPE LOSS: 25.50090717499067\n","batch 48 | flow #4 | EPE LOSS: 25.15134020875574\n","batch 48 | flow #5 | EPE LOSS: 25.04192242220486\n","batch 48 | flow #6 | EPE LOSS: 25.172292504134486\n","batch 48 | flow #7 | EPE LOSS: 25.44623993336884\n","batch 48 | flow #8 | EPE LOSS: 25.832131552007013\n","batch 48 | flow #9 | EPE LOSS: 26.108774146796666\n","batch 48 | flow #10 | EPE LOSS: 26.385400465767802\n","batch 48 | flow #11 | EPE LOSS: 26.546360473966217\n","batch 48 | flow #12 | EPE LOSS: 26.84786376061308\n","batch 48 average EPE LOSS: 22.188923583060568\n"]},{"output_type":"stream","name":"stderr","text":["\r 15%|█▍        | 49/336 [17:34<1:14:49, 15.64s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 49 | flow #1 | EPE LOSS: 27.430940810831263\n","batch 49 | flow #2 | EPE LOSS: 29.561962823773086\n","batch 49 | flow #3 | EPE LOSS: 30.97339511899197\n","batch 49 | flow #4 | EPE LOSS: 32.13762283803369\n","batch 49 | flow #5 | EPE LOSS: 32.54675508609198\n","batch 49 | flow #6 | EPE LOSS: 31.954393560290132\n","batch 49 | flow #7 | EPE LOSS: 31.805969727668625\n","batch 49 | flow #8 | EPE LOSS: 32.43452655760427\n","batch 49 | flow #9 | EPE LOSS: 33.30357067756703\n","batch 49 | flow #10 | EPE LOSS: 33.87403658570081\n","batch 49 | flow #11 | EPE LOSS: 34.143346954760084\n","batch 49 | flow #12 | EPE LOSS: 33.77515192673761\n","batch 49 average EPE LOSS: 28.30475574404383\n"]},{"output_type":"stream","name":"stderr","text":["\r 15%|█▍        | 50/336 [17:51<1:16:29, 16.05s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 50 | flow #1 | EPE LOSS: 26.505127783902015\n","batch 50 | flow #2 | EPE LOSS: 28.100007139938157\n","batch 50 | flow #3 | EPE LOSS: 26.58763835998906\n","batch 50 | flow #4 | EPE LOSS: 24.330848615505175\n","batch 50 | flow #5 | EPE LOSS: 22.7813767489685\n","batch 50 | flow #6 | EPE LOSS: 21.455190394947504\n","batch 50 | flow #7 | EPE LOSS: 21.5682703173626\n","batch 50 | flow #8 | EPE LOSS: 22.286037374653475\n","batch 50 | flow #9 | EPE LOSS: 22.587182690740548\n","batch 50 | flow #10 | EPE LOSS: 22.88508193370248\n","batch 50 | flow #11 | EPE LOSS: 23.145003960287436\n","batch 50 | flow #12 | EPE LOSS: 23.65758795866437\n","batch 50 average EPE LOSS: 19.267828415229932\n"]},{"output_type":"stream","name":"stderr","text":["\r 15%|█▌        | 51/336 [18:07<1:15:31, 15.90s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 51 | flow #1 | EPE LOSS: 28.134101424086367\n","batch 51 | flow #2 | EPE LOSS: 26.983812943417963\n","batch 51 | flow #3 | EPE LOSS: 26.669133767654973\n","batch 51 | flow #4 | EPE LOSS: 26.805033781516354\n","batch 51 | flow #5 | EPE LOSS: 27.74114521261389\n","batch 51 | flow #6 | EPE LOSS: 29.89876541625225\n","batch 51 | flow #7 | EPE LOSS: 32.9547245239845\n","batch 51 | flow #8 | EPE LOSS: 35.69588951589523\n","batch 51 | flow #9 | EPE LOSS: 37.195254926992014\n","batch 51 | flow #10 | EPE LOSS: 38.73542008879143\n","batch 51 | flow #11 | EPE LOSS: 40.32760450946626\n","batch 51 | flow #12 | EPE LOSS: 41.9951605452252\n","batch 51 average EPE LOSS: 28.27666037758333\n"]},{"output_type":"stream","name":"stderr","text":["\r 15%|█▌        | 52/336 [18:24<1:16:27, 16.15s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 52 | flow #1 | EPE LOSS: 31.586181513720295\n","batch 52 | flow #2 | EPE LOSS: 27.69548304900385\n","batch 52 | flow #3 | EPE LOSS: 27.032251829212665\n","batch 52 | flow #4 | EPE LOSS: 27.25604599400246\n","batch 52 | flow #5 | EPE LOSS: 26.67674388172502\n","batch 52 | flow #6 | EPE LOSS: 25.97951263745161\n","batch 52 | flow #7 | EPE LOSS: 26.67884714236163\n","batch 52 | flow #8 | EPE LOSS: 27.716866554250633\n","batch 52 | flow #9 | EPE LOSS: 29.060638420983604\n","batch 52 | flow #10 | EPE LOSS: 30.10155293810365\n","batch 52 | flow #11 | EPE LOSS: 30.325180311522406\n","batch 52 | flow #12 | EPE LOSS: 30.563296009466637\n","batch 52 average EPE LOSS: 23.109716455347215\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 53/336 [18:40<1:15:42, 16.05s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 53 | flow #1 | EPE LOSS: 29.45656576263849\n","batch 53 | flow #2 | EPE LOSS: 31.76174268162676\n","batch 53 | flow #3 | EPE LOSS: 33.69498554543292\n","batch 53 | flow #4 | EPE LOSS: 35.29630087834446\n","batch 53 | flow #5 | EPE LOSS: 36.17854813171653\n","batch 53 | flow #6 | EPE LOSS: 37.02476269033921\n","batch 53 | flow #7 | EPE LOSS: 38.76067967809194\n","batch 53 | flow #8 | EPE LOSS: 40.69553425755603\n","batch 53 | flow #9 | EPE LOSS: 42.54403514562361\n","batch 53 | flow #10 | EPE LOSS: 43.65903366917926\n","batch 53 | flow #11 | EPE LOSS: 43.884451252320936\n","batch 53 | flow #12 | EPE LOSS: 44.088526966760725\n","batch 53 average EPE LOSS: 34.24211948082006\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 54/336 [18:55<1:14:25, 15.83s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 54 | flow #1 | EPE LOSS: 30.355316129679533\n","batch 54 | flow #2 | EPE LOSS: 34.54041659833957\n","batch 54 | flow #3 | EPE LOSS: 39.00731603138456\n","batch 54 | flow #4 | EPE LOSS: 40.588449617462395\n","batch 54 | flow #5 | EPE LOSS: 40.41392393739918\n","batch 54 | flow #6 | EPE LOSS: 39.91232391027555\n","batch 54 | flow #7 | EPE LOSS: 40.17655192545967\n","batch 54 | flow #8 | EPE LOSS: 41.28839831856107\n","batch 54 | flow #9 | EPE LOSS: 42.99155732114735\n","batch 54 | flow #10 | EPE LOSS: 44.80075556287073\n","batch 54 | flow #11 | EPE LOSS: 46.145249732740204\n","batch 54 | flow #12 | EPE LOSS: 46.77123993341464\n","batch 54 average EPE LOSS: 38.009478747928505\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▋        | 55/336 [19:12<1:15:33, 16.13s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 55 | flow #1 | EPE LOSS: 30.943296210881616\n","batch 55 | flow #2 | EPE LOSS: 35.05851269309901\n","batch 55 | flow #3 | EPE LOSS: 37.77596813777536\n","batch 55 | flow #4 | EPE LOSS: 40.09239518736277\n","batch 55 | flow #5 | EPE LOSS: 41.46030593707055\n","batch 55 | flow #6 | EPE LOSS: 42.63400755285222\n","batch 55 | flow #7 | EPE LOSS: 43.759384196316944\n","batch 55 | flow #8 | EPE LOSS: 44.80577452632597\n","batch 55 | flow #9 | EPE LOSS: 45.532801852389866\n","batch 55 | flow #10 | EPE LOSS: 46.14144564692908\n","batch 55 | flow #11 | EPE LOSS: 46.712610754020574\n","batch 55 | flow #12 | EPE LOSS: 47.301977811332215\n","batch 55 average EPE LOSS: 39.34512490704666\n"]},{"output_type":"stream","name":"stderr","text":["\r 17%|█▋        | 56/336 [19:28<1:15:16, 16.13s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 56 | flow #1 | EPE LOSS: 31.636279464457036\n","batch 56 | flow #2 | EPE LOSS: 34.37984162037207\n","batch 56 | flow #3 | EPE LOSS: 34.623009966415296\n","batch 56 | flow #4 | EPE LOSS: 32.38335300249503\n","batch 56 | flow #5 | EPE LOSS: 30.01749616210356\n","batch 56 | flow #6 | EPE LOSS: 28.24628440383861\n","batch 56 | flow #7 | EPE LOSS: 26.848304214252178\n","batch 56 | flow #8 | EPE LOSS: 26.008386726907002\n","batch 56 | flow #9 | EPE LOSS: 25.12518758349619\n","batch 56 | flow #10 | EPE LOSS: 24.82776106150128\n","batch 56 | flow #11 | EPE LOSS: 24.568429082559476\n","batch 56 | flow #12 | EPE LOSS: 24.505810597916696\n","batch 56 average EPE LOSS: 25.4162584244072\n"]},{"output_type":"stream","name":"stderr","text":["\r 17%|█▋        | 57/336 [19:44<1:15:17, 16.19s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 57 | flow #1 | EPE LOSS: 27.564199974286662\n","batch 57 | flow #2 | EPE LOSS: 27.688912842186188\n","batch 57 | flow #3 | EPE LOSS: 26.11295556490064\n","batch 57 | flow #4 | EPE LOSS: 24.96188345779416\n","batch 57 | flow #5 | EPE LOSS: 23.33782658446219\n","batch 57 | flow #6 | EPE LOSS: 22.62364646530301\n","batch 57 | flow #7 | EPE LOSS: 22.161526916525833\n","batch 57 | flow #8 | EPE LOSS: 21.31753570556061\n","batch 57 | flow #9 | EPE LOSS: 20.81001372067299\n","batch 57 | flow #10 | EPE LOSS: 20.599654543432624\n","batch 57 | flow #11 | EPE LOSS: 20.223713747704338\n","batch 57 | flow #12 | EPE LOSS: 19.565108480299738\n","batch 57 average EPE LOSS: 19.925271601822725\n"]},{"output_type":"stream","name":"stderr","text":["\r 17%|█▋        | 58/336 [20:01<1:15:30, 16.30s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 58 | flow #1 | EPE LOSS: 25.703922465352274\n","batch 58 | flow #2 | EPE LOSS: 26.532961786107133\n","batch 58 | flow #3 | EPE LOSS: 25.02131840187541\n","batch 58 | flow #4 | EPE LOSS: 22.245174175308524\n","batch 58 | flow #5 | EPE LOSS: 21.237128709413664\n","batch 58 | flow #6 | EPE LOSS: 21.190075159878628\n","batch 58 | flow #7 | EPE LOSS: 21.569100064397798\n","batch 58 | flow #8 | EPE LOSS: 21.98586589811714\n","batch 58 | flow #9 | EPE LOSS: 22.528437665516126\n","batch 58 | flow #10 | EPE LOSS: 22.987314138342782\n","batch 58 | flow #11 | EPE LOSS: 23.135011148294442\n","batch 58 | flow #12 | EPE LOSS: 23.307854656033115\n","batch 58 average EPE LOSS: 20.512666903375393\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 59/336 [20:17<1:14:36, 16.16s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 59 | flow #1 | EPE LOSS: 24.589634450505244\n","batch 59 | flow #2 | EPE LOSS: 24.338512956879924\n","batch 59 | flow #3 | EPE LOSS: 23.35492437183004\n","batch 59 | flow #4 | EPE LOSS: 21.712076917844332\n","batch 59 | flow #5 | EPE LOSS: 21.461403260911723\n","batch 59 | flow #6 | EPE LOSS: 21.528088758497542\n","batch 59 | flow #7 | EPE LOSS: 22.088246002195987\n","batch 59 | flow #8 | EPE LOSS: 22.845537634579387\n","batch 59 | flow #9 | EPE LOSS: 23.719146245527963\n","batch 59 | flow #10 | EPE LOSS: 24.31598993492042\n","batch 59 | flow #11 | EPE LOSS: 24.873122223481115\n","batch 59 | flow #12 | EPE LOSS: 25.56423188702246\n","batch 59 average EPE LOSS: 20.508187496215196\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 60/336 [20:33<1:15:06, 16.33s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 60 | flow #1 | EPE LOSS: 23.438054594549815\n","batch 60 | flow #2 | EPE LOSS: 23.210748554573083\n","batch 60 | flow #3 | EPE LOSS: 22.13468897188841\n","batch 60 | flow #4 | EPE LOSS: 21.72535979285294\n","batch 60 | flow #5 | EPE LOSS: 21.44128299568116\n","batch 60 | flow #6 | EPE LOSS: 21.552535706145886\n","batch 60 | flow #7 | EPE LOSS: 22.140860893798234\n","batch 60 | flow #8 | EPE LOSS: 22.898292216959717\n","batch 60 | flow #9 | EPE LOSS: 23.90037361979581\n","batch 60 | flow #10 | EPE LOSS: 24.480598284460882\n","batch 60 | flow #11 | EPE LOSS: 24.99149829890041\n","batch 60 | flow #12 | EPE LOSS: 25.52024159985686\n","batch 60 average EPE LOSS: 20.843273690043276\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 61/336 [20:49<1:13:59, 16.14s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 61 | flow #1 | EPE LOSS: 22.62105114907003\n","batch 61 | flow #2 | EPE LOSS: 21.74019306477359\n","batch 61 | flow #3 | EPE LOSS: 20.726969378832287\n","batch 61 | flow #4 | EPE LOSS: 20.6441807823831\n","batch 61 | flow #5 | EPE LOSS: 21.335118901118577\n","batch 61 | flow #6 | EPE LOSS: 21.954824344748527\n","batch 61 | flow #7 | EPE LOSS: 22.553589999667352\n","batch 61 | flow #8 | EPE LOSS: 23.194206728959575\n","batch 61 | flow #9 | EPE LOSS: 23.719928279883792\n","batch 61 | flow #10 | EPE LOSS: 24.221624556180654\n","batch 61 | flow #11 | EPE LOSS: 24.475802940949578\n","batch 61 | flow #12 | EPE LOSS: 24.78979037765733\n","batch 61 average EPE LOSS: 20.225947762507584\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 62/336 [21:04<1:12:28, 15.87s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 62 | flow #1 | EPE LOSS: 23.422558831079133\n","batch 62 | flow #2 | EPE LOSS: 24.191078212748348\n","batch 62 | flow #3 | EPE LOSS: 24.4134710590405\n","batch 62 | flow #4 | EPE LOSS: 25.219477046532432\n","batch 62 | flow #5 | EPE LOSS: 25.85116554440442\n","batch 62 | flow #6 | EPE LOSS: 25.972060288231607\n","batch 62 | flow #7 | EPE LOSS: 26.624922834456886\n","batch 62 | flow #8 | EPE LOSS: 27.609633207025468\n","batch 62 | flow #9 | EPE LOSS: 28.42234623328064\n","batch 62 | flow #10 | EPE LOSS: 29.17528726535717\n","batch 62 | flow #11 | EPE LOSS: 29.7777226588768\n","batch 62 | flow #12 | EPE LOSS: 30.418319264877244\n","batch 62 average EPE LOSS: 25.084211986033036\n"]},{"output_type":"stream","name":"stderr","text":["\r 19%|█▉        | 63/336 [21:20<1:11:59, 15.82s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 63 | flow #1 | EPE LOSS: 22.19980663975342\n","batch 63 | flow #2 | EPE LOSS: 21.120156648286482\n","batch 63 | flow #3 | EPE LOSS: 20.866110613823395\n","batch 63 | flow #4 | EPE LOSS: 20.747771861494957\n","batch 63 | flow #5 | EPE LOSS: 21.002684051536562\n","batch 63 | flow #6 | EPE LOSS: 21.391904720118827\n","batch 63 | flow #7 | EPE LOSS: 22.100742995025485\n","batch 63 | flow #8 | EPE LOSS: 22.8588236912494\n","batch 63 | flow #9 | EPE LOSS: 23.819791479121875\n","batch 63 | flow #10 | EPE LOSS: 24.729445197323116\n","batch 63 | flow #11 | EPE LOSS: 25.704706004979204\n","batch 63 | flow #12 | EPE LOSS: 26.558446751299595\n","batch 63 average EPE LOSS: 20.860880404714905\n"]},{"output_type":"stream","name":"stderr","text":["\r 19%|█▉        | 64/336 [21:35<1:11:04, 15.68s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 64 | flow #1 | EPE LOSS: 23.36046431046289\n","batch 64 | flow #2 | EPE LOSS: 24.487783528666004\n","batch 64 | flow #3 | EPE LOSS: 25.502346842492713\n","batch 64 | flow #4 | EPE LOSS: 25.48468641657683\n","batch 64 | flow #5 | EPE LOSS: 25.78106461424844\n","batch 64 | flow #6 | EPE LOSS: 25.9983258137617\n","batch 64 | flow #7 | EPE LOSS: 26.593918077261165\n","batch 64 | flow #8 | EPE LOSS: 27.655539133178838\n","batch 64 | flow #9 | EPE LOSS: 28.537740873406687\n","batch 64 | flow #10 | EPE LOSS: 29.53965836689594\n","batch 64 | flow #11 | EPE LOSS: 30.4813660322483\n","batch 64 | flow #12 | EPE LOSS: 31.20721669317929\n","batch 64 average EPE LOSS: 23.95376144984449\n"]},{"output_type":"stream","name":"stderr","text":["\r 19%|█▉        | 65/336 [21:50<1:09:40, 15.43s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 65 | flow #1 | EPE LOSS: 25.80974816350521\n","batch 65 | flow #2 | EPE LOSS: 27.86266899160894\n","batch 65 | flow #3 | EPE LOSS: 27.329782266292373\n","batch 65 | flow #4 | EPE LOSS: 24.914157778341075\n","batch 65 | flow #5 | EPE LOSS: 23.941980188481807\n","batch 65 | flow #6 | EPE LOSS: 23.481862124060644\n","batch 65 | flow #7 | EPE LOSS: 22.539351425899515\n","batch 65 | flow #8 | EPE LOSS: 21.59363256616854\n","batch 65 | flow #9 | EPE LOSS: 21.19915851576826\n","batch 65 | flow #10 | EPE LOSS: 20.923999999455457\n","batch 65 | flow #11 | EPE LOSS: 20.81879949367449\n","batch 65 | flow #12 | EPE LOSS: 20.756144120769928\n","batch 65 average EPE LOSS: 20.169532761395402\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|█▉        | 66/336 [22:06<1:09:31, 15.45s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 66 | flow #1 | EPE LOSS: 22.342642564419943\n","batch 66 | flow #2 | EPE LOSS: 22.844638997245614\n","batch 66 | flow #3 | EPE LOSS: 22.20154623809954\n","batch 66 | flow #4 | EPE LOSS: 21.275217263327235\n","batch 66 | flow #5 | EPE LOSS: 21.411401638340738\n","batch 66 | flow #6 | EPE LOSS: 21.23247851065327\n","batch 66 | flow #7 | EPE LOSS: 21.122269417090266\n","batch 66 | flow #8 | EPE LOSS: 20.827913411853217\n","batch 66 | flow #9 | EPE LOSS: 20.603159352478656\n","batch 66 | flow #10 | EPE LOSS: 20.335484055553653\n","batch 66 | flow #11 | EPE LOSS: 20.04175844529422\n","batch 66 | flow #12 | EPE LOSS: 19.59902231500551\n","batch 66 average EPE LOSS: 18.08058061849078\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|█▉        | 67/336 [22:20<1:08:19, 15.24s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 67 | flow #1 | EPE LOSS: 24.72145774196257\n","batch 67 | flow #2 | EPE LOSS: 26.202057421699028\n","batch 67 | flow #3 | EPE LOSS: 26.109018592185794\n","batch 67 | flow #4 | EPE LOSS: 24.90095776283972\n","batch 67 | flow #5 | EPE LOSS: 24.423273203409178\n","batch 67 | flow #6 | EPE LOSS: 24.280344324340188\n","batch 67 | flow #7 | EPE LOSS: 24.151413349308104\n","batch 67 | flow #8 | EPE LOSS: 23.595347064290653\n","batch 67 | flow #9 | EPE LOSS: 23.36110215472014\n","batch 67 | flow #10 | EPE LOSS: 23.156770409297295\n","batch 67 | flow #11 | EPE LOSS: 22.891965591941673\n","batch 67 | flow #12 | EPE LOSS: 22.955904436791943\n","batch 67 average EPE LOSS: 20.66821167222231\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 68/336 [22:36<1:08:05, 15.25s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 68 | flow #1 | EPE LOSS: 24.658030118651137\n","batch 68 | flow #2 | EPE LOSS: 26.945750958277937\n","batch 68 | flow #3 | EPE LOSS: 27.172529788154602\n","batch 68 | flow #4 | EPE LOSS: 26.575136853414296\n","batch 68 | flow #5 | EPE LOSS: 26.186324307182886\n","batch 68 | flow #6 | EPE LOSS: 26.04069999956899\n","batch 68 | flow #7 | EPE LOSS: 26.196462782792594\n","batch 68 | flow #8 | EPE LOSS: 26.545801903259\n","batch 68 | flow #9 | EPE LOSS: 27.271942725652746\n","batch 68 | flow #10 | EPE LOSS: 28.08613196244025\n","batch 68 | flow #11 | EPE LOSS: 29.001896339830154\n","batch 68 | flow #12 | EPE LOSS: 29.665966816286716\n","batch 68 average EPE LOSS: 24.211081713589987\n"]},{"output_type":"stream","name":"stderr","text":["\r 21%|██        | 69/336 [22:52<1:08:58, 15.50s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 69 | flow #1 | EPE LOSS: 23.847771898369896\n","batch 69 | flow #2 | EPE LOSS: 24.913084729681234\n","batch 69 | flow #3 | EPE LOSS: 24.540121467825344\n","batch 69 | flow #4 | EPE LOSS: 23.806296087832305\n","batch 69 | flow #5 | EPE LOSS: 23.298774925756064\n","batch 69 | flow #6 | EPE LOSS: 23.07047088045805\n","batch 69 | flow #7 | EPE LOSS: 23.062527248584857\n","batch 69 | flow #8 | EPE LOSS: 22.876911172801044\n","batch 69 | flow #9 | EPE LOSS: 23.02349956466467\n","batch 69 | flow #10 | EPE LOSS: 23.21897261177729\n","batch 69 | flow #11 | EPE LOSS: 23.524835067948285\n","batch 69 | flow #12 | EPE LOSS: 23.77468037768404\n","batch 69 average EPE LOSS: 20.840716036773483\n"]},{"output_type":"stream","name":"stderr","text":["\r 21%|██        | 70/336 [23:07<1:08:39, 15.49s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 70 | flow #1 | EPE LOSS: 27.051561162425312\n","batch 70 | flow #2 | EPE LOSS: 25.926310920363857\n","batch 70 | flow #3 | EPE LOSS: 24.407147824061497\n","batch 70 | flow #4 | EPE LOSS: 22.928478903176885\n","batch 70 | flow #5 | EPE LOSS: 21.552144860141883\n","batch 70 | flow #6 | EPE LOSS: 20.020482310683644\n","batch 70 | flow #7 | EPE LOSS: 19.245901613364925\n","batch 70 | flow #8 | EPE LOSS: 18.232762489811552\n","batch 70 | flow #9 | EPE LOSS: 17.257829584231704\n","batch 70 | flow #10 | EPE LOSS: 16.679955204507348\n","batch 70 | flow #11 | EPE LOSS: 16.055578284430275\n","batch 70 | flow #12 | EPE LOSS: 15.671261463980686\n","batch 70 average EPE LOSS: 16.635204321897618\n"]},{"output_type":"stream","name":"stderr","text":["\r 21%|██        | 71/336 [23:23<1:08:17, 15.46s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 71 | flow #1 | EPE LOSS: 23.646257023201496\n","batch 71 | flow #2 | EPE LOSS: 25.613542116304988\n","batch 71 | flow #3 | EPE LOSS: 25.168005757896825\n","batch 71 | flow #4 | EPE LOSS: 22.838019624916843\n","batch 71 | flow #5 | EPE LOSS: 21.245686961157798\n","batch 71 | flow #6 | EPE LOSS: 20.61420335913321\n","batch 71 | flow #7 | EPE LOSS: 20.46765772187427\n","batch 71 | flow #8 | EPE LOSS: 20.38274825898833\n","batch 71 | flow #9 | EPE LOSS: 20.26258693159268\n","batch 71 | flow #10 | EPE LOSS: 20.440256559623098\n","batch 71 | flow #11 | EPE LOSS: 20.337338889031514\n","batch 71 | flow #12 | EPE LOSS: 20.017523317204155\n","batch 71 average EPE LOSS: 19.5711924593826\n"]},{"output_type":"stream","name":"stderr","text":["\r 21%|██▏       | 72/336 [23:38<1:08:20, 15.53s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 72 | flow #1 | EPE LOSS: 22.499313074603567\n","batch 72 | flow #2 | EPE LOSS: 23.377637703235646\n","batch 72 | flow #3 | EPE LOSS: 22.422675733732124\n","batch 72 | flow #4 | EPE LOSS: 20.013673370235065\n","batch 72 | flow #5 | EPE LOSS: 18.1153232157571\n","batch 72 | flow #6 | EPE LOSS: 16.933844702578863\n","batch 72 | flow #7 | EPE LOSS: 16.000873193634195\n","batch 72 | flow #8 | EPE LOSS: 15.262838512918691\n","batch 72 | flow #9 | EPE LOSS: 14.653470208171756\n","batch 72 | flow #10 | EPE LOSS: 14.352677193667182\n","batch 72 | flow #11 | EPE LOSS: 14.23390180046102\n","batch 72 | flow #12 | EPE LOSS: 14.095810868841882\n","batch 72 average EPE LOSS: 14.85153101393194\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 73/336 [23:54<1:07:45, 15.46s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 73 | flow #1 | EPE LOSS: 25.348484721085725\n","batch 73 | flow #2 | EPE LOSS: 25.92369982187192\n","batch 73 | flow #3 | EPE LOSS: 24.687585807620238\n","batch 73 | flow #4 | EPE LOSS: 23.618258252317787\n","batch 73 | flow #5 | EPE LOSS: 22.73573316403058\n","batch 73 | flow #6 | EPE LOSS: 21.238829713310185\n","batch 73 | flow #7 | EPE LOSS: 20.036625089810855\n","batch 73 | flow #8 | EPE LOSS: 19.51957186858874\n","batch 73 | flow #9 | EPE LOSS: 19.458303583303557\n","batch 73 | flow #10 | EPE LOSS: 19.772687110761478\n","batch 73 | flow #11 | EPE LOSS: 20.088786177609155\n","batch 73 | flow #12 | EPE LOSS: 20.51821259924866\n","batch 73 average EPE LOSS: 19.2152721502142\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 74/336 [24:09<1:07:53, 15.55s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 74 | flow #1 | EPE LOSS: 24.191196930660613\n","batch 74 | flow #2 | EPE LOSS: 26.321290926058026\n","batch 74 | flow #3 | EPE LOSS: 27.330364309963667\n","batch 74 | flow #4 | EPE LOSS: 28.081957308888253\n","batch 74 | flow #5 | EPE LOSS: 29.00446577754754\n","batch 74 | flow #6 | EPE LOSS: 29.184579888751358\n","batch 74 | flow #7 | EPE LOSS: 29.265541846069237\n","batch 74 | flow #8 | EPE LOSS: 29.325277175901594\n","batch 74 | flow #9 | EPE LOSS: 29.688375309840577\n","batch 74 | flow #10 | EPE LOSS: 30.15945593364299\n","batch 74 | flow #11 | EPE LOSS: 30.66143831723167\n","batch 74 | flow #12 | EPE LOSS: 31.161637388212977\n","batch 74 average EPE LOSS: 26.466748175155132\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 75/336 [24:25<1:07:38, 15.55s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 75 | flow #1 | EPE LOSS: 21.812151253904958\n","batch 75 | flow #2 | EPE LOSS: 23.5783938233315\n","batch 75 | flow #3 | EPE LOSS: 26.463035784520216\n","batch 75 | flow #4 | EPE LOSS: 29.501327122169965\n","batch 75 | flow #5 | EPE LOSS: 32.1693798095434\n","batch 75 | flow #6 | EPE LOSS: 34.19536896246901\n","batch 75 | flow #7 | EPE LOSS: 36.12449958206124\n","batch 75 | flow #8 | EPE LOSS: 37.63322025097709\n","batch 75 | flow #9 | EPE LOSS: 38.714338943670064\n","batch 75 | flow #10 | EPE LOSS: 39.56126122575079\n","batch 75 | flow #11 | EPE LOSS: 39.932716733005066\n","batch 75 | flow #12 | EPE LOSS: 40.169201453855415\n","batch 75 average EPE LOSS: 32.030836252108536\n"]},{"output_type":"stream","name":"stderr","text":["\r 23%|██▎       | 76/336 [24:41<1:07:31, 15.58s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 76 | flow #1 | EPE LOSS: 21.37432657527747\n","batch 76 | flow #2 | EPE LOSS: 22.951777571698027\n","batch 76 | flow #3 | EPE LOSS: 25.574938896182847\n","batch 76 | flow #4 | EPE LOSS: 28.615879554745025\n","batch 76 | flow #5 | EPE LOSS: 31.435471694873062\n","batch 76 | flow #6 | EPE LOSS: 34.50803151213175\n","batch 76 | flow #7 | EPE LOSS: 36.68722201273403\n","batch 76 | flow #8 | EPE LOSS: 38.35186796673241\n","batch 76 | flow #9 | EPE LOSS: 39.462952164089735\n","batch 76 | flow #10 | EPE LOSS: 40.78225234562097\n","batch 76 | flow #11 | EPE LOSS: 41.87753117177832\n","batch 76 | flow #12 | EPE LOSS: 42.824002691600036\n","batch 76 average EPE LOSS: 32.29183435886846\n"]},{"output_type":"stream","name":"stderr","text":["\r 23%|██▎       | 77/336 [24:56<1:07:10, 15.56s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 77 | flow #1 | EPE LOSS: 24.346823767398536\n","batch 77 | flow #2 | EPE LOSS: 26.32377818526146\n","batch 77 | flow #3 | EPE LOSS: 27.540465706416757\n","batch 77 | flow #4 | EPE LOSS: 27.749646040285324\n","batch 77 | flow #5 | EPE LOSS: 28.403019337446565\n","batch 77 | flow #6 | EPE LOSS: 28.879867613732085\n","batch 77 | flow #7 | EPE LOSS: 29.51762393966166\n","batch 77 | flow #8 | EPE LOSS: 30.407601407527096\n","batch 77 | flow #9 | EPE LOSS: 31.171106862317046\n","batch 77 | flow #10 | EPE LOSS: 31.949729618503767\n","batch 77 | flow #11 | EPE LOSS: 32.801668956218954\n","batch 77 | flow #12 | EPE LOSS: 33.67797585073584\n","batch 77 average EPE LOSS: 27.865271857382307\n"]},{"output_type":"stream","name":"stderr","text":["\r 23%|██▎       | 78/336 [25:12<1:07:23, 15.67s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 78 | flow #1 | EPE LOSS: 26.059369470805162\n","batch 78 | flow #2 | EPE LOSS: 25.514929014056378\n","batch 78 | flow #3 | EPE LOSS: 25.506340374765045\n","batch 78 | flow #4 | EPE LOSS: 25.92256300233736\n","batch 78 | flow #5 | EPE LOSS: 26.494231481188223\n","batch 78 | flow #6 | EPE LOSS: 26.816116976988692\n","batch 78 | flow #7 | EPE LOSS: 26.873113110275117\n","batch 78 | flow #8 | EPE LOSS: 27.22822055358806\n","batch 78 | flow #9 | EPE LOSS: 27.678201511399777\n","batch 78 | flow #10 | EPE LOSS: 28.334336062817414\n","batch 78 | flow #11 | EPE LOSS: 28.919975414767055\n","batch 78 | flow #12 | EPE LOSS: 29.30998784466567\n","batch 78 average EPE LOSS: 25.124325151200164\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▎       | 79/336 [25:27<1:06:35, 15.55s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 79 | flow #1 | EPE LOSS: 28.007204849674793\n","batch 79 | flow #2 | EPE LOSS: 26.568398921393225\n","batch 79 | flow #3 | EPE LOSS: 25.031518844519557\n","batch 79 | flow #4 | EPE LOSS: 23.254083931083482\n","batch 79 | flow #5 | EPE LOSS: 22.583286582587522\n","batch 79 | flow #6 | EPE LOSS: 22.782174928017337\n","batch 79 | flow #7 | EPE LOSS: 23.063218502492674\n","batch 79 | flow #8 | EPE LOSS: 23.43264779610557\n","batch 79 | flow #9 | EPE LOSS: 23.899932739180386\n","batch 79 | flow #10 | EPE LOSS: 24.276776568239168\n","batch 79 | flow #11 | EPE LOSS: 24.568495465572976\n","batch 79 | flow #12 | EPE LOSS: 25.016851971128258\n","batch 79 average EPE LOSS: 21.045205320022713\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 80/336 [25:44<1:07:15, 15.76s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 80 | flow #1 | EPE LOSS: 27.800998941369997\n","batch 80 | flow #2 | EPE LOSS: 26.133640086731788\n","batch 80 | flow #3 | EPE LOSS: 23.88800357083726\n","batch 80 | flow #4 | EPE LOSS: 21.617356688249444\n","batch 80 | flow #5 | EPE LOSS: 20.553763752803672\n","batch 80 | flow #6 | EPE LOSS: 19.425989936052527\n","batch 80 | flow #7 | EPE LOSS: 18.621844477468\n","batch 80 | flow #8 | EPE LOSS: 18.523572792449425\n","batch 80 | flow #9 | EPE LOSS: 18.308285981486204\n","batch 80 | flow #10 | EPE LOSS: 18.394479747918556\n","batch 80 | flow #11 | EPE LOSS: 18.539895179162205\n","batch 80 | flow #12 | EPE LOSS: 18.77287249114096\n","batch 80 average EPE LOSS: 15.813465316232287\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 81/336 [25:59<1:06:12, 15.58s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 81 | flow #1 | EPE LOSS: 30.224183655355517\n","batch 81 | flow #2 | EPE LOSS: 31.31998788902195\n","batch 81 | flow #3 | EPE LOSS: 29.099403100697206\n","batch 81 | flow #4 | EPE LOSS: 27.362776172200544\n","batch 81 | flow #5 | EPE LOSS: 26.741178656690362\n","batch 81 | flow #6 | EPE LOSS: 26.71404796745129\n","batch 81 | flow #7 | EPE LOSS: 27.515184771093534\n","batch 81 | flow #8 | EPE LOSS: 28.062523080284635\n","batch 81 | flow #9 | EPE LOSS: 28.814095671427975\n","batch 81 | flow #10 | EPE LOSS: 28.42251061291745\n","batch 81 | flow #11 | EPE LOSS: 27.567319038980575\n","batch 81 | flow #12 | EPE LOSS: 27.342760885303875\n","batch 81 average EPE LOSS: 23.091001612757964\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 82/336 [26:14<1:05:18, 15.43s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 82 | flow #1 | EPE LOSS: 28.15228158207245\n","batch 82 | flow #2 | EPE LOSS: 30.143501616596534\n","batch 82 | flow #3 | EPE LOSS: 30.44554836369583\n","batch 82 | flow #4 | EPE LOSS: 30.763349680350558\n","batch 82 | flow #5 | EPE LOSS: 29.51164791091112\n","batch 82 | flow #6 | EPE LOSS: 29.010314078820358\n","batch 82 | flow #7 | EPE LOSS: 29.203956245753787\n","batch 82 | flow #8 | EPE LOSS: 28.98688397271751\n","batch 82 | flow #9 | EPE LOSS: 29.13434610595832\n","batch 82 | flow #10 | EPE LOSS: 29.369616062339514\n","batch 82 | flow #11 | EPE LOSS: 29.52265287394965\n","batch 82 | flow #12 | EPE LOSS: 29.80227650324329\n","batch 82 average EPE LOSS: 25.351141427349813\n"]},{"output_type":"stream","name":"stderr","text":["\r 25%|██▍       | 83/336 [26:29<1:04:58, 15.41s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 83 | flow #1 | EPE LOSS: 31.471844821596786\n","batch 83 | flow #2 | EPE LOSS: 32.227657137059886\n","batch 83 | flow #3 | EPE LOSS: 29.68209568974172\n","batch 83 | flow #4 | EPE LOSS: 28.53332075317826\n","batch 83 | flow #5 | EPE LOSS: 28.42145602654091\n","batch 83 | flow #6 | EPE LOSS: 28.601312126044217\n","batch 83 | flow #7 | EPE LOSS: 29.71863879108936\n","batch 83 | flow #8 | EPE LOSS: 30.76068180841389\n","batch 83 | flow #9 | EPE LOSS: 31.809186354911915\n","batch 83 | flow #10 | EPE LOSS: 32.438474366395155\n","batch 83 | flow #11 | EPE LOSS: 32.67893430754071\n","batch 83 | flow #12 | EPE LOSS: 33.159108354412275\n","batch 83 average EPE LOSS: 26.689642292193977\n"]},{"output_type":"stream","name":"stderr","text":["\r 25%|██▌       | 84/336 [26:45<1:05:13, 15.53s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 84 | flow #1 | EPE LOSS: 28.95026616843519\n","batch 84 | flow #2 | EPE LOSS: 30.409086185328846\n","batch 84 | flow #3 | EPE LOSS: 29.35039528688408\n","batch 84 | flow #4 | EPE LOSS: 27.908170471151134\n","batch 84 | flow #5 | EPE LOSS: 25.99900642714148\n","batch 84 | flow #6 | EPE LOSS: 24.840995631739514\n","batch 84 | flow #7 | EPE LOSS: 24.275750788151484\n","batch 84 | flow #8 | EPE LOSS: 24.006415221481735\n","batch 84 | flow #9 | EPE LOSS: 23.87873076946364\n","batch 84 | flow #10 | EPE LOSS: 24.03855793256655\n","batch 84 | flow #11 | EPE LOSS: 24.324162363958425\n","batch 84 | flow #12 | EPE LOSS: 24.332422010455588\n","batch 84 average EPE LOSS: 21.418460722531623\n"]},{"output_type":"stream","name":"stderr","text":["\r 25%|██▌       | 85/336 [27:00<1:04:26, 15.40s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 85 | flow #1 | EPE LOSS: 28.168871457499392\n","batch 85 | flow #2 | EPE LOSS: 27.84515122894078\n","batch 85 | flow #3 | EPE LOSS: 27.140126214126685\n","batch 85 | flow #4 | EPE LOSS: 25.152518118474394\n","batch 85 | flow #5 | EPE LOSS: 23.345712434813063\n","batch 85 | flow #6 | EPE LOSS: 21.522400198073402\n","batch 85 | flow #7 | EPE LOSS: 20.735715469672442\n","batch 85 | flow #8 | EPE LOSS: 20.657058213611148\n","batch 85 | flow #9 | EPE LOSS: 19.697087578072267\n","batch 85 | flow #10 | EPE LOSS: 19.069272159590508\n","batch 85 | flow #11 | EPE LOSS: 19.04520551747077\n","batch 85 | flow #12 | EPE LOSS: 19.202216497905162\n","batch 85 average EPE LOSS: 17.881695728058652\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 86/336 [27:16<1:04:17, 15.43s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 86 | flow #1 | EPE LOSS: 27.07521694396752\n","batch 86 | flow #2 | EPE LOSS: 28.796352641019414\n","batch 86 | flow #3 | EPE LOSS: 30.119667762342946\n","batch 86 | flow #4 | EPE LOSS: 30.870980889250433\n","batch 86 | flow #5 | EPE LOSS: 31.100679216376623\n","batch 86 | flow #6 | EPE LOSS: 29.909346232477343\n","batch 86 | flow #7 | EPE LOSS: 29.268864838914936\n","batch 86 | flow #8 | EPE LOSS: 29.800790692986098\n","batch 86 | flow #9 | EPE LOSS: 29.737296208842533\n","batch 86 | flow #10 | EPE LOSS: 30.011034302077636\n","batch 86 | flow #11 | EPE LOSS: 30.700148665334638\n","batch 86 | flow #12 | EPE LOSS: 30.943336987831902\n","batch 86 average EPE LOSS: 25.762888898184727\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 87/336 [27:31<1:03:54, 15.40s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 87 | flow #1 | EPE LOSS: 23.33708520775868\n","batch 87 | flow #2 | EPE LOSS: 22.554614160504318\n","batch 87 | flow #3 | EPE LOSS: 21.250206368799297\n","batch 87 | flow #4 | EPE LOSS: 19.82469502152517\n","batch 87 | flow #5 | EPE LOSS: 18.220386683133565\n","batch 87 | flow #6 | EPE LOSS: 16.690865584723777\n","batch 87 | flow #7 | EPE LOSS: 15.749303495119813\n","batch 87 | flow #8 | EPE LOSS: 15.079051507280035\n","batch 87 | flow #9 | EPE LOSS: 14.57120572898918\n","batch 87 | flow #10 | EPE LOSS: 14.047278907166834\n","batch 87 | flow #11 | EPE LOSS: 13.625977304661285\n","batch 87 | flow #12 | EPE LOSS: 13.342435198671893\n","batch 87 average EPE LOSS: 14.833112598018232\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 88/336 [27:46<1:03:19, 15.32s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 88 | flow #1 | EPE LOSS: 21.19239544078562\n","batch 88 | flow #2 | EPE LOSS: 21.146566169219696\n","batch 88 | flow #3 | EPE LOSS: 19.928151034393416\n","batch 88 | flow #4 | EPE LOSS: 19.0716330836591\n","batch 88 | flow #5 | EPE LOSS: 19.103756615456767\n","batch 88 | flow #6 | EPE LOSS: 19.119406173441575\n","batch 88 | flow #7 | EPE LOSS: 19.12259626405543\n","batch 88 | flow #8 | EPE LOSS: 19.370359624221077\n","batch 88 | flow #9 | EPE LOSS: 19.66993047540807\n","batch 88 | flow #10 | EPE LOSS: 19.983509982936738\n","batch 88 | flow #11 | EPE LOSS: 20.280915337716298\n","batch 88 | flow #12 | EPE LOSS: 20.332538058777025\n","batch 88 average EPE LOSS: 18.440388968021832\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▋       | 89/336 [28:01<1:02:44, 15.24s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 89 | flow #1 | EPE LOSS: 23.332002245566915\n","batch 89 | flow #2 | EPE LOSS: 21.962193416870118\n","batch 89 | flow #3 | EPE LOSS: 20.222874636389538\n","batch 89 | flow #4 | EPE LOSS: 18.812930415271275\n","batch 89 | flow #5 | EPE LOSS: 18.29494151135123\n","batch 89 | flow #6 | EPE LOSS: 18.156850098804053\n","batch 89 | flow #7 | EPE LOSS: 18.351190679479238\n","batch 89 | flow #8 | EPE LOSS: 18.692430064148038\n","batch 89 | flow #9 | EPE LOSS: 19.255322823974414\n","batch 89 | flow #10 | EPE LOSS: 19.613313013201576\n","batch 89 | flow #11 | EPE LOSS: 19.78055001945959\n","batch 89 | flow #12 | EPE LOSS: 19.741598646386887\n","batch 89 average EPE LOSS: 18.075192336283354\n"]},{"output_type":"stream","name":"stderr","text":["\r 27%|██▋       | 90/336 [28:17<1:02:59, 15.37s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 90 | flow #1 | EPE LOSS: 25.11053711203893\n","batch 90 | flow #2 | EPE LOSS: 25.499458573673987\n","batch 90 | flow #3 | EPE LOSS: 23.775212152253022\n","batch 90 | flow #4 | EPE LOSS: 23.449704545252974\n","batch 90 | flow #5 | EPE LOSS: 23.371005132077244\n","batch 90 | flow #6 | EPE LOSS: 23.16121118397368\n","batch 90 | flow #7 | EPE LOSS: 22.943342005176074\n","batch 90 | flow #8 | EPE LOSS: 23.0790944474976\n","batch 90 | flow #9 | EPE LOSS: 23.102346482586963\n","batch 90 | flow #10 | EPE LOSS: 23.25484696824439\n","batch 90 | flow #11 | EPE LOSS: 23.265441729774505\n","batch 90 | flow #12 | EPE LOSS: 23.208955404441237\n","batch 90 average EPE LOSS: 21.75936753476897\n"]},{"output_type":"stream","name":"stderr","text":["\r 27%|██▋       | 91/336 [28:32<1:02:02, 15.20s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 91 | flow #1 | EPE LOSS: 24.707451551913973\n","batch 91 | flow #2 | EPE LOSS: 23.731607890900577\n","batch 91 | flow #3 | EPE LOSS: 22.405405957845414\n","batch 91 | flow #4 | EPE LOSS: 22.267776802637076\n","batch 91 | flow #5 | EPE LOSS: 22.13336997304344\n","batch 91 | flow #6 | EPE LOSS: 22.086707069768362\n","batch 91 | flow #7 | EPE LOSS: 22.64067195373904\n","batch 91 | flow #8 | EPE LOSS: 22.943827025723397\n","batch 91 | flow #9 | EPE LOSS: 23.215038643913758\n","batch 91 | flow #10 | EPE LOSS: 23.20562983230982\n","batch 91 | flow #11 | EPE LOSS: 23.099565728260483\n","batch 91 | flow #12 | EPE LOSS: 22.91794448669263\n","batch 91 average EPE LOSS: 18.816981996379514\n"]},{"output_type":"stream","name":"stderr","text":["\r 27%|██▋       | 92/336 [28:47<1:01:42, 15.18s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 92 | flow #1 | EPE LOSS: 26.009785208730577\n","batch 92 | flow #2 | EPE LOSS: 27.546537585476347\n","batch 92 | flow #3 | EPE LOSS: 27.249921858115883\n","batch 92 | flow #4 | EPE LOSS: 26.309292196777673\n","batch 92 | flow #5 | EPE LOSS: 25.488649752886477\n","batch 92 | flow #6 | EPE LOSS: 24.764035858438415\n","batch 92 | flow #7 | EPE LOSS: 24.764907758646814\n","batch 92 | flow #8 | EPE LOSS: 24.983107613220877\n","batch 92 | flow #9 | EPE LOSS: 25.089771741863828\n","batch 92 | flow #10 | EPE LOSS: 25.071120057766866\n","batch 92 | flow #11 | EPE LOSS: 25.22666387401105\n","batch 92 | flow #12 | EPE LOSS: 25.19948966842884\n","batch 92 average EPE LOSS: 22.839289459722426\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 93/336 [29:01<1:01:01, 15.07s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 93 | flow #1 | EPE LOSS: 21.845085241677086\n","batch 93 | flow #2 | EPE LOSS: 23.253018918396492\n","batch 93 | flow #3 | EPE LOSS: 21.859324000118832\n","batch 93 | flow #4 | EPE LOSS: 20.357211188615242\n","batch 93 | flow #5 | EPE LOSS: 18.687538914017157\n","batch 93 | flow #6 | EPE LOSS: 17.53372370723004\n","batch 93 | flow #7 | EPE LOSS: 17.25972765145295\n","batch 93 | flow #8 | EPE LOSS: 17.50336586776566\n","batch 93 | flow #9 | EPE LOSS: 17.908873276280556\n","batch 93 | flow #10 | EPE LOSS: 18.492552517007613\n","batch 93 | flow #11 | EPE LOSS: 19.551925065311803\n","batch 93 | flow #12 | EPE LOSS: 20.17084191064005\n","batch 93 average EPE LOSS: 15.218457595000805\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 94/336 [29:17<1:01:24, 15.22s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 94 | flow #1 | EPE LOSS: 20.690291658428013\n","batch 94 | flow #2 | EPE LOSS: 19.704366846388925\n","batch 94 | flow #3 | EPE LOSS: 17.46983569170551\n","batch 94 | flow #4 | EPE LOSS: 16.217466294652812\n","batch 94 | flow #5 | EPE LOSS: 15.683340456400245\n","batch 94 | flow #6 | EPE LOSS: 15.04996932054234\n","batch 94 | flow #7 | EPE LOSS: 14.520956390631788\n","batch 94 | flow #8 | EPE LOSS: 14.297788605518397\n","batch 94 | flow #9 | EPE LOSS: 14.179164557027171\n","batch 94 | flow #10 | EPE LOSS: 14.038682541414653\n","batch 94 | flow #11 | EPE LOSS: 13.829132547292668\n","batch 94 | flow #12 | EPE LOSS: 13.691390771645196\n","batch 94 average EPE LOSS: 13.439694238817353\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 95/336 [29:32<1:00:44, 15.12s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 95 | flow #1 | EPE LOSS: 22.325900059894977\n","batch 95 | flow #2 | EPE LOSS: 21.710487691204392\n","batch 95 | flow #3 | EPE LOSS: 19.42880620213671\n","batch 95 | flow #4 | EPE LOSS: 17.579754824868324\n","batch 95 | flow #5 | EPE LOSS: 16.105734364526167\n","batch 95 | flow #6 | EPE LOSS: 15.162689301201018\n","batch 95 | flow #7 | EPE LOSS: 14.576663461494329\n","batch 95 | flow #8 | EPE LOSS: 14.251135784675453\n","batch 95 | flow #9 | EPE LOSS: 13.758502552144082\n","batch 95 | flow #10 | EPE LOSS: 13.372763504042723\n","batch 95 | flow #11 | EPE LOSS: 13.044185798639612\n","batch 95 | flow #12 | EPE LOSS: 12.83782803980633\n","batch 95 average EPE LOSS: 14.230000511309788\n"]},{"output_type":"stream","name":"stderr","text":["\r 29%|██▊       | 96/336 [29:47<1:00:18, 15.08s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 96 | flow #1 | EPE LOSS: 23.07846015897829\n","batch 96 | flow #2 | EPE LOSS: 21.862349935055832\n","batch 96 | flow #3 | EPE LOSS: 20.613618030339985\n","batch 96 | flow #4 | EPE LOSS: 20.703488586725005\n","batch 96 | flow #5 | EPE LOSS: 20.96239335393502\n","batch 96 | flow #6 | EPE LOSS: 21.02382839300751\n","batch 96 | flow #7 | EPE LOSS: 21.21869638299746\n","batch 96 | flow #8 | EPE LOSS: 21.64274267536329\n","batch 96 | flow #9 | EPE LOSS: 22.794378773788143\n","batch 96 | flow #10 | EPE LOSS: 23.629353474689974\n","batch 96 | flow #11 | EPE LOSS: 24.175490893667842\n","batch 96 | flow #12 | EPE LOSS: 24.568518037287514\n","batch 96 average EPE LOSS: 17.772174290519683\n"]},{"output_type":"stream","name":"stderr","text":["\r 29%|██▉       | 97/336 [30:02<59:39, 14.98s/it]  "]},{"output_type":"stream","name":"stdout","text":["batch 97 | flow #1 | EPE LOSS: 26.023612303761052\n","batch 97 | flow #2 | EPE LOSS: 27.62288315966802\n","batch 97 | flow #3 | EPE LOSS: 29.163941387846673\n","batch 97 | flow #4 | EPE LOSS: 30.07183527517574\n","batch 97 | flow #5 | EPE LOSS: 30.95873672061494\n","batch 97 | flow #6 | EPE LOSS: 31.192067959729062\n","batch 97 | flow #7 | EPE LOSS: 31.699935176702144\n","batch 97 | flow #8 | EPE LOSS: 32.51947240819839\n","batch 97 | flow #9 | EPE LOSS: 33.268973613507974\n","batch 97 | flow #10 | EPE LOSS: 33.87356018901718\n","batch 97 | flow #11 | EPE LOSS: 34.41078789921212\n","batch 97 | flow #12 | EPE LOSS: 34.64305197789864\n","batch 97 average EPE LOSS: 29.22794372940747\n"]},{"output_type":"stream","name":"stderr","text":["\r 29%|██▉       | 98/336 [30:17<59:19, 14.96s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 98 | flow #1 | EPE LOSS: 25.133190695152205\n","batch 98 | flow #2 | EPE LOSS: 27.21058917858404\n","batch 98 | flow #3 | EPE LOSS: 27.727161720972934\n","batch 98 | flow #4 | EPE LOSS: 28.273526520146508\n","batch 98 | flow #5 | EPE LOSS: 29.190327909612325\n","batch 98 | flow #6 | EPE LOSS: 30.1712843575411\n","batch 98 | flow #7 | EPE LOSS: 31.08009664838705\n","batch 98 | flow #8 | EPE LOSS: 31.561493671704067\n","batch 98 | flow #9 | EPE LOSS: 31.652830845659295\n","batch 98 | flow #10 | EPE LOSS: 31.63614884834706\n","batch 98 | flow #11 | EPE LOSS: 31.891198404736055\n","batch 98 | flow #12 | EPE LOSS: 32.008941003738734\n","batch 98 average EPE LOSS: 27.779840585728916\n"]},{"output_type":"stream","name":"stderr","text":["\r 29%|██▉       | 99/336 [30:32<1:00:04, 15.21s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 99 | flow #1 | EPE LOSS: 28.555250887387235\n","batch 99 | flow #2 | EPE LOSS: 29.836604285478433\n","batch 99 | flow #3 | EPE LOSS: 29.86272440816953\n","batch 99 | flow #4 | EPE LOSS: 29.817340094674616\n","batch 99 | flow #5 | EPE LOSS: 30.002581714849356\n","batch 99 | flow #6 | EPE LOSS: 30.769730769098388\n","batch 99 | flow #7 | EPE LOSS: 30.505347752799782\n","batch 99 | flow #8 | EPE LOSS: 30.167068175381687\n","batch 99 | flow #9 | EPE LOSS: 30.059588938573246\n","batch 99 | flow #10 | EPE LOSS: 29.81710853779295\n","batch 99 | flow #11 | EPE LOSS: 29.261966111771\n","batch 99 | flow #12 | EPE LOSS: 28.703643419700075\n","batch 99 average EPE LOSS: 27.62350364222439\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|██▉       | 100/336 [30:48<59:56, 15.24s/it] "]},{"output_type":"stream","name":"stdout","text":["batch 100 | flow #1 | EPE LOSS: 27.26301968506542\n","batch 100 | flow #2 | EPE LOSS: 27.37813162912221\n","batch 100 | flow #3 | EPE LOSS: 25.358302009683225\n","batch 100 | flow #4 | EPE LOSS: 23.837686534770782\n","batch 100 | flow #5 | EPE LOSS: 22.77413140376727\n","batch 100 | flow #6 | EPE LOSS: 22.256802768238718\n","batch 100 | flow #7 | EPE LOSS: 21.708036596919666\n","batch 100 | flow #8 | EPE LOSS: 21.44129257134924\n","batch 100 | flow #9 | EPE LOSS: 21.02922616738262\n","batch 100 | flow #10 | EPE LOSS: 20.338423375544274\n","batch 100 | flow #11 | EPE LOSS: 19.64888309369535\n","batch 100 | flow #12 | EPE LOSS: 19.003925037380448\n","batch 100 average EPE LOSS: 19.079019377280293\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 101/336 [31:03<59:52, 15.29s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 101 | flow #1 | EPE LOSS: 25.965459577283283\n","batch 101 | flow #2 | EPE LOSS: 26.987461658773967\n","batch 101 | flow #3 | EPE LOSS: 24.725244131428568\n","batch 101 | flow #4 | EPE LOSS: 22.376246344701258\n","batch 101 | flow #5 | EPE LOSS: 20.62231036859936\n","batch 101 | flow #6 | EPE LOSS: 19.57922585296389\n","batch 101 | flow #7 | EPE LOSS: 19.278009083175444\n","batch 101 | flow #8 | EPE LOSS: 19.91882490859948\n","batch 101 | flow #9 | EPE LOSS: 20.41313754436712\n","batch 101 | flow #10 | EPE LOSS: 21.18412966470444\n","batch 101 | flow #11 | EPE LOSS: 21.933038248369286\n","batch 101 | flow #12 | EPE LOSS: 22.66495863132192\n","batch 101 average EPE LOSS: 17.226006676932755\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 102/336 [31:19<1:00:49, 15.60s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 102 | flow #1 | EPE LOSS: 22.77341014203491\n","batch 102 | flow #2 | EPE LOSS: 20.73887068406088\n","batch 102 | flow #3 | EPE LOSS: 17.658672492085334\n","batch 102 | flow #4 | EPE LOSS: 15.952131684114013\n","batch 102 | flow #5 | EPE LOSS: 16.79196239326451\n","batch 102 | flow #6 | EPE LOSS: 17.329116818624488\n","batch 102 | flow #7 | EPE LOSS: 17.901173965037227\n","batch 102 | flow #8 | EPE LOSS: 18.46115501584461\n","batch 102 | flow #9 | EPE LOSS: 19.286660440755817\n","batch 102 | flow #10 | EPE LOSS: 20.212637865030278\n","batch 102 | flow #11 | EPE LOSS: 20.904866652402372\n","batch 102 | flow #12 | EPE LOSS: 21.668523039103643\n","batch 102 average EPE LOSS: 15.769998598407895\n"]},{"output_type":"stream","name":"stderr","text":["\r 31%|███       | 103/336 [31:34<59:15, 15.26s/it]  "]},{"output_type":"stream","name":"stdout","text":["batch 103 | flow #1 | EPE LOSS: 23.51022347278879\n","batch 103 | flow #2 | EPE LOSS: 21.175704297353942\n","batch 103 | flow #3 | EPE LOSS: 19.316817742065957\n","batch 103 | flow #4 | EPE LOSS: 18.26437717474112\n","batch 103 | flow #5 | EPE LOSS: 18.057439233307523\n","batch 103 | flow #6 | EPE LOSS: 18.52363428996653\n","batch 103 | flow #7 | EPE LOSS: 19.593057522712776\n","batch 103 | flow #8 | EPE LOSS: 20.78243334548627\n","batch 103 | flow #9 | EPE LOSS: 21.715665266820853\n","batch 103 | flow #10 | EPE LOSS: 22.662757427760738\n","batch 103 | flow #11 | EPE LOSS: 23.57391517284684\n","batch 103 | flow #12 | EPE LOSS: 24.4722090840225\n","batch 103 average EPE LOSS: 18.49132519888308\n"]},{"output_type":"stream","name":"stderr","text":["\r 31%|███       | 104/336 [31:49<58:55, 15.24s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 104 | flow #1 | EPE LOSS: 21.943801394098255\n","batch 104 | flow #2 | EPE LOSS: 21.01379840169654\n","batch 104 | flow #3 | EPE LOSS: 19.102637529816203\n","batch 104 | flow #4 | EPE LOSS: 17.254563649535818\n","batch 104 | flow #5 | EPE LOSS: 16.14115389747303\n","batch 104 | flow #6 | EPE LOSS: 15.68683804533939\n","batch 104 | flow #7 | EPE LOSS: 15.567593783413125\n","batch 104 | flow #8 | EPE LOSS: 15.64446364097659\n","batch 104 | flow #9 | EPE LOSS: 15.72063930047048\n","batch 104 | flow #10 | EPE LOSS: 15.708530126995088\n","batch 104 | flow #11 | EPE LOSS: 15.632244274121769\n","batch 104 | flow #12 | EPE LOSS: 15.625798204613266\n","batch 104 average EPE LOSS: 14.649165864902479\n"]},{"output_type":"stream","name":"stderr","text":["\r 31%|███▏      | 105/336 [32:06<1:00:24, 15.69s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 105 | flow #1 | EPE LOSS: 21.98024460539031\n","batch 105 | flow #2 | EPE LOSS: 20.470131950947735\n","batch 105 | flow #3 | EPE LOSS: 18.80114259912025\n","batch 105 | flow #4 | EPE LOSS: 18.054112037997765\n","batch 105 | flow #5 | EPE LOSS: 17.990207168768492\n","batch 105 | flow #6 | EPE LOSS: 17.949974558359386\n","batch 105 | flow #7 | EPE LOSS: 17.831485812382308\n","batch 105 | flow #8 | EPE LOSS: 17.84096862292668\n","batch 105 | flow #9 | EPE LOSS: 17.85398321378922\n","batch 105 | flow #10 | EPE LOSS: 17.718360561546803\n","batch 105 | flow #11 | EPE LOSS: 17.577621240388897\n","batch 105 | flow #12 | EPE LOSS: 17.370330691203094\n","batch 105 average EPE LOSS: 16.235954363960527\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 106/336 [32:21<59:50, 15.61s/it]  "]},{"output_type":"stream","name":"stdout","text":["batch 106 | flow #1 | EPE LOSS: 21.491804832580247\n","batch 106 | flow #2 | EPE LOSS: 21.12963049110866\n","batch 106 | flow #3 | EPE LOSS: 19.21551045266322\n","batch 106 | flow #4 | EPE LOSS: 17.806196513744045\n","batch 106 | flow #5 | EPE LOSS: 16.788876241754238\n","batch 106 | flow #6 | EPE LOSS: 15.786768155377535\n","batch 106 | flow #7 | EPE LOSS: 14.987206741649414\n","batch 106 | flow #8 | EPE LOSS: 14.426217156163432\n","batch 106 | flow #9 | EPE LOSS: 13.959058407308385\n","batch 106 | flow #10 | EPE LOSS: 13.433266361796088\n","batch 106 | flow #11 | EPE LOSS: 12.980818353502675\n","batch 106 | flow #12 | EPE LOSS: 12.452214119142544\n","batch 106 average EPE LOSS: 13.86933416372924\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 107/336 [32:37<1:00:12, 15.77s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 107 | flow #1 | EPE LOSS: 21.54485949693553\n","batch 107 | flow #2 | EPE LOSS: 20.1071601621145\n","batch 107 | flow #3 | EPE LOSS: 17.8727965595992\n","batch 107 | flow #4 | EPE LOSS: 15.915541564998803\n","batch 107 | flow #5 | EPE LOSS: 14.896392219101122\n","batch 107 | flow #6 | EPE LOSS: 14.779984654358708\n","batch 107 | flow #7 | EPE LOSS: 14.856093200853943\n","batch 107 | flow #8 | EPE LOSS: 14.769996697505748\n","batch 107 | flow #9 | EPE LOSS: 14.617175594069494\n","batch 107 | flow #10 | EPE LOSS: 14.508970158428857\n","batch 107 | flow #11 | EPE LOSS: 14.339615524902927\n","batch 107 | flow #12 | EPE LOSS: 14.140496883891913\n","batch 107 average EPE LOSS: 13.30231493560929\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 108/336 [32:53<59:56, 15.78s/it]  "]},{"output_type":"stream","name":"stdout","text":["batch 108 | flow #1 | EPE LOSS: 20.488437795531155\n","batch 108 | flow #2 | EPE LOSS: 19.22757164069296\n","batch 108 | flow #3 | EPE LOSS: 18.015477162535\n","batch 108 | flow #4 | EPE LOSS: 16.44215200123623\n","batch 108 | flow #5 | EPE LOSS: 15.69881917927526\n","batch 108 | flow #6 | EPE LOSS: 14.84332992386764\n","batch 108 | flow #7 | EPE LOSS: 14.325018749504958\n","batch 108 | flow #8 | EPE LOSS: 14.158970200105552\n","batch 108 | flow #9 | EPE LOSS: 14.157079312011637\n","batch 108 | flow #10 | EPE LOSS: 14.216921501742403\n","batch 108 | flow #11 | EPE LOSS: 14.348050140508974\n","batch 108 | flow #12 | EPE LOSS: 14.477202895985522\n","batch 108 average EPE LOSS: 13.365324042916319\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 109/336 [33:09<59:23, 15.70s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 109 | flow #1 | EPE LOSS: 21.384293705430903\n","batch 109 | flow #2 | EPE LOSS: 21.026641289887902\n","batch 109 | flow #3 | EPE LOSS: 20.543135073357885\n","batch 109 | flow #4 | EPE LOSS: 20.474113223961506\n","batch 109 | flow #5 | EPE LOSS: 19.931152022973766\n","batch 109 | flow #6 | EPE LOSS: 19.02336296837959\n","batch 109 | flow #7 | EPE LOSS: 18.327639482511653\n","batch 109 | flow #8 | EPE LOSS: 17.885577714151967\n","batch 109 | flow #9 | EPE LOSS: 17.519470597174514\n","batch 109 | flow #10 | EPE LOSS: 17.400207943334518\n","batch 109 | flow #11 | EPE LOSS: 17.368871809541147\n","batch 109 | flow #12 | EPE LOSS: 17.216151230153717\n","batch 109 average EPE LOSS: 16.731005489873024\n"]},{"output_type":"stream","name":"stderr","text":["\r 33%|███▎      | 110/336 [33:24<59:13, 15.72s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 110 | flow #1 | EPE LOSS: 24.24493267468115\n","batch 110 | flow #2 | EPE LOSS: 24.028592758049715\n","batch 110 | flow #3 | EPE LOSS: 22.69604393781429\n","batch 110 | flow #4 | EPE LOSS: 22.03636326785947\n","batch 110 | flow #5 | EPE LOSS: 23.005264796281654\n","batch 110 | flow #6 | EPE LOSS: 24.098589988303164\n","batch 110 | flow #7 | EPE LOSS: 25.09219054713053\n","batch 110 | flow #8 | EPE LOSS: 25.45436478301463\n","batch 110 | flow #9 | EPE LOSS: 25.965045757783304\n","batch 110 | flow #10 | EPE LOSS: 26.592934805257148\n","batch 110 | flow #11 | EPE LOSS: 27.25384307384398\n","batch 110 | flow #12 | EPE LOSS: 27.62795740331775\n","batch 110 average EPE LOSS: 22.645156931540626\n"]},{"output_type":"stream","name":"stderr","text":["\r 33%|███▎      | 111/336 [33:40<58:52, 15.70s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 111 | flow #1 | EPE LOSS: 21.677854013589197\n","batch 111 | flow #2 | EPE LOSS: 21.072881022655654\n","batch 111 | flow #3 | EPE LOSS: 21.336909012481115\n","batch 111 | flow #4 | EPE LOSS: 22.108344694194187\n","batch 111 | flow #5 | EPE LOSS: 22.082242074904723\n","batch 111 | flow #6 | EPE LOSS: 22.18261913821078\n","batch 111 | flow #7 | EPE LOSS: 22.765708759975567\n","batch 111 | flow #8 | EPE LOSS: 23.620340916928612\n","batch 111 | flow #9 | EPE LOSS: 24.2195084410385\n","batch 111 | flow #10 | EPE LOSS: 24.562397941564047\n","batch 111 | flow #11 | EPE LOSS: 24.718878420984282\n","batch 111 | flow #12 | EPE LOSS: 24.992891636224538\n","batch 111 average EPE LOSS: 20.802467723368512\n"]},{"output_type":"stream","name":"stderr","text":["\r 33%|███▎      | 112/336 [33:56<58:19, 15.62s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 112 | flow #1 | EPE LOSS: 17.902677312628068\n","batch 112 | flow #2 | EPE LOSS: 17.86276310793309\n","batch 112 | flow #3 | EPE LOSS: 17.13966378188655\n","batch 112 | flow #4 | EPE LOSS: 15.896608858271511\n","batch 112 | flow #5 | EPE LOSS: 15.513541151063214\n","batch 112 | flow #6 | EPE LOSS: 16.05937116733961\n","batch 112 | flow #7 | EPE LOSS: 16.79112415855718\n","batch 112 | flow #8 | EPE LOSS: 17.64348453269525\n","batch 112 | flow #9 | EPE LOSS: 18.300761045492717\n","batch 112 | flow #10 | EPE LOSS: 18.857423414502907\n","batch 112 | flow #11 | EPE LOSS: 19.35894385127497\n","batch 112 | flow #12 | EPE LOSS: 19.769522426636502\n","batch 112 average EPE LOSS: 16.124759803899554\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▎      | 113/336 [34:11<58:07, 15.64s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 113 | flow #1 | EPE LOSS: 21.44497618058634\n","batch 113 | flow #2 | EPE LOSS: 20.12232786327643\n","batch 113 | flow #3 | EPE LOSS: 18.58166268486755\n","batch 113 | flow #4 | EPE LOSS: 17.042142687808468\n","batch 113 | flow #5 | EPE LOSS: 15.803929543119823\n","batch 113 | flow #6 | EPE LOSS: 15.145509644738041\n","batch 113 | flow #7 | EPE LOSS: 14.912481801728196\n","batch 113 | flow #8 | EPE LOSS: 14.755685682728535\n","batch 113 | flow #9 | EPE LOSS: 14.601574731239515\n","batch 113 | flow #10 | EPE LOSS: 14.515811183593081\n","batch 113 | flow #11 | EPE LOSS: 14.511112645717704\n","batch 113 | flow #12 | EPE LOSS: 14.527293663766656\n","batch 113 average EPE LOSS: 15.20235837173314\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 114/336 [34:27<57:42, 15.60s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 114 | flow #1 | EPE LOSS: 24.23823600315522\n","batch 114 | flow #2 | EPE LOSS: 25.27493231153052\n","batch 114 | flow #3 | EPE LOSS: 23.758494972628522\n","batch 114 | flow #4 | EPE LOSS: 22.243293203801514\n","batch 114 | flow #5 | EPE LOSS: 20.80956159223768\n","batch 114 | flow #6 | EPE LOSS: 19.936238618969334\n","batch 114 | flow #7 | EPE LOSS: 19.664409704805646\n","batch 114 | flow #8 | EPE LOSS: 19.838057064999838\n","batch 114 | flow #9 | EPE LOSS: 20.176976898152926\n","batch 114 | flow #10 | EPE LOSS: 20.27318875433289\n","batch 114 | flow #11 | EPE LOSS: 20.298630634183638\n","batch 114 | flow #12 | EPE LOSS: 20.427068799120548\n","batch 114 average EPE LOSS: 20.310169116965934\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 115/336 [34:43<57:59, 15.75s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 115 | flow #1 | EPE LOSS: 23.87973527660996\n","batch 115 | flow #2 | EPE LOSS: 23.580405554665386\n","batch 115 | flow #3 | EPE LOSS: 22.79723007454153\n","batch 115 | flow #4 | EPE LOSS: 22.44980235900122\n","batch 115 | flow #5 | EPE LOSS: 20.78795230611192\n","batch 115 | flow #6 | EPE LOSS: 19.273630064492696\n","batch 115 | flow #7 | EPE LOSS: 18.458484130548705\n","batch 115 | flow #8 | EPE LOSS: 17.630365008818437\n","batch 115 | flow #9 | EPE LOSS: 17.15264679627129\n","batch 115 | flow #10 | EPE LOSS: 16.75981902562339\n","batch 115 | flow #11 | EPE LOSS: 16.384998660005355\n","batch 115 | flow #12 | EPE LOSS: 16.17264062679554\n","batch 115 average EPE LOSS: 16.602224724605524\n"]},{"output_type":"stream","name":"stderr","text":["\r 35%|███▍      | 116/336 [34:58<57:38, 15.72s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 116 | flow #1 | EPE LOSS: 22.090141789653938\n","batch 116 | flow #2 | EPE LOSS: 22.77662016042242\n","batch 116 | flow #3 | EPE LOSS: 22.58450582373297\n","batch 116 | flow #4 | EPE LOSS: 22.266431565525306\n","batch 116 | flow #5 | EPE LOSS: 21.15395253635194\n","batch 116 | flow #6 | EPE LOSS: 20.32320579680159\n","batch 116 | flow #7 | EPE LOSS: 19.668941889203825\n","batch 116 | flow #8 | EPE LOSS: 19.63095808894724\n","batch 116 | flow #9 | EPE LOSS: 19.67335020574473\n","batch 116 | flow #10 | EPE LOSS: 19.850182017213736\n","batch 116 | flow #11 | EPE LOSS: 19.662611974571217\n","batch 116 | flow #12 | EPE LOSS: 19.3024595004886\n","batch 116 average EPE LOSS: 18.576648315066723\n"]},{"output_type":"stream","name":"stderr","text":["\r 35%|███▍      | 117/336 [35:15<58:07, 15.92s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 117 | flow #1 | EPE LOSS: 24.611721641065287\n","batch 117 | flow #2 | EPE LOSS: 24.297352105249413\n","batch 117 | flow #3 | EPE LOSS: 24.32385264218288\n","batch 117 | flow #4 | EPE LOSS: 24.08696704282441\n","batch 117 | flow #5 | EPE LOSS: 23.019949954781467\n","batch 117 | flow #6 | EPE LOSS: 22.11294022592462\n","batch 117 | flow #7 | EPE LOSS: 21.792510059593088\n","batch 117 | flow #8 | EPE LOSS: 21.354990813299224\n","batch 117 | flow #9 | EPE LOSS: 21.173643857621695\n","batch 117 | flow #10 | EPE LOSS: 21.037733979177062\n","batch 117 | flow #11 | EPE LOSS: 20.85722133248129\n","batch 117 | flow #12 | EPE LOSS: 20.710911263748958\n","batch 117 average EPE LOSS: 19.870003655174163\n"]},{"output_type":"stream","name":"stderr","text":["\r 35%|███▌      | 118/336 [35:31<57:37, 15.86s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 118 | flow #1 | EPE LOSS: 26.982455194894104\n","batch 118 | flow #2 | EPE LOSS: 28.260383296352014\n","batch 118 | flow #3 | EPE LOSS: 27.50015429287028\n","batch 118 | flow #4 | EPE LOSS: 26.31582562848118\n","batch 118 | flow #5 | EPE LOSS: 25.771161122441757\n","batch 118 | flow #6 | EPE LOSS: 26.841884887732217\n","batch 118 | flow #7 | EPE LOSS: 28.955031054704232\n","batch 118 | flow #8 | EPE LOSS: 30.952532935203738\n","batch 118 | flow #9 | EPE LOSS: 32.53381496788559\n","batch 118 | flow #10 | EPE LOSS: 33.18508908394869\n","batch 118 | flow #11 | EPE LOSS: 33.25736616939803\n","batch 118 | flow #12 | EPE LOSS: 33.35931242735112\n","batch 118 average EPE LOSS: 25.72589915715321\n"]},{"output_type":"stream","name":"stderr","text":["\r 35%|███▌      | 119/336 [35:46<56:31, 15.63s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 119 | flow #1 | EPE LOSS: 29.51756503996118\n","batch 119 | flow #2 | EPE LOSS: 32.06085091044589\n","batch 119 | flow #3 | EPE LOSS: 32.954806685675855\n","batch 119 | flow #4 | EPE LOSS: 32.9745367718916\n","batch 119 | flow #5 | EPE LOSS: 32.97667923729168\n","batch 119 | flow #6 | EPE LOSS: 32.272389004048684\n","batch 119 | flow #7 | EPE LOSS: 31.632189147644322\n","batch 119 | flow #8 | EPE LOSS: 31.11827056343461\n","batch 119 | flow #9 | EPE LOSS: 30.7800593433862\n","batch 119 | flow #10 | EPE LOSS: 30.223080678985117\n","batch 119 | flow #11 | EPE LOSS: 29.560564507812874\n","batch 119 | flow #12 | EPE LOSS: 29.017702674279523\n","batch 119 average EPE LOSS: 28.738774120570653\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 120/336 [36:01<56:23, 15.66s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 120 | flow #1 | EPE LOSS: 30.827769066051143\n","batch 120 | flow #2 | EPE LOSS: 32.32753112742257\n","batch 120 | flow #3 | EPE LOSS: 31.409974438628623\n","batch 120 | flow #4 | EPE LOSS: 30.92265222960533\n","batch 120 | flow #5 | EPE LOSS: 31.51777152115047\n","batch 120 | flow #6 | EPE LOSS: 31.449097129989738\n","batch 120 | flow #7 | EPE LOSS: 30.961894637654876\n","batch 120 | flow #8 | EPE LOSS: 30.78204670929364\n","batch 120 | flow #9 | EPE LOSS: 30.661032464773765\n","batch 120 | flow #10 | EPE LOSS: 30.58575701433124\n","batch 120 | flow #11 | EPE LOSS: 30.471765868485956\n","batch 120 | flow #12 | EPE LOSS: 30.069291671565214\n","batch 120 average EPE LOSS: 27.44612897764588\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 121/336 [36:17<56:33, 15.78s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 121 | flow #1 | EPE LOSS: 29.368356453198956\n","batch 121 | flow #2 | EPE LOSS: 29.432537652305133\n","batch 121 | flow #3 | EPE LOSS: 27.48041020400703\n","batch 121 | flow #4 | EPE LOSS: 24.650178527935058\n","batch 121 | flow #5 | EPE LOSS: 22.504562117844205\n","batch 121 | flow #6 | EPE LOSS: 21.301795016544684\n","batch 121 | flow #7 | EPE LOSS: 20.757438101144526\n","batch 121 | flow #8 | EPE LOSS: 21.00802652995683\n","batch 121 | flow #9 | EPE LOSS: 20.651994187163485\n","batch 121 | flow #10 | EPE LOSS: 19.90640918410745\n","batch 121 | flow #11 | EPE LOSS: 18.955383275793455\n","batch 121 | flow #12 | EPE LOSS: 18.158726254280662\n","batch 121 average EPE LOSS: 18.181730732784306\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▋      | 122/336 [36:33<55:32, 15.57s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 122 | flow #1 | EPE LOSS: 21.422978905606556\n","batch 122 | flow #2 | EPE LOSS: 20.75401250597806\n","batch 122 | flow #3 | EPE LOSS: 19.832332153425735\n","batch 122 | flow #4 | EPE LOSS: 17.99274337953175\n","batch 122 | flow #5 | EPE LOSS: 16.577383304205604\n","batch 122 | flow #6 | EPE LOSS: 14.89362268820743\n","batch 122 | flow #7 | EPE LOSS: 13.846926743526332\n","batch 122 | flow #8 | EPE LOSS: 12.939994084040684\n","batch 122 | flow #9 | EPE LOSS: 12.487052799318986\n","batch 122 | flow #10 | EPE LOSS: 12.168484303378364\n","batch 122 | flow #11 | EPE LOSS: 11.923413349987413\n","batch 122 | flow #12 | EPE LOSS: 11.691440436342317\n","batch 122 average EPE LOSS: 12.285703598992333\n"]},{"output_type":"stream","name":"stderr","text":["\r 37%|███▋      | 123/336 [36:48<55:25, 15.61s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 123 | flow #1 | EPE LOSS: 22.497540059591653\n","batch 123 | flow #2 | EPE LOSS: 21.902769650164498\n","batch 123 | flow #3 | EPE LOSS: 21.299088000420884\n","batch 123 | flow #4 | EPE LOSS: 19.635056482081563\n","batch 123 | flow #5 | EPE LOSS: 18.2685685049841\n","batch 123 | flow #6 | EPE LOSS: 17.925361125130767\n","batch 123 | flow #7 | EPE LOSS: 18.3231692517967\n","batch 123 | flow #8 | EPE LOSS: 18.40443400223769\n","batch 123 | flow #9 | EPE LOSS: 18.581662533352496\n","batch 123 | flow #10 | EPE LOSS: 18.655713815977904\n","batch 123 | flow #11 | EPE LOSS: 19.02544397056467\n","batch 123 | flow #12 | EPE LOSS: 19.463298639572475\n","batch 123 average EPE LOSS: 16.505732980329828\n"]},{"output_type":"stream","name":"stderr","text":["\r 37%|███▋      | 124/336 [37:05<55:53, 15.82s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 124 | flow #1 | EPE LOSS: 23.1876773887158\n","batch 124 | flow #2 | EPE LOSS: 23.684356801973063\n","batch 124 | flow #3 | EPE LOSS: 20.930509874129402\n","batch 124 | flow #4 | EPE LOSS: 17.68733758968739\n","batch 124 | flow #5 | EPE LOSS: 15.182365116633372\n","batch 124 | flow #6 | EPE LOSS: 14.110225844709724\n","batch 124 | flow #7 | EPE LOSS: 13.419915135509825\n","batch 124 | flow #8 | EPE LOSS: 12.960947982076254\n","batch 124 | flow #9 | EPE LOSS: 12.890384705978844\n","batch 124 | flow #10 | EPE LOSS: 13.128272477232619\n","batch 124 | flow #11 | EPE LOSS: 13.586679827683652\n","batch 124 | flow #12 | EPE LOSS: 13.9031409846572\n","batch 124 average EPE LOSS: 11.627851295676662\n"]},{"output_type":"stream","name":"stderr","text":["\r 37%|███▋      | 125/336 [37:19<54:13, 15.42s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 125 | flow #1 | EPE LOSS: 21.972524084349633\n","batch 125 | flow #2 | EPE LOSS: 20.82416302110981\n","batch 125 | flow #3 | EPE LOSS: 18.501701212115808\n","batch 125 | flow #4 | EPE LOSS: 17.39716267495334\n","batch 125 | flow #5 | EPE LOSS: 17.15979373338803\n","batch 125 | flow #6 | EPE LOSS: 16.78057423061918\n","batch 125 | flow #7 | EPE LOSS: 16.657024603408313\n","batch 125 | flow #8 | EPE LOSS: 16.479875362150874\n","batch 125 | flow #9 | EPE LOSS: 16.47663289329184\n","batch 125 | flow #10 | EPE LOSS: 16.102833633385355\n","batch 125 | flow #11 | EPE LOSS: 15.72287662539438\n","batch 125 | flow #12 | EPE LOSS: 15.32462269363866\n","batch 125 average EPE LOSS: 13.436412486318451\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 126/336 [37:34<53:51, 15.39s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 126 | flow #1 | EPE LOSS: 22.957462861633946\n","batch 126 | flow #2 | EPE LOSS: 21.799435154503424\n","batch 126 | flow #3 | EPE LOSS: 19.462701157169523\n","batch 126 | flow #4 | EPE LOSS: 17.588597745556413\n","batch 126 | flow #5 | EPE LOSS: 16.89501962923901\n","batch 126 | flow #6 | EPE LOSS: 16.54036421788681\n","batch 126 | flow #7 | EPE LOSS: 15.978811628867929\n","batch 126 | flow #8 | EPE LOSS: 15.155354341341559\n","batch 126 | flow #9 | EPE LOSS: 14.646619830415375\n","batch 126 | flow #10 | EPE LOSS: 13.86101839212875\n","batch 126 | flow #11 | EPE LOSS: 13.34213671647533\n","batch 126 | flow #12 | EPE LOSS: 13.140409313643467\n","batch 126 average EPE LOSS: 12.572826263127476\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 127/336 [37:50<54:18, 15.59s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 127 | flow #1 | EPE LOSS: 22.43409114467113\n","batch 127 | flow #2 | EPE LOSS: 22.81942329420735\n","batch 127 | flow #3 | EPE LOSS: 21.073950182384397\n","batch 127 | flow #4 | EPE LOSS: 19.923923022798302\n","batch 127 | flow #5 | EPE LOSS: 19.32967450975802\n","batch 127 | flow #6 | EPE LOSS: 19.011262926287397\n","batch 127 | flow #7 | EPE LOSS: 18.54972648308675\n","batch 127 | flow #8 | EPE LOSS: 18.155846867637603\n","batch 127 | flow #9 | EPE LOSS: 17.66031386201636\n","batch 127 | flow #10 | EPE LOSS: 17.223566704707665\n","batch 127 | flow #11 | EPE LOSS: 17.107324214540544\n","batch 127 | flow #12 | EPE LOSS: 16.980405971394795\n","batch 127 average EPE LOSS: 15.622718783692184\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 128/336 [38:05<53:22, 15.40s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 128 | flow #1 | EPE LOSS: 23.053815365103286\n","batch 128 | flow #2 | EPE LOSS: 21.800232197359932\n","batch 128 | flow #3 | EPE LOSS: 20.427270031985092\n","batch 128 | flow #4 | EPE LOSS: 19.24813186505453\n","batch 128 | flow #5 | EPE LOSS: 17.893368664882455\n","batch 128 | flow #6 | EPE LOSS: 17.14960121538905\n","batch 128 | flow #7 | EPE LOSS: 16.605087796820772\n","batch 128 | flow #8 | EPE LOSS: 16.641132755519745\n","batch 128 | flow #9 | EPE LOSS: 16.6070315315163\n","batch 128 | flow #10 | EPE LOSS: 16.145183616819228\n","batch 128 | flow #11 | EPE LOSS: 15.592695762602185\n","batch 128 | flow #12 | EPE LOSS: 15.232269845072482\n","batch 128 average EPE LOSS: 15.352277490169511\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 129/336 [38:21<53:23, 15.48s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 129 | flow #1 | EPE LOSS: 21.882798541719353\n","batch 129 | flow #2 | EPE LOSS: 21.228561917235805\n","batch 129 | flow #3 | EPE LOSS: 19.971969974083127\n","batch 129 | flow #4 | EPE LOSS: 18.41311223704001\n","batch 129 | flow #5 | EPE LOSS: 16.81233196897524\n","batch 129 | flow #6 | EPE LOSS: 15.809791795558745\n","batch 129 | flow #7 | EPE LOSS: 15.463388815517888\n","batch 129 | flow #8 | EPE LOSS: 14.91657707943898\n","batch 129 | flow #9 | EPE LOSS: 14.72541327107321\n","batch 129 | flow #10 | EPE LOSS: 14.762238337137704\n","batch 129 | flow #11 | EPE LOSS: 14.870014315215982\n","batch 129 | flow #12 | EPE LOSS: 14.809485829871873\n","batch 129 average EPE LOSS: 15.222680059741629\n"]},{"output_type":"stream","name":"stderr","text":["\r 39%|███▊      | 130/336 [38:37<54:08, 15.77s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 130 | flow #1 | EPE LOSS: 24.257970577924908\n","batch 130 | flow #2 | EPE LOSS: 23.66326790401388\n","batch 130 | flow #3 | EPE LOSS: 22.130038092365535\n","batch 130 | flow #4 | EPE LOSS: 20.152839921138163\n","batch 130 | flow #5 | EPE LOSS: 18.70894449172353\n","batch 130 | flow #6 | EPE LOSS: 17.749312818920785\n","batch 130 | flow #7 | EPE LOSS: 17.34828963711276\n","batch 130 | flow #8 | EPE LOSS: 17.077265415509597\n","batch 130 | flow #9 | EPE LOSS: 17.088424648384855\n","batch 130 | flow #10 | EPE LOSS: 16.917194681752253\n","batch 130 | flow #11 | EPE LOSS: 16.972065079674255\n","batch 130 | flow #12 | EPE LOSS: 16.855169604201595\n","batch 130 average EPE LOSS: 17.39622386863026\n"]},{"output_type":"stream","name":"stderr","text":["\r 39%|███▉      | 131/336 [38:52<53:01, 15.52s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 131 | flow #1 | EPE LOSS: 21.770309156412782\n","batch 131 | flow #2 | EPE LOSS: 21.468713657650255\n","batch 131 | flow #3 | EPE LOSS: 20.232514709110717\n","batch 131 | flow #4 | EPE LOSS: 19.121051009150847\n","batch 131 | flow #5 | EPE LOSS: 18.17403588932518\n","batch 131 | flow #6 | EPE LOSS: 17.441572240028922\n","batch 131 | flow #7 | EPE LOSS: 16.594958197094503\n","batch 131 | flow #8 | EPE LOSS: 16.279470932637903\n","batch 131 | flow #9 | EPE LOSS: 16.11337562302184\n","batch 131 | flow #10 | EPE LOSS: 15.903712485680659\n","batch 131 | flow #11 | EPE LOSS: 15.91613707093809\n","batch 131 | flow #12 | EPE LOSS: 15.933573119693095\n","batch 131 average EPE LOSS: 15.55278924896632\n"]},{"output_type":"stream","name":"stderr","text":["\r 39%|███▉      | 132/336 [39:08<53:13, 15.65s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 132 | flow #1 | EPE LOSS: 23.884068615238892\n","batch 132 | flow #2 | EPE LOSS: 25.743845464526775\n","batch 132 | flow #3 | EPE LOSS: 27.270747859531326\n","batch 132 | flow #4 | EPE LOSS: 27.912481395055284\n","batch 132 | flow #5 | EPE LOSS: 28.453371197418836\n","batch 132 | flow #6 | EPE LOSS: 28.920520116147163\n","batch 132 | flow #7 | EPE LOSS: 29.34976951478392\n","batch 132 | flow #8 | EPE LOSS: 29.635501019165144\n","batch 132 | flow #9 | EPE LOSS: 29.684485629005867\n","batch 132 | flow #10 | EPE LOSS: 30.244041961780045\n","batch 132 | flow #11 | EPE LOSS: 31.167324543873118\n","batch 132 | flow #12 | EPE LOSS: 31.949755463553796\n","batch 132 average EPE LOSS: 26.60875436380792\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|███▉      | 133/336 [39:24<53:10, 15.72s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 133 | flow #1 | EPE LOSS: 22.650242980034182\n","batch 133 | flow #2 | EPE LOSS: 23.58841323694391\n","batch 133 | flow #3 | EPE LOSS: 24.052766067177558\n","batch 133 | flow #4 | EPE LOSS: 25.352547769321834\n","batch 133 | flow #5 | EPE LOSS: 27.139186923870103\n","batch 133 | flow #6 | EPE LOSS: 29.23406401228117\n","batch 133 | flow #7 | EPE LOSS: 30.875529193656373\n","batch 133 | flow #8 | EPE LOSS: 32.24403639079291\n","batch 133 | flow #9 | EPE LOSS: 33.146023526439734\n","batch 133 | flow #10 | EPE LOSS: 33.86133201172354\n","batch 133 | flow #11 | EPE LOSS: 34.441152692097255\n","batch 133 | flow #12 | EPE LOSS: 34.63423835017383\n","batch 133 average EPE LOSS: 27.091959030579705\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|███▉      | 134/336 [39:41<53:54, 16.01s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 134 | flow #1 | EPE LOSS: 24.716946543457553\n","batch 134 | flow #2 | EPE LOSS: 26.601586930372203\n","batch 134 | flow #3 | EPE LOSS: 29.606405513702445\n","batch 134 | flow #4 | EPE LOSS: 33.184739294387306\n","batch 134 | flow #5 | EPE LOSS: 35.454404790309816\n","batch 134 | flow #6 | EPE LOSS: 36.732187495380664\n","batch 134 | flow #7 | EPE LOSS: 37.447071547956156\n","batch 134 | flow #8 | EPE LOSS: 38.01876985467302\n","batch 134 | flow #9 | EPE LOSS: 38.171550931451776\n","batch 134 | flow #10 | EPE LOSS: 38.29325890130179\n","batch 134 | flow #11 | EPE LOSS: 38.629197745134384\n","batch 134 | flow #12 | EPE LOSS: 38.89735140293517\n","batch 134 average EPE LOSS: 32.9711581579896\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 135/336 [39:56<53:07, 15.86s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 135 | flow #1 | EPE LOSS: 27.042547959455714\n","batch 135 | flow #2 | EPE LOSS: 31.016439007440514\n","batch 135 | flow #3 | EPE LOSS: 34.349917386356076\n","batch 135 | flow #4 | EPE LOSS: 37.598874255000325\n","batch 135 | flow #5 | EPE LOSS: 40.74642501058465\n","batch 135 | flow #6 | EPE LOSS: 43.047940589747384\n","batch 135 | flow #7 | EPE LOSS: 44.640370709188026\n","batch 135 | flow #8 | EPE LOSS: 46.21734751281134\n","batch 135 | flow #9 | EPE LOSS: 47.302400076859634\n","batch 135 | flow #10 | EPE LOSS: 48.04134470239989\n","batch 135 | flow #11 | EPE LOSS: 48.49310191615723\n","batch 135 | flow #12 | EPE LOSS: 48.87627943444044\n","batch 135 average EPE LOSS: 40.26117116925759\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 136/336 [40:12<52:32, 15.76s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 136 | flow #1 | EPE LOSS: 25.052658582905835\n","batch 136 | flow #2 | EPE LOSS: 29.356473383852983\n","batch 136 | flow #3 | EPE LOSS: 33.51283450221586\n","batch 136 | flow #4 | EPE LOSS: 37.99452906848927\n","batch 136 | flow #5 | EPE LOSS: 41.75125696609874\n","batch 136 | flow #6 | EPE LOSS: 45.25274505421006\n","batch 136 | flow #7 | EPE LOSS: 47.25763437636595\n","batch 136 | flow #8 | EPE LOSS: 48.69132312527335\n","batch 136 | flow #9 | EPE LOSS: 50.04708100971447\n","batch 136 | flow #10 | EPE LOSS: 51.1627069437034\n","batch 136 | flow #11 | EPE LOSS: 52.02268422108104\n","batch 136 | flow #12 | EPE LOSS: 52.686250571288525\n","batch 136 average EPE LOSS: 41.42891683891518\n"]},{"output_type":"stream","name":"stderr","text":["\r 41%|████      | 137/336 [40:28<52:12, 15.74s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 137 | flow #1 | EPE LOSS: 28.887369857243353\n","batch 137 | flow #2 | EPE LOSS: 32.58439941338922\n","batch 137 | flow #3 | EPE LOSS: 35.85270264669688\n","batch 137 | flow #4 | EPE LOSS: 39.346370500161036\n","batch 137 | flow #5 | EPE LOSS: 42.30543868489369\n","batch 137 | flow #6 | EPE LOSS: 44.161372124037776\n","batch 137 | flow #7 | EPE LOSS: 45.846458140364234\n","batch 137 | flow #8 | EPE LOSS: 47.191502176891305\n","batch 137 | flow #9 | EPE LOSS: 48.40027746811386\n","batch 137 | flow #10 | EPE LOSS: 49.56088842007222\n","batch 137 | flow #11 | EPE LOSS: 50.490632169986895\n","batch 137 | flow #12 | EPE LOSS: 51.07602733898971\n","batch 137 average EPE LOSS: 41.964119971501894\n"]},{"output_type":"stream","name":"stderr","text":["\r 41%|████      | 138/336 [40:44<52:45, 15.99s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 138 | flow #1 | EPE LOSS: 28.607059714575474\n","batch 138 | flow #2 | EPE LOSS: 29.97453289898206\n","batch 138 | flow #3 | EPE LOSS: 29.799993961500945\n","batch 138 | flow #4 | EPE LOSS: 29.81406047009523\n","batch 138 | flow #5 | EPE LOSS: 29.473222404045202\n","batch 138 | flow #6 | EPE LOSS: 29.00915152213774\n","batch 138 | flow #7 | EPE LOSS: 28.327177729289136\n","batch 138 | flow #8 | EPE LOSS: 27.75596491009534\n","batch 138 | flow #9 | EPE LOSS: 27.57742175197554\n","batch 138 | flow #10 | EPE LOSS: 27.48090619216057\n","batch 138 | flow #11 | EPE LOSS: 27.20809082589145\n","batch 138 | flow #12 | EPE LOSS: 26.900530014869346\n","batch 138 average EPE LOSS: 26.53738720760981\n"]},{"output_type":"stream","name":"stderr","text":["\r 41%|████▏     | 139/336 [41:00<52:09, 15.88s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 139 | flow #1 | EPE LOSS: 24.950266889663748\n","batch 139 | flow #2 | EPE LOSS: 22.83200993817462\n","batch 139 | flow #3 | EPE LOSS: 21.416035344956885\n","batch 139 | flow #4 | EPE LOSS: 20.73041711859427\n","batch 139 | flow #5 | EPE LOSS: 19.945042624748314\n","batch 139 | flow #6 | EPE LOSS: 19.47732869164615\n","batch 139 | flow #7 | EPE LOSS: 19.089800801701124\n","batch 139 | flow #8 | EPE LOSS: 18.708924253291364\n","batch 139 | flow #9 | EPE LOSS: 18.52000027728972\n","batch 139 | flow #10 | EPE LOSS: 18.40312345835235\n","batch 139 | flow #11 | EPE LOSS: 18.139153506399403\n","batch 139 | flow #12 | EPE LOSS: 17.961401545183232\n","batch 139 average EPE LOSS: 18.079315713157033\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 140/336 [41:16<52:00, 15.92s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 140 | flow #1 | EPE LOSS: 18.284879678928828\n","batch 140 | flow #2 | EPE LOSS: 17.844674661689627\n","batch 140 | flow #3 | EPE LOSS: 17.932216607611565\n","batch 140 | flow #4 | EPE LOSS: 17.929787020924\n","batch 140 | flow #5 | EPE LOSS: 17.762984216238653\n","batch 140 | flow #6 | EPE LOSS: 17.73531585678691\n","batch 140 | flow #7 | EPE LOSS: 17.663836631230673\n","batch 140 | flow #8 | EPE LOSS: 17.490663494209738\n","batch 140 | flow #9 | EPE LOSS: 17.259289876839105\n","batch 140 | flow #10 | EPE LOSS: 17.09619138762842\n","batch 140 | flow #11 | EPE LOSS: 16.94492880922301\n","batch 140 | flow #12 | EPE LOSS: 16.868765459986452\n","batch 140 average EPE LOSS: 16.46519431686663\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 141/336 [41:32<51:41, 15.91s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 141 | flow #1 | EPE LOSS: 16.13626445072082\n","batch 141 | flow #2 | EPE LOSS: 15.135151442066622\n","batch 141 | flow #3 | EPE LOSS: 14.779883989594381\n","batch 141 | flow #4 | EPE LOSS: 14.1825545098188\n","batch 141 | flow #5 | EPE LOSS: 13.704495976374739\n","batch 141 | flow #6 | EPE LOSS: 13.373172559262338\n","batch 141 | flow #7 | EPE LOSS: 13.29279211505066\n","batch 141 | flow #8 | EPE LOSS: 13.239204946596681\n","batch 141 | flow #9 | EPE LOSS: 13.109245567187356\n","batch 141 | flow #10 | EPE LOSS: 13.050871378502748\n","batch 141 | flow #11 | EPE LOSS: 12.936282372863028\n","batch 141 | flow #12 | EPE LOSS: 12.870606774376684\n","batch 141 average EPE LOSS: 12.828637496369643\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 142/336 [41:47<51:11, 15.83s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 142 | flow #1 | EPE LOSS: 22.18009190892724\n","batch 142 | flow #2 | EPE LOSS: 19.849572049537063\n","batch 142 | flow #3 | EPE LOSS: 17.502620884896373\n","batch 142 | flow #4 | EPE LOSS: 15.818295603107353\n","batch 142 | flow #5 | EPE LOSS: 14.440651477300912\n","batch 142 | flow #6 | EPE LOSS: 13.370077879049242\n","batch 142 | flow #7 | EPE LOSS: 12.80293916314963\n","batch 142 | flow #8 | EPE LOSS: 12.659628431478302\n","batch 142 | flow #9 | EPE LOSS: 12.872623656201823\n","batch 142 | flow #10 | EPE LOSS: 13.056669860489244\n","batch 142 | flow #11 | EPE LOSS: 13.136400393354334\n","batch 142 | flow #12 | EPE LOSS: 13.273736372672081\n","batch 142 average EPE LOSS: 11.933511338037883\n"]},{"output_type":"stream","name":"stderr","text":["\r 43%|████▎     | 143/336 [42:04<51:37, 16.05s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 143 | flow #1 | EPE LOSS: 21.02545535205717\n","batch 143 | flow #2 | EPE LOSS: 19.1138235468765\n","batch 143 | flow #3 | EPE LOSS: 18.56392892235682\n","batch 143 | flow #4 | EPE LOSS: 18.7022912731211\n","batch 143 | flow #5 | EPE LOSS: 19.06759825835923\n","batch 143 | flow #6 | EPE LOSS: 19.584493765619992\n","batch 143 | flow #7 | EPE LOSS: 19.960509373332552\n","batch 143 | flow #8 | EPE LOSS: 20.290587650880774\n","batch 143 | flow #9 | EPE LOSS: 20.29151069267647\n","batch 143 | flow #10 | EPE LOSS: 20.23985802988233\n","batch 143 | flow #11 | EPE LOSS: 20.205119622528215\n","batch 143 | flow #12 | EPE LOSS: 20.141037479087895\n","batch 143 average EPE LOSS: 17.533724105836086\n"]},{"output_type":"stream","name":"stderr","text":["\r 43%|████▎     | 144/336 [42:19<50:39, 15.83s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 144 | flow #1 | EPE LOSS: 21.137510866668183\n","batch 144 | flow #2 | EPE LOSS: 20.115733784352802\n","batch 144 | flow #3 | EPE LOSS: 19.979064660349717\n","batch 144 | flow #4 | EPE LOSS: 20.514910200495745\n","batch 144 | flow #5 | EPE LOSS: 20.882475739596416\n","batch 144 | flow #6 | EPE LOSS: 21.050219003427472\n","batch 144 | flow #7 | EPE LOSS: 21.016901464486953\n","batch 144 | flow #8 | EPE LOSS: 20.870723950634105\n","batch 144 | flow #9 | EPE LOSS: 20.75387070859831\n","batch 144 | flow #10 | EPE LOSS: 20.685797625156376\n","batch 144 | flow #11 | EPE LOSS: 20.592703677288693\n","batch 144 | flow #12 | EPE LOSS: 20.48069301115191\n","batch 144 average EPE LOSS: 19.763869445502305\n"]},{"output_type":"stream","name":"stderr","text":["\r 43%|████▎     | 145/336 [42:33<47:58, 15.07s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 145 | flow #1 | EPE LOSS: 19.49594297020347\n","batch 145 | flow #2 | EPE LOSS: 17.177355779381866\n","batch 145 | flow #3 | EPE LOSS: 16.560790994525302\n","batch 145 | flow #4 | EPE LOSS: 16.328001900836984\n","batch 145 | flow #5 | EPE LOSS: 16.131687904328746\n","batch 145 | flow #6 | EPE LOSS: 16.010881015848017\n","batch 145 | flow #7 | EPE LOSS: 15.911621953120054\n","batch 145 | flow #8 | EPE LOSS: 15.68962059028364\n","batch 145 | flow #9 | EPE LOSS: 15.524583382868272\n","batch 145 | flow #10 | EPE LOSS: 15.363345442026779\n","batch 145 | flow #11 | EPE LOSS: 15.202942941818485\n","batch 145 | flow #12 | EPE LOSS: 15.08782402890033\n","batch 145 average EPE LOSS: 15.282977558232334\n"]},{"output_type":"stream","name":"stderr","text":["\r 43%|████▎     | 146/336 [42:47<46:46, 14.77s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 146 | flow #1 | EPE LOSS: 19.868662774082285\n","batch 146 | flow #2 | EPE LOSS: 18.231041931405997\n","batch 146 | flow #3 | EPE LOSS: 17.929885587349457\n","batch 146 | flow #4 | EPE LOSS: 17.702097123529487\n","batch 146 | flow #5 | EPE LOSS: 17.539450107885603\n","batch 146 | flow #6 | EPE LOSS: 17.342774587020443\n","batch 146 | flow #7 | EPE LOSS: 17.356309697723958\n","batch 146 | flow #8 | EPE LOSS: 17.371380144372946\n","batch 146 | flow #9 | EPE LOSS: 17.409140503837392\n","batch 146 | flow #10 | EPE LOSS: 17.444421888523614\n","batch 146 | flow #11 | EPE LOSS: 17.430561160665206\n","batch 146 | flow #12 | EPE LOSS: 17.394603262608786\n","batch 146 average EPE LOSS: 17.125571430684563\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 147/336 [43:01<46:22, 14.72s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 147 | flow #1 | EPE LOSS: 25.39279222713455\n","batch 147 | flow #2 | EPE LOSS: 26.000837079443357\n","batch 147 | flow #3 | EPE LOSS: 24.770616534567633\n","batch 147 | flow #4 | EPE LOSS: 22.582588229624918\n","batch 147 | flow #5 | EPE LOSS: 20.542830032618124\n","batch 147 | flow #6 | EPE LOSS: 19.829150750004246\n","batch 147 | flow #7 | EPE LOSS: 19.013966011933523\n","batch 147 | flow #8 | EPE LOSS: 18.078142474782315\n","batch 147 | flow #9 | EPE LOSS: 17.55647332492559\n","batch 147 | flow #10 | EPE LOSS: 17.316008069719967\n","batch 147 | flow #11 | EPE LOSS: 17.22507671480722\n","batch 147 | flow #12 | EPE LOSS: 17.190627860962284\n","batch 147 average EPE LOSS: 16.124558429220496\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 148/336 [43:16<46:07, 14.72s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 148 | flow #1 | EPE LOSS: 35.113943196908124\n","batch 148 | flow #2 | EPE LOSS: 39.04663194679801\n","batch 148 | flow #3 | EPE LOSS: 39.27512095566009\n","batch 148 | flow #4 | EPE LOSS: 41.5081115422983\n","batch 148 | flow #5 | EPE LOSS: 43.86804765435274\n","batch 148 | flow #6 | EPE LOSS: 45.97221815292628\n","batch 148 | flow #7 | EPE LOSS: 48.26770488082195\n","batch 148 | flow #8 | EPE LOSS: 49.14866641067647\n","batch 148 | flow #9 | EPE LOSS: 49.39808785120477\n","batch 148 | flow #10 | EPE LOSS: 49.15717066255053\n","batch 148 | flow #11 | EPE LOSS: 49.044358452679525\n","batch 148 | flow #12 | EPE LOSS: 49.482686489091066\n","batch 148 average EPE LOSS: 39.22816411330316\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 149/336 [43:31<45:48, 14.70s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 149 | flow #1 | EPE LOSS: 35.26970135542488\n","batch 149 | flow #2 | EPE LOSS: 42.17621270557389\n","batch 149 | flow #3 | EPE LOSS: 47.943410338721435\n","batch 149 | flow #4 | EPE LOSS: 52.545342563002045\n","batch 149 | flow #5 | EPE LOSS: 57.61074023898066\n","batch 149 | flow #6 | EPE LOSS: 63.076868544664144\n","batch 149 | flow #7 | EPE LOSS: 68.45949687408974\n","batch 149 | flow #8 | EPE LOSS: 75.34010187994254\n","batch 149 | flow #9 | EPE LOSS: 80.74480625847791\n","batch 149 | flow #10 | EPE LOSS: 85.3733940242552\n","batch 149 | flow #11 | EPE LOSS: 90.08124702847988\n","batch 149 | flow #12 | EPE LOSS: 95.26089117143863\n","batch 149 average EPE LOSS: 54.56201754048251\n"]},{"output_type":"stream","name":"stderr","text":["\r 45%|████▍     | 150/336 [43:45<45:21, 14.63s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 150 | flow #1 | EPE LOSS: 36.657601486711506\n","batch 150 | flow #2 | EPE LOSS: 43.32856252803381\n","batch 150 | flow #3 | EPE LOSS: 49.357955214890715\n","batch 150 | flow #4 | EPE LOSS: 57.70892187302926\n","batch 150 | flow #5 | EPE LOSS: 68.21984343064905\n","batch 150 | flow #6 | EPE LOSS: 80.91940594591067\n","batch 150 | flow #7 | EPE LOSS: 91.69147515249438\n","batch 150 | flow #8 | EPE LOSS: 102.27572569410323\n","batch 150 | flow #9 | EPE LOSS: 113.25486701795592\n","batch 150 | flow #10 | EPE LOSS: 120.99907450923983\n","batch 150 | flow #11 | EPE LOSS: 125.45193381353793\n","batch 150 | flow #12 | EPE LOSS: 128.3000074701815\n","batch 150 average EPE LOSS: 74.55694902951033\n"]},{"output_type":"stream","name":"stderr","text":["\r 45%|████▍     | 151/336 [44:00<45:22, 14.72s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 151 | flow #1 | EPE LOSS: 38.28100187304759\n","batch 151 | flow #2 | EPE LOSS: 43.36465534472158\n","batch 151 | flow #3 | EPE LOSS: 46.486020204638216\n","batch 151 | flow #4 | EPE LOSS: 49.51034794774154\n","batch 151 | flow #5 | EPE LOSS: 53.637535831542614\n","batch 151 | flow #6 | EPE LOSS: 59.56800316993624\n","batch 151 | flow #7 | EPE LOSS: 68.62919866001208\n","batch 151 | flow #8 | EPE LOSS: 80.187270344197\n","batch 151 | flow #9 | EPE LOSS: 90.21296492074025\n","batch 151 | flow #10 | EPE LOSS: 99.62857875947701\n","batch 151 | flow #11 | EPE LOSS: 107.28389809643481\n","batch 151 | flow #12 | EPE LOSS: 112.7144420032372\n","batch 151 average EPE LOSS: 54.02324266424375\n"]},{"output_type":"stream","name":"stderr","text":["\r 45%|████▌     | 152/336 [44:14<44:32, 14.53s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 152 | flow #1 | EPE LOSS: 38.448229803073524\n","batch 152 | flow #2 | EPE LOSS: 46.63501579645767\n","batch 152 | flow #3 | EPE LOSS: 52.36116691266298\n","batch 152 | flow #4 | EPE LOSS: 60.71446972652284\n","batch 152 | flow #5 | EPE LOSS: 68.2202369234716\n","batch 152 | flow #6 | EPE LOSS: 75.38783658195665\n","batch 152 | flow #7 | EPE LOSS: 82.4211968964481\n","batch 152 | flow #8 | EPE LOSS: 90.14415041317403\n","batch 152 | flow #9 | EPE LOSS: 95.83173453992005\n","batch 152 | flow #10 | EPE LOSS: 100.37838386421144\n","batch 152 | flow #11 | EPE LOSS: 104.05609140811914\n","batch 152 | flow #12 | EPE LOSS: 106.91094026321122\n","batch 152 average EPE LOSS: 67.01505366029524\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 153/336 [44:29<44:27, 14.58s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 153 | flow #1 | EPE LOSS: 32.37008329379683\n","batch 153 | flow #2 | EPE LOSS: 39.73926824821334\n","batch 153 | flow #3 | EPE LOSS: 48.24346288624889\n","batch 153 | flow #4 | EPE LOSS: 59.220364120868815\n","batch 153 | flow #5 | EPE LOSS: 68.56867606585746\n","batch 153 | flow #6 | EPE LOSS: 75.07796968116936\n","batch 153 | flow #7 | EPE LOSS: 78.80319798468622\n","batch 153 | flow #8 | EPE LOSS: 82.28238934123526\n","batch 153 | flow #9 | EPE LOSS: 85.34589173491624\n","batch 153 | flow #10 | EPE LOSS: 87.79104678072919\n","batch 153 | flow #11 | EPE LOSS: 90.67912026889452\n","batch 153 | flow #12 | EPE LOSS: 93.77759590392687\n","batch 153 average EPE LOSS: 66.2515783167621\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 154/336 [44:44<44:21, 14.62s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 154 | flow #1 | EPE LOSS: 37.76565234040084\n","batch 154 | flow #2 | EPE LOSS: 46.2064004457703\n","batch 154 | flow #3 | EPE LOSS: 55.70257660306734\n","batch 154 | flow #4 | EPE LOSS: 69.60518205494903\n","batch 154 | flow #5 | EPE LOSS: 83.09786013879966\n","batch 154 | flow #6 | EPE LOSS: 92.848786327634\n","batch 154 | flow #7 | EPE LOSS: 98.33446769326889\n","batch 154 | flow #8 | EPE LOSS: 101.4990344415678\n","batch 154 | flow #9 | EPE LOSS: 103.44643160651843\n","batch 154 | flow #10 | EPE LOSS: 104.83140798083964\n","batch 154 | flow #11 | EPE LOSS: 105.76197018965335\n","batch 154 | flow #12 | EPE LOSS: 106.73036379673643\n","batch 154 average EPE LOSS: 80.06176245615096\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 155/336 [44:58<44:08, 14.63s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 155 | flow #1 | EPE LOSS: 34.81682966272802\n","batch 155 | flow #2 | EPE LOSS: 41.20200736590814\n","batch 155 | flow #3 | EPE LOSS: 48.1516712444963\n","batch 155 | flow #4 | EPE LOSS: 57.8952945671982\n","batch 155 | flow #5 | EPE LOSS: 66.42349532092136\n","batch 155 | flow #6 | EPE LOSS: 73.26146966464061\n","batch 155 | flow #7 | EPE LOSS: 79.04122887878236\n","batch 155 | flow #8 | EPE LOSS: 82.11744297756651\n","batch 155 | flow #9 | EPE LOSS: 83.70523239141055\n","batch 155 | flow #10 | EPE LOSS: 84.51840676110835\n","batch 155 | flow #11 | EPE LOSS: 85.11725518587409\n","batch 155 | flow #12 | EPE LOSS: 85.01806980868044\n","batch 155 average EPE LOSS: 65.29940796070251\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▋     | 156/336 [45:13<43:45, 14.58s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 156 | flow #1 | EPE LOSS: 28.83467900269918\n","batch 156 | flow #2 | EPE LOSS: 29.115591978646012\n","batch 156 | flow #3 | EPE LOSS: 28.680500883256368\n","batch 156 | flow #4 | EPE LOSS: 30.074005299251752\n","batch 156 | flow #5 | EPE LOSS: 31.584126353990808\n","batch 156 | flow #6 | EPE LOSS: 33.02124072870889\n","batch 156 | flow #7 | EPE LOSS: 32.80262201277139\n","batch 156 | flow #8 | EPE LOSS: 32.63935685049895\n","batch 156 | flow #9 | EPE LOSS: 32.84254738454871\n","batch 156 | flow #10 | EPE LOSS: 33.24621702064367\n","batch 156 | flow #11 | EPE LOSS: 33.37864404004151\n","batch 156 | flow #12 | EPE LOSS: 33.438040403060306\n","batch 156 average EPE LOSS: 28.476923291442535\n"]},{"output_type":"stream","name":"stderr","text":["\r 47%|████▋     | 157/336 [45:26<42:36, 14.28s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 157 | flow #1 | EPE LOSS: 26.177265648737713\n","batch 157 | flow #2 | EPE LOSS: 25.75418386231999\n","batch 157 | flow #3 | EPE LOSS: 23.535371893060553\n","batch 157 | flow #4 | EPE LOSS: 21.722772842182945\n","batch 157 | flow #5 | EPE LOSS: 20.64622680229402\n","batch 157 | flow #6 | EPE LOSS: 20.09220146853512\n","batch 157 | flow #7 | EPE LOSS: 20.124300086216333\n","batch 157 | flow #8 | EPE LOSS: 20.290065358000906\n","batch 157 | flow #9 | EPE LOSS: 20.462879720182936\n","batch 157 | flow #10 | EPE LOSS: 20.607876883967375\n","batch 157 | flow #11 | EPE LOSS: 20.819629711477983\n","batch 157 | flow #12 | EPE LOSS: 21.065087527753636\n","batch 157 average EPE LOSS: 19.42834073262852\n"]},{"output_type":"stream","name":"stderr","text":["\r 47%|████▋     | 158/336 [45:40<42:15, 14.25s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 158 | flow #1 | EPE LOSS: 22.99453859805166\n","batch 158 | flow #2 | EPE LOSS: 22.11626597267369\n","batch 158 | flow #3 | EPE LOSS: 21.707866001164735\n","batch 158 | flow #4 | EPE LOSS: 20.955404173683302\n","batch 158 | flow #5 | EPE LOSS: 20.024912240667938\n","batch 158 | flow #6 | EPE LOSS: 19.270957131210793\n","batch 158 | flow #7 | EPE LOSS: 19.27403080797612\n","batch 158 | flow #8 | EPE LOSS: 19.813663279345935\n","batch 158 | flow #9 | EPE LOSS: 20.196758279910505\n","batch 158 | flow #10 | EPE LOSS: 20.623981128175114\n","batch 158 | flow #11 | EPE LOSS: 20.639455729528446\n","batch 158 | flow #12 | EPE LOSS: 20.43037616872818\n","batch 158 average EPE LOSS: 18.65100475600338\n"]},{"output_type":"stream","name":"stderr","text":["\r 47%|████▋     | 159/336 [45:55<42:43, 14.48s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 159 | flow #1 | EPE LOSS: 20.61247370692394\n","batch 159 | flow #2 | EPE LOSS: 19.537534587691265\n","batch 159 | flow #3 | EPE LOSS: 19.00624995229753\n","batch 159 | flow #4 | EPE LOSS: 18.964767365996895\n","batch 159 | flow #5 | EPE LOSS: 19.12652839189041\n","batch 159 | flow #6 | EPE LOSS: 19.4488764497922\n","batch 159 | flow #7 | EPE LOSS: 19.85777861651697\n","batch 159 | flow #8 | EPE LOSS: 20.284495354118125\n","batch 159 | flow #9 | EPE LOSS: 20.598228622493597\n","batch 159 | flow #10 | EPE LOSS: 20.869240169363543\n","batch 159 | flow #11 | EPE LOSS: 21.007198302708193\n","batch 159 | flow #12 | EPE LOSS: 21.070355546296977\n","batch 159 average EPE LOSS: 18.193834373988633\n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 160/336 [46:10<42:09, 14.37s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 160 | flow #1 | EPE LOSS: 23.849022817955174\n","batch 160 | flow #2 | EPE LOSS: 22.192045937948283\n","batch 160 | flow #3 | EPE LOSS: 20.691283608342797\n","batch 160 | flow #4 | EPE LOSS: 20.40112762641094\n","batch 160 | flow #5 | EPE LOSS: 20.97412108956262\n","batch 160 | flow #6 | EPE LOSS: 20.6276459755157\n","batch 160 | flow #7 | EPE LOSS: 20.485113342141727\n","batch 160 | flow #8 | EPE LOSS: 20.51542126596402\n","batch 160 | flow #9 | EPE LOSS: 20.24355956346271\n","batch 160 | flow #10 | EPE LOSS: 20.222020728892435\n","batch 160 | flow #11 | EPE LOSS: 20.040121711104767\n","batch 160 | flow #12 | EPE LOSS: 19.88686905973582\n","batch 160 average EPE LOSS: 18.326086393190735\n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 161/336 [46:25<42:49, 14.68s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 161 | flow #1 | EPE LOSS: 27.888789893700473\n","batch 161 | flow #2 | EPE LOSS: 30.974189564539955\n","batch 161 | flow #3 | EPE LOSS: 33.76516976126168\n","batch 161 | flow #4 | EPE LOSS: 35.870110174746266\n","batch 161 | flow #5 | EPE LOSS: 38.20419819524545\n","batch 161 | flow #6 | EPE LOSS: 39.25768705355157\n","batch 161 | flow #7 | EPE LOSS: 39.81070221717963\n","batch 161 | flow #8 | EPE LOSS: 40.21685836634097\n","batch 161 | flow #9 | EPE LOSS: 39.55158953558269\n","batch 161 | flow #10 | EPE LOSS: 39.04083279597743\n","batch 161 | flow #11 | EPE LOSS: 38.739263357583184\n","batch 161 | flow #12 | EPE LOSS: 38.635950388141595\n","batch 161 average EPE LOSS: 33.39985403284698\n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 162/336 [46:40<42:42, 14.73s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 162 | flow #1 | EPE LOSS: 32.40290030478733\n","batch 162 | flow #2 | EPE LOSS: 40.222504613704906\n","batch 162 | flow #3 | EPE LOSS: 49.05079285680963\n","batch 162 | flow #4 | EPE LOSS: 58.74540595605432\n","batch 162 | flow #5 | EPE LOSS: 68.9981854023186\n","batch 162 | flow #6 | EPE LOSS: 76.99723118807763\n","batch 162 | flow #7 | EPE LOSS: 83.70169754664094\n","batch 162 | flow #8 | EPE LOSS: 87.60751750328616\n","batch 162 | flow #9 | EPE LOSS: 90.06117558203518\n","batch 162 | flow #10 | EPE LOSS: 91.73200806459467\n","batch 162 | flow #11 | EPE LOSS: 92.97306380714984\n","batch 162 | flow #12 | EPE LOSS: 93.87011441481302\n","batch 162 average EPE LOSS: 70.67539480980462\n"]},{"output_type":"stream","name":"stderr","text":["\r 49%|████▊     | 163/336 [46:54<41:56, 14.54s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 163 | flow #1 | EPE LOSS: 30.753581917038968\n","batch 163 | flow #2 | EPE LOSS: 36.26164413287902\n","batch 163 | flow #3 | EPE LOSS: 41.997218026725896\n","batch 163 | flow #4 | EPE LOSS: 46.47373874508204\n","batch 163 | flow #5 | EPE LOSS: 50.425314502429664\n","batch 163 | flow #6 | EPE LOSS: 54.08782813872943\n","batch 163 | flow #7 | EPE LOSS: 57.11892900915418\n","batch 163 | flow #8 | EPE LOSS: 58.929209404558485\n","batch 163 | flow #9 | EPE LOSS: 60.187278419886525\n","batch 163 | flow #10 | EPE LOSS: 60.96586670115872\n","batch 163 | flow #11 | EPE LOSS: 61.39040128870899\n","batch 163 | flow #12 | EPE LOSS: 61.67933706182209\n","batch 163 average EPE LOSS: 51.02156912020808\n"]},{"output_type":"stream","name":"stderr","text":["\r 49%|████▉     | 164/336 [47:09<41:52, 14.61s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 164 | flow #1 | EPE LOSS: 23.201904178610075\n","batch 164 | flow #2 | EPE LOSS: 24.45232044633733\n","batch 164 | flow #3 | EPE LOSS: 25.729639543356566\n","batch 164 | flow #4 | EPE LOSS: 27.426837225155122\n","batch 164 | flow #5 | EPE LOSS: 28.76518153746143\n","batch 164 | flow #6 | EPE LOSS: 29.594481174560325\n","batch 164 | flow #7 | EPE LOSS: 30.220039150250525\n","batch 164 | flow #8 | EPE LOSS: 30.769246169602322\n","batch 164 | flow #9 | EPE LOSS: 31.253573376523345\n","batch 164 | flow #10 | EPE LOSS: 31.69847694502201\n","batch 164 | flow #11 | EPE LOSS: 32.08735233640795\n","batch 164 | flow #12 | EPE LOSS: 32.27118155557587\n","batch 164 average EPE LOSS: 28.138272779199966\n"]},{"output_type":"stream","name":"stderr","text":["\r 49%|████▉     | 165/336 [47:23<41:37, 14.61s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 165 | flow #1 | EPE LOSS: 23.8956353936222\n","batch 165 | flow #2 | EPE LOSS: 23.167499820758906\n","batch 165 | flow #3 | EPE LOSS: 23.6503368297644\n","batch 165 | flow #4 | EPE LOSS: 23.51955452792868\n","batch 165 | flow #5 | EPE LOSS: 23.434689836996416\n","batch 165 | flow #6 | EPE LOSS: 23.351651937775614\n","batch 165 | flow #7 | EPE LOSS: 22.94846824453013\n","batch 165 | flow #8 | EPE LOSS: 22.66533889469686\n","batch 165 | flow #9 | EPE LOSS: 22.443227061917757\n","batch 165 | flow #10 | EPE LOSS: 21.863342924508334\n","batch 165 | flow #11 | EPE LOSS: 21.00785392829631\n","batch 165 | flow #12 | EPE LOSS: 20.051658423494956\n","batch 165 average EPE LOSS: 21.16109396501495\n"]},{"output_type":"stream","name":"stderr","text":["\r 49%|████▉     | 166/336 [47:38<41:06, 14.51s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 166 | flow #1 | EPE LOSS: 19.127517545265857\n","batch 166 | flow #2 | EPE LOSS: 18.06597092180036\n","batch 166 | flow #3 | EPE LOSS: 17.288688598282004\n","batch 166 | flow #4 | EPE LOSS: 16.144628071226418\n","batch 166 | flow #5 | EPE LOSS: 14.886021642558385\n","batch 166 | flow #6 | EPE LOSS: 13.746721595288475\n","batch 166 | flow #7 | EPE LOSS: 13.115085227899437\n","batch 166 | flow #8 | EPE LOSS: 12.528615285560365\n","batch 166 | flow #9 | EPE LOSS: 12.100968763213588\n","batch 166 | flow #10 | EPE LOSS: 11.418179003980121\n","batch 166 | flow #11 | EPE LOSS: 10.741940998272304\n","batch 166 | flow #12 | EPE LOSS: 10.627691869209213\n","batch 166 average EPE LOSS: 11.817608778221334\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|████▉     | 167/336 [47:52<41:08, 14.60s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 167 | flow #1 | EPE LOSS: 17.049689236351057\n","batch 167 | flow #2 | EPE LOSS: 17.252707107208998\n","batch 167 | flow #3 | EPE LOSS: 18.09481354627535\n","batch 167 | flow #4 | EPE LOSS: 17.77480504593203\n","batch 167 | flow #5 | EPE LOSS: 17.11860385167944\n","batch 167 | flow #6 | EPE LOSS: 16.23998798353286\n","batch 167 | flow #7 | EPE LOSS: 15.150127807012792\n","batch 167 | flow #8 | EPE LOSS: 14.295203162168981\n","batch 167 | flow #9 | EPE LOSS: 13.564480330421967\n","batch 167 | flow #10 | EPE LOSS: 12.905782468002656\n","batch 167 | flow #11 | EPE LOSS: 12.440359324465506\n","batch 167 | flow #12 | EPE LOSS: 12.118053979648977\n","batch 167 average EPE LOSS: 14.238508234860168\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 168/336 [48:07<40:32, 14.48s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 168 | flow #1 | EPE LOSS: 16.85143002627102\n","batch 168 | flow #2 | EPE LOSS: 17.5942316532377\n","batch 168 | flow #3 | EPE LOSS: 18.66974962011482\n","batch 168 | flow #4 | EPE LOSS: 19.34893214321881\n","batch 168 | flow #5 | EPE LOSS: 18.635080570771827\n","batch 168 | flow #6 | EPE LOSS: 17.86679405363186\n","batch 168 | flow #7 | EPE LOSS: 17.406728730478658\n","batch 168 | flow #8 | EPE LOSS: 17.05585797797281\n","batch 168 | flow #9 | EPE LOSS: 16.77591278278123\n","batch 168 | flow #10 | EPE LOSS: 16.481107278666304\n","batch 168 | flow #11 | EPE LOSS: 16.08238368068211\n","batch 168 | flow #12 | EPE LOSS: 15.833401110412103\n","batch 168 average EPE LOSS: 16.523131793253985\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 169/336 [48:21<40:31, 14.56s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 169 | flow #1 | EPE LOSS: 18.479152083143234\n","batch 169 | flow #2 | EPE LOSS: 20.385451901463046\n","batch 169 | flow #3 | EPE LOSS: 22.788517380485626\n","batch 169 | flow #4 | EPE LOSS: 24.71810188123726\n","batch 169 | flow #5 | EPE LOSS: 25.737777315274045\n","batch 169 | flow #6 | EPE LOSS: 25.761854030812177\n","batch 169 | flow #7 | EPE LOSS: 25.48574407130218\n","batch 169 | flow #8 | EPE LOSS: 25.2306039249045\n","batch 169 | flow #9 | EPE LOSS: 25.3235479118726\n","batch 169 | flow #10 | EPE LOSS: 25.32384568002284\n","batch 169 | flow #11 | EPE LOSS: 25.38678713007892\n","batch 169 | flow #12 | EPE LOSS: 25.309037101281024\n","batch 169 average EPE LOSS: 23.33124005366217\n"]},{"output_type":"stream","name":"stderr","text":["\r 51%|█████     | 170/336 [48:36<40:18, 14.57s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 170 | flow #1 | EPE LOSS: 15.799273572942097\n","batch 170 | flow #2 | EPE LOSS: 15.195282198707\n","batch 170 | flow #3 | EPE LOSS: 15.49251879353104\n","batch 170 | flow #4 | EPE LOSS: 16.289347313451454\n","batch 170 | flow #5 | EPE LOSS: 16.43519151800096\n","batch 170 | flow #6 | EPE LOSS: 16.146472645489094\n","batch 170 | flow #7 | EPE LOSS: 15.803381198130891\n","batch 170 | flow #8 | EPE LOSS: 15.861411818986568\n","batch 170 | flow #9 | EPE LOSS: 16.25346739506974\n","batch 170 | flow #10 | EPE LOSS: 16.753275979334422\n","batch 170 | flow #11 | EPE LOSS: 17.120085368240606\n","batch 170 | flow #12 | EPE LOSS: 17.36058035589622\n","batch 170 average EPE LOSS: 15.11288100107466\n"]},{"output_type":"stream","name":"stderr","text":["\r 51%|█████     | 171/336 [48:51<40:05, 14.58s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 171 | flow #1 | EPE LOSS: 17.371756384246304\n","batch 171 | flow #2 | EPE LOSS: 17.075011376861298\n","batch 171 | flow #3 | EPE LOSS: 17.03665529243031\n","batch 171 | flow #4 | EPE LOSS: 17.336787712008615\n","batch 171 | flow #5 | EPE LOSS: 17.55463645707131\n","batch 171 | flow #6 | EPE LOSS: 17.257489308445663\n","batch 171 | flow #7 | EPE LOSS: 16.861758803618898\n","batch 171 | flow #8 | EPE LOSS: 16.44779940024628\n","batch 171 | flow #9 | EPE LOSS: 16.068805132025826\n","batch 171 | flow #10 | EPE LOSS: 16.095869438724137\n","batch 171 | flow #11 | EPE LOSS: 16.1864428062526\n","batch 171 | flow #12 | EPE LOSS: 16.177762959853773\n","batch 171 average EPE LOSS: 15.888598332423042\n"]},{"output_type":"stream","name":"stderr","text":["\r 51%|█████     | 172/336 [49:06<40:12, 14.71s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 172 | flow #1 | EPE LOSS: 16.743754716156555\n","batch 172 | flow #2 | EPE LOSS: 16.424110511341247\n","batch 172 | flow #3 | EPE LOSS: 16.183055766556368\n","batch 172 | flow #4 | EPE LOSS: 16.50323497243701\n","batch 172 | flow #5 | EPE LOSS: 16.76133813495301\n","batch 172 | flow #6 | EPE LOSS: 16.647112673133254\n","batch 172 | flow #7 | EPE LOSS: 16.157684585923622\n","batch 172 | flow #8 | EPE LOSS: 15.569449765938755\n","batch 172 | flow #9 | EPE LOSS: 14.97637333118667\n","batch 172 | flow #10 | EPE LOSS: 14.39902989718525\n","batch 172 | flow #11 | EPE LOSS: 13.966460869120986\n","batch 172 | flow #12 | EPE LOSS: 13.701657540526007\n","batch 172 average EPE LOSS: 14.918637068076686\n"]},{"output_type":"stream","name":"stderr","text":["\r 51%|█████▏    | 173/336 [49:21<40:56, 15.07s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 173 | flow #1 | EPE LOSS: 19.792470089552292\n","batch 173 | flow #2 | EPE LOSS: 19.3925879505762\n","batch 173 | flow #3 | EPE LOSS: 20.10482917116599\n","batch 173 | flow #4 | EPE LOSS: 21.115329869991815\n","batch 173 | flow #5 | EPE LOSS: 21.62122333737464\n","batch 173 | flow #6 | EPE LOSS: 21.83538390161357\n","batch 173 | flow #7 | EPE LOSS: 22.00566065814226\n","batch 173 | flow #8 | EPE LOSS: 21.99112418002581\n","batch 173 | flow #9 | EPE LOSS: 21.908053129327797\n","batch 173 | flow #10 | EPE LOSS: 21.76616846790161\n","batch 173 | flow #11 | EPE LOSS: 21.522998607126713\n","batch 173 | flow #12 | EPE LOSS: 21.314564299509556\n","batch 173 average EPE LOSS: 20.29608377899228\n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 174/336 [49:36<40:06, 14.85s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 174 | flow #1 | EPE LOSS: 21.61448528139495\n","batch 174 | flow #2 | EPE LOSS: 22.278490174001778\n","batch 174 | flow #3 | EPE LOSS: 22.61101114382382\n","batch 174 | flow #4 | EPE LOSS: 22.573920943740777\n","batch 174 | flow #5 | EPE LOSS: 21.994082390271974\n","batch 174 | flow #6 | EPE LOSS: 21.123725510384716\n","batch 174 | flow #7 | EPE LOSS: 20.93735708119446\n","batch 174 | flow #8 | EPE LOSS: 21.164364911244405\n","batch 174 | flow #9 | EPE LOSS: 21.158677296715968\n","batch 174 | flow #10 | EPE LOSS: 21.07274744719077\n","batch 174 | flow #11 | EPE LOSS: 20.905811888038013\n","batch 174 | flow #12 | EPE LOSS: 20.733649326208432\n","batch 174 average EPE LOSS: 20.367530511654863\n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 175/336 [49:50<39:39, 14.78s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 175 | flow #1 | EPE LOSS: 21.820639344632756\n","batch 175 | flow #2 | EPE LOSS: 20.993374489741257\n","batch 175 | flow #3 | EPE LOSS: 19.13803035759226\n","batch 175 | flow #4 | EPE LOSS: 18.009484338096385\n","batch 175 | flow #5 | EPE LOSS: 16.970168747650696\n","batch 175 | flow #6 | EPE LOSS: 15.820012570058454\n","batch 175 | flow #7 | EPE LOSS: 15.094384290205019\n","batch 175 | flow #8 | EPE LOSS: 14.467903038746547\n","batch 175 | flow #9 | EPE LOSS: 14.227262763419539\n","batch 175 | flow #10 | EPE LOSS: 14.08452390897442\n","batch 175 | flow #11 | EPE LOSS: 13.953842729037476\n","batch 175 | flow #12 | EPE LOSS: 13.699048396507921\n","batch 175 average EPE LOSS: 14.637202828765192\n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 176/336 [50:06<40:04, 15.03s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 176 | flow #1 | EPE LOSS: 20.202599042778388\n","batch 176 | flow #2 | EPE LOSS: 21.09506153144316\n","batch 176 | flow #3 | EPE LOSS: 21.24587002560772\n","batch 176 | flow #4 | EPE LOSS: 20.87534835795857\n","batch 176 | flow #5 | EPE LOSS: 20.19408171347492\n","batch 176 | flow #6 | EPE LOSS: 19.770178133808848\n","batch 176 | flow #7 | EPE LOSS: 19.57924783347409\n","batch 176 | flow #8 | EPE LOSS: 19.643949303588954\n","batch 176 | flow #9 | EPE LOSS: 19.579289339758287\n","batch 176 | flow #10 | EPE LOSS: 19.530886724619286\n","batch 176 | flow #11 | EPE LOSS: 19.358718473426233\n","batch 176 | flow #12 | EPE LOSS: 19.23626099502614\n","batch 176 average EPE LOSS: 18.918104066808038\n"]},{"output_type":"stream","name":"stderr","text":["\r 53%|█████▎    | 177/336 [50:19<38:35, 14.56s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 177 | flow #1 | EPE LOSS: 18.342741957927373\n","batch 177 | flow #2 | EPE LOSS: 19.063503038015543\n","batch 177 | flow #3 | EPE LOSS: 19.5850070447618\n","batch 177 | flow #4 | EPE LOSS: 20.70916080671798\n","batch 177 | flow #5 | EPE LOSS: 21.83623595713598\n","batch 177 | flow #6 | EPE LOSS: 22.42045465224932\n","batch 177 | flow #7 | EPE LOSS: 22.922487183110572\n","batch 177 | flow #8 | EPE LOSS: 23.346824783391785\n","batch 177 | flow #9 | EPE LOSS: 23.801191903837857\n","batch 177 | flow #10 | EPE LOSS: 24.04897827530016\n","batch 177 | flow #11 | EPE LOSS: 24.01130900066754\n","batch 177 | flow #12 | EPE LOSS: 24.086790905687405\n","batch 177 average EPE LOSS: 21.290755271572685\n"]},{"output_type":"stream","name":"stderr","text":["\r 53%|█████▎    | 178/336 [50:35<38:54, 14.77s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 178 | flow #1 | EPE LOSS: 18.8627176264898\n","batch 178 | flow #2 | EPE LOSS: 18.438885936001153\n","batch 178 | flow #3 | EPE LOSS: 18.6510519950927\n","batch 178 | flow #4 | EPE LOSS: 19.26998695371753\n","batch 178 | flow #5 | EPE LOSS: 19.707992696868633\n","batch 178 | flow #6 | EPE LOSS: 19.78580058024087\n","batch 178 | flow #7 | EPE LOSS: 19.88853090564251\n","batch 178 | flow #8 | EPE LOSS: 20.196945222909246\n","batch 178 | flow #9 | EPE LOSS: 20.609200357447136\n","batch 178 | flow #10 | EPE LOSS: 20.734088604663267\n","batch 178 | flow #11 | EPE LOSS: 20.749329976212746\n","batch 178 | flow #12 | EPE LOSS: 20.756027731261117\n","batch 178 average EPE LOSS: 18.81242679721421\n"]},{"output_type":"stream","name":"stderr","text":["\r 53%|█████▎    | 179/336 [50:49<38:37, 14.76s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 179 | flow #1 | EPE LOSS: 21.51174210997668\n","batch 179 | flow #2 | EPE LOSS: 21.47745681167433\n","batch 179 | flow #3 | EPE LOSS: 20.98643432751656\n","batch 179 | flow #4 | EPE LOSS: 20.63434682528741\n","batch 179 | flow #5 | EPE LOSS: 20.753872573778732\n","batch 179 | flow #6 | EPE LOSS: 20.75297232877664\n","batch 179 | flow #7 | EPE LOSS: 20.70226905109051\n","batch 179 | flow #8 | EPE LOSS: 20.959879110344907\n","batch 179 | flow #9 | EPE LOSS: 21.623257636286485\n","batch 179 | flow #10 | EPE LOSS: 22.26890116768799\n","batch 179 | flow #11 | EPE LOSS: 22.99615855152862\n","batch 179 | flow #12 | EPE LOSS: 23.72739847046266\n","batch 179 average EPE LOSS: 19.16100109318509\n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▎    | 180/336 [51:04<38:13, 14.70s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 180 | flow #1 | EPE LOSS: 21.32262730620989\n","batch 180 | flow #2 | EPE LOSS: 20.857019381431414\n","batch 180 | flow #3 | EPE LOSS: 19.916573279010752\n","batch 180 | flow #4 | EPE LOSS: 19.788720598949592\n","batch 180 | flow #5 | EPE LOSS: 19.394909891277205\n","batch 180 | flow #6 | EPE LOSS: 18.928303723904502\n","batch 180 | flow #7 | EPE LOSS: 18.860685718350847\n","batch 180 | flow #8 | EPE LOSS: 18.31811775861591\n","batch 180 | flow #9 | EPE LOSS: 18.103192120212288\n","batch 180 | flow #10 | EPE LOSS: 18.336723838949982\n","batch 180 | flow #11 | EPE LOSS: 18.649059760115247\n","batch 180 | flow #12 | EPE LOSS: 18.74731904877321\n","batch 180 average EPE LOSS: 15.936590619085088\n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▍    | 181/336 [51:19<38:11, 14.78s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 181 | flow #1 | EPE LOSS: 21.57159072279792\n","batch 181 | flow #2 | EPE LOSS: 21.788136617421657\n","batch 181 | flow #3 | EPE LOSS: 20.26729574869694\n","batch 181 | flow #4 | EPE LOSS: 18.250579655834485\n","batch 181 | flow #5 | EPE LOSS: 16.438623568440722\n","batch 181 | flow #6 | EPE LOSS: 15.564752530301668\n","batch 181 | flow #7 | EPE LOSS: 15.3124516447388\n","batch 181 | flow #8 | EPE LOSS: 14.849322044147938\n","batch 181 | flow #9 | EPE LOSS: 14.37569413729623\n","batch 181 | flow #10 | EPE LOSS: 13.862475808731702\n","batch 181 | flow #11 | EPE LOSS: 13.628824389769148\n","batch 181 | flow #12 | EPE LOSS: 13.464175344984946\n","batch 181 average EPE LOSS: 14.531496047805932\n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▍    | 182/336 [51:34<38:19, 14.93s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 182 | flow #1 | EPE LOSS: 20.097477581068688\n","batch 182 | flow #2 | EPE LOSS: 18.769852884882482\n","batch 182 | flow #3 | EPE LOSS: 16.860780382434488\n","batch 182 | flow #4 | EPE LOSS: 15.452828628827666\n","batch 182 | flow #5 | EPE LOSS: 14.78997907148972\n","batch 182 | flow #6 | EPE LOSS: 14.490316391847061\n","batch 182 | flow #7 | EPE LOSS: 14.249275376579119\n","batch 182 | flow #8 | EPE LOSS: 14.051934200525281\n","batch 182 | flow #9 | EPE LOSS: 13.858569624515455\n","batch 182 | flow #10 | EPE LOSS: 13.725834003786836\n","batch 182 | flow #11 | EPE LOSS: 13.8018629621728\n","batch 182 | flow #12 | EPE LOSS: 13.82995950339384\n","batch 182 average EPE LOSS: 13.828313037744984\n"]},{"output_type":"stream","name":"stderr","text":["\r 54%|█████▍    | 183/336 [51:49<37:43, 14.80s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 183 | flow #1 | EPE LOSS: 19.572313910788797\n","batch 183 | flow #2 | EPE LOSS: 19.131365485794237\n","batch 183 | flow #3 | EPE LOSS: 18.1818733137208\n","batch 183 | flow #4 | EPE LOSS: 17.70519359800147\n","batch 183 | flow #5 | EPE LOSS: 17.234863145271056\n","batch 183 | flow #6 | EPE LOSS: 16.86054797190387\n","batch 183 | flow #7 | EPE LOSS: 16.78667912540957\n","batch 183 | flow #8 | EPE LOSS: 16.857408263721634\n","batch 183 | flow #9 | EPE LOSS: 16.825012290781917\n","batch 183 | flow #10 | EPE LOSS: 16.744427136021613\n","batch 183 | flow #11 | EPE LOSS: 16.80269435470963\n","batch 183 | flow #12 | EPE LOSS: 16.8253521059092\n","batch 183 average EPE LOSS: 16.418182300244606\n"]},{"output_type":"stream","name":"stderr","text":["\r 55%|█████▍    | 184/336 [52:04<37:29, 14.80s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 184 | flow #1 | EPE LOSS: 20.483880607509686\n","batch 184 | flow #2 | EPE LOSS: 19.370188160985744\n","batch 184 | flow #3 | EPE LOSS: 17.674130396495627\n","batch 184 | flow #4 | EPE LOSS: 16.682209429056243\n","batch 184 | flow #5 | EPE LOSS: 16.346096724523104\n","batch 184 | flow #6 | EPE LOSS: 16.142389583735554\n","batch 184 | flow #7 | EPE LOSS: 16.29658904693731\n","batch 184 | flow #8 | EPE LOSS: 16.52995599986691\n","batch 184 | flow #9 | EPE LOSS: 16.709324648796684\n","batch 184 | flow #10 | EPE LOSS: 16.853491051223717\n","batch 184 | flow #11 | EPE LOSS: 16.985685366827585\n","batch 184 | flow #12 | EPE LOSS: 17.095531408498278\n","batch 184 average EPE LOSS: 15.256535214627581\n"]},{"output_type":"stream","name":"stderr","text":["\r 55%|█████▌    | 185/336 [52:19<37:25, 14.87s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 185 | flow #1 | EPE LOSS: 23.873176951851363\n","batch 185 | flow #2 | EPE LOSS: 22.853813878350646\n","batch 185 | flow #3 | EPE LOSS: 20.57284189004537\n","batch 185 | flow #4 | EPE LOSS: 18.997101584405296\n","batch 185 | flow #5 | EPE LOSS: 18.23076870268928\n","batch 185 | flow #6 | EPE LOSS: 18.01474275762671\n","batch 185 | flow #7 | EPE LOSS: 17.89215192397517\n","batch 185 | flow #8 | EPE LOSS: 17.704323061507992\n","batch 185 | flow #9 | EPE LOSS: 17.599084594107783\n","batch 185 | flow #10 | EPE LOSS: 17.54797458020388\n","batch 185 | flow #11 | EPE LOSS: 17.33138607168451\n","batch 185 | flow #12 | EPE LOSS: 17.243245895495352\n","batch 185 average EPE LOSS: 16.23646225430892\n"]},{"output_type":"stream","name":"stderr","text":["\r 55%|█████▌    | 186/336 [52:33<36:36, 14.65s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 186 | flow #1 | EPE LOSS: 20.092662461431594\n","batch 186 | flow #2 | EPE LOSS: 20.693792750157\n","batch 186 | flow #3 | EPE LOSS: 19.643152192907724\n","batch 186 | flow #4 | EPE LOSS: 18.936463943706844\n","batch 186 | flow #5 | EPE LOSS: 18.15528021537948\n","batch 186 | flow #6 | EPE LOSS: 17.312943306181804\n","batch 186 | flow #7 | EPE LOSS: 16.69065844334381\n","batch 186 | flow #8 | EPE LOSS: 16.106196308961064\n","batch 186 | flow #9 | EPE LOSS: 15.76751149667073\n","batch 186 | flow #10 | EPE LOSS: 15.429850071633956\n","batch 186 | flow #11 | EPE LOSS: 15.004031965621948\n","batch 186 | flow #12 | EPE LOSS: 14.657576965045507\n","batch 186 average EPE LOSS: 15.978086973664036\n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 187/336 [52:47<36:17, 14.61s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 187 | flow #1 | EPE LOSS: 18.04670736649195\n","batch 187 | flow #2 | EPE LOSS: 18.45562764417347\n","batch 187 | flow #3 | EPE LOSS: 18.86339455295355\n","batch 187 | flow #4 | EPE LOSS: 19.81743353413302\n","batch 187 | flow #5 | EPE LOSS: 20.53058922165938\n","batch 187 | flow #6 | EPE LOSS: 21.290616482361983\n","batch 187 | flow #7 | EPE LOSS: 21.602408890600355\n","batch 187 | flow #8 | EPE LOSS: 21.665285216352633\n","batch 187 | flow #9 | EPE LOSS: 21.826145720391434\n","batch 187 | flow #10 | EPE LOSS: 21.735279329854276\n","batch 187 | flow #11 | EPE LOSS: 21.57542658108159\n","batch 187 | flow #12 | EPE LOSS: 21.54884660628436\n","batch 187 average EPE LOSS: 19.417257910306166\n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 188/336 [53:02<36:16, 14.70s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 188 | flow #1 | EPE LOSS: 17.39408095674657\n","batch 188 | flow #2 | EPE LOSS: 18.424042859387523\n","batch 188 | flow #3 | EPE LOSS: 20.145005769879184\n","batch 188 | flow #4 | EPE LOSS: 21.680001093335253\n","batch 188 | flow #5 | EPE LOSS: 22.377896289329584\n","batch 188 | flow #6 | EPE LOSS: 22.23278358124539\n","batch 188 | flow #7 | EPE LOSS: 21.8492661336896\n","batch 188 | flow #8 | EPE LOSS: 21.282308106028207\n","batch 188 | flow #9 | EPE LOSS: 20.73402189831444\n","batch 188 | flow #10 | EPE LOSS: 20.40374586806109\n","batch 188 | flow #11 | EPE LOSS: 20.12055282853341\n","batch 188 | flow #12 | EPE LOSS: 19.94241308517631\n","batch 188 average EPE LOSS: 19.591834993048973\n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▋    | 189/336 [53:16<35:34, 14.52s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 189 | flow #1 | EPE LOSS: 16.85357991857543\n","batch 189 | flow #2 | EPE LOSS: 16.87060787321182\n","batch 189 | flow #3 | EPE LOSS: 16.92428242953097\n","batch 189 | flow #4 | EPE LOSS: 17.14253836067218\n","batch 189 | flow #5 | EPE LOSS: 17.088496247004368\n","batch 189 | flow #6 | EPE LOSS: 16.785068419740576\n","batch 189 | flow #7 | EPE LOSS: 16.24834569499795\n","batch 189 | flow #8 | EPE LOSS: 15.90112449623345\n","batch 189 | flow #9 | EPE LOSS: 15.70627371811959\n","batch 189 | flow #10 | EPE LOSS: 15.587296836073596\n","batch 189 | flow #11 | EPE LOSS: 15.395844441953924\n","batch 189 | flow #12 | EPE LOSS: 15.209842919145013\n","batch 189 average EPE LOSS: 15.665592928295043\n"]},{"output_type":"stream","name":"stderr","text":["\r 57%|█████▋    | 190/336 [53:30<35:04, 14.42s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 190 | flow #1 | EPE LOSS: 16.181838894701134\n","batch 190 | flow #2 | EPE LOSS: 15.253249862274496\n","batch 190 | flow #3 | EPE LOSS: 15.335452849291068\n","batch 190 | flow #4 | EPE LOSS: 15.461158151085575\n","batch 190 | flow #5 | EPE LOSS: 15.402100396858197\n","batch 190 | flow #6 | EPE LOSS: 15.282944020495007\n","batch 190 | flow #7 | EPE LOSS: 15.217862826080621\n","batch 190 | flow #8 | EPE LOSS: 15.013870066058752\n","batch 190 | flow #9 | EPE LOSS: 14.794686659216675\n","batch 190 | flow #10 | EPE LOSS: 14.519777165734858\n","batch 190 | flow #11 | EPE LOSS: 14.261824797913922\n","batch 190 | flow #12 | EPE LOSS: 14.111393568859114\n","batch 190 average EPE LOSS: 14.3420834193088\n"]},{"output_type":"stream","name":"stderr","text":["\r 57%|█████▋    | 191/336 [53:46<35:25, 14.66s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 191 | flow #1 | EPE LOSS: 14.75655090911004\n","batch 191 | flow #2 | EPE LOSS: 15.509636256624288\n","batch 191 | flow #3 | EPE LOSS: 15.870105599028447\n","batch 191 | flow #4 | EPE LOSS: 15.567051135537346\n","batch 191 | flow #5 | EPE LOSS: 15.226043330753976\n","batch 191 | flow #6 | EPE LOSS: 14.958499095837551\n","batch 191 | flow #7 | EPE LOSS: 14.603775831006184\n","batch 191 | flow #8 | EPE LOSS: 14.374324240657932\n","batch 191 | flow #9 | EPE LOSS: 14.156330575618512\n","batch 191 | flow #10 | EPE LOSS: 13.964462310137181\n","batch 191 | flow #11 | EPE LOSS: 13.877121749598153\n","batch 191 | flow #12 | EPE LOSS: 13.781944892556787\n","batch 191 average EPE LOSS: 14.145906645930866\n"]},{"output_type":"stream","name":"stderr","text":["\r 57%|█████▋    | 192/336 [54:00<35:05, 14.62s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 192 | flow #1 | EPE LOSS: 14.916988814905737\n","batch 192 | flow #2 | EPE LOSS: 15.405914639668929\n","batch 192 | flow #3 | EPE LOSS: 16.10932048657676\n","batch 192 | flow #4 | EPE LOSS: 16.318305111234768\n","batch 192 | flow #5 | EPE LOSS: 16.19798203246938\n","batch 192 | flow #6 | EPE LOSS: 16.014759078138926\n","batch 192 | flow #7 | EPE LOSS: 16.021165106735285\n","batch 192 | flow #8 | EPE LOSS: 16.04692917158187\n","batch 192 | flow #9 | EPE LOSS: 16.08388844011446\n","batch 192 | flow #10 | EPE LOSS: 16.113450595768196\n","batch 192 | flow #11 | EPE LOSS: 16.137807676603376\n","batch 192 | flow #12 | EPE LOSS: 16.15979417061903\n","batch 192 average EPE LOSS: 15.318127302597988\n"]},{"output_type":"stream","name":"stderr","text":["\r 57%|█████▋    | 193/336 [54:14<33:59, 14.26s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 193 | flow #1 | EPE LOSS: 14.80635187497717\n","batch 193 | flow #2 | EPE LOSS: 16.269665601895895\n","batch 193 | flow #3 | EPE LOSS: 18.408956569401443\n","batch 193 | flow #4 | EPE LOSS: 19.60013247600128\n","batch 193 | flow #5 | EPE LOSS: 20.551891449237683\n","batch 193 | flow #6 | EPE LOSS: 21.215523733568308\n","batch 193 | flow #7 | EPE LOSS: 21.565829070353928\n","batch 193 | flow #8 | EPE LOSS: 21.693767784691445\n","batch 193 | flow #9 | EPE LOSS: 21.79748027359845\n","batch 193 | flow #10 | EPE LOSS: 21.760751734348265\n","batch 193 | flow #11 | EPE LOSS: 21.634796656307874\n","batch 193 | flow #12 | EPE LOSS: 21.57421385475161\n","batch 193 average EPE LOSS: 19.271520723103468\n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 194/336 [54:28<33:31, 14.17s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 194 | flow #1 | EPE LOSS: 17.444596685662756\n","batch 194 | flow #2 | EPE LOSS: 17.539252934732726\n","batch 194 | flow #3 | EPE LOSS: 17.73960954305363\n","batch 194 | flow #4 | EPE LOSS: 18.62468122129658\n","batch 194 | flow #5 | EPE LOSS: 19.502173444185946\n","batch 194 | flow #6 | EPE LOSS: 20.953563210214295\n","batch 194 | flow #7 | EPE LOSS: 22.280248710245534\n","batch 194 | flow #8 | EPE LOSS: 23.239797696883354\n","batch 194 | flow #9 | EPE LOSS: 23.709015151741383\n","batch 194 | flow #10 | EPE LOSS: 23.969935185174844\n","batch 194 | flow #11 | EPE LOSS: 24.07900485179702\n","batch 194 | flow #12 | EPE LOSS: 24.09136264916445\n","batch 194 average EPE LOSS: 19.591124127298446\n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 195/336 [54:41<32:40, 13.91s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 195 | flow #1 | EPE LOSS: 20.01018873375539\n","batch 195 | flow #2 | EPE LOSS: 18.87025734649674\n","batch 195 | flow #3 | EPE LOSS: 18.946648982210387\n","batch 195 | flow #4 | EPE LOSS: 19.648110437740915\n","batch 195 | flow #5 | EPE LOSS: 20.28673880588015\n","batch 195 | flow #6 | EPE LOSS: 21.037891830152933\n","batch 195 | flow #7 | EPE LOSS: 21.844047734090342\n","batch 195 | flow #8 | EPE LOSS: 22.15430976818929\n","batch 195 | flow #9 | EPE LOSS: 22.434351896645826\n","batch 195 | flow #10 | EPE LOSS: 22.70578536501598\n","batch 195 | flow #11 | EPE LOSS: 22.859547833515204\n","batch 195 | flow #12 | EPE LOSS: 22.8352168539857\n","batch 195 average EPE LOSS: 19.741786319348\n"]},{"output_type":"stream","name":"stderr","text":["\r 58%|█████▊    | 196/336 [54:55<32:53, 14.10s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 196 | flow #1 | EPE LOSS: 21.498175658353176\n","batch 196 | flow #2 | EPE LOSS: 22.376881970959072\n","batch 196 | flow #3 | EPE LOSS: 21.92173480277396\n","batch 196 | flow #4 | EPE LOSS: 21.65218722683927\n","batch 196 | flow #5 | EPE LOSS: 22.262991593009232\n","batch 196 | flow #6 | EPE LOSS: 22.688031584219555\n","batch 196 | flow #7 | EPE LOSS: 22.841738935287655\n","batch 196 | flow #8 | EPE LOSS: 22.725581753568118\n","batch 196 | flow #9 | EPE LOSS: 22.675073826282514\n","batch 196 | flow #10 | EPE LOSS: 22.81902034936495\n","batch 196 | flow #11 | EPE LOSS: 23.142385853520835\n","batch 196 | flow #12 | EPE LOSS: 23.413216673069325\n","batch 196 average EPE LOSS: 20.96282023429112\n"]},{"output_type":"stream","name":"stderr","text":["\r 59%|█████▊    | 197/336 [55:11<33:33, 14.48s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 197 | flow #1 | EPE LOSS: 19.669459563847994\n","batch 197 | flow #2 | EPE LOSS: 18.843574800387778\n","batch 197 | flow #3 | EPE LOSS: 19.431765805530006\n","batch 197 | flow #4 | EPE LOSS: 20.07490825673931\n","batch 197 | flow #5 | EPE LOSS: 20.62694395803493\n","batch 197 | flow #6 | EPE LOSS: 21.02346864697284\n","batch 197 | flow #7 | EPE LOSS: 21.46999796679543\n","batch 197 | flow #8 | EPE LOSS: 22.41332543324449\n","batch 197 | flow #9 | EPE LOSS: 23.29329806825791\n","batch 197 | flow #10 | EPE LOSS: 23.769416450874846\n","batch 197 | flow #11 | EPE LOSS: 24.324723933009924\n","batch 197 | flow #12 | EPE LOSS: 25.155151968711166\n","batch 197 average EPE LOSS: 19.809756540490955\n"]},{"output_type":"stream","name":"stderr","text":["\r 59%|█████▉    | 198/336 [55:25<33:01, 14.36s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 198 | flow #1 | EPE LOSS: 19.932125665364435\n","batch 198 | flow #2 | EPE LOSS: 18.911759209303963\n","batch 198 | flow #3 | EPE LOSS: 18.83816256536292\n","batch 198 | flow #4 | EPE LOSS: 18.92885238478046\n","batch 198 | flow #5 | EPE LOSS: 19.410931410359016\n","batch 198 | flow #6 | EPE LOSS: 20.24205901814229\n","batch 198 | flow #7 | EPE LOSS: 20.874657436262186\n","batch 198 | flow #8 | EPE LOSS: 21.177026210433358\n","batch 198 | flow #9 | EPE LOSS: 21.61069292361476\n","batch 198 | flow #10 | EPE LOSS: 22.018243756733487\n","batch 198 | flow #11 | EPE LOSS: 22.42166632883747\n","batch 198 | flow #12 | EPE LOSS: 22.839275403806607\n","batch 198 average EPE LOSS: 19.19185269426117\n"]},{"output_type":"stream","name":"stderr","text":["\r 59%|█████▉    | 199/336 [55:39<32:48, 14.37s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 199 | flow #1 | EPE LOSS: 25.33094345768068\n","batch 199 | flow #2 | EPE LOSS: 25.602298955213254\n","batch 199 | flow #3 | EPE LOSS: 24.42746896054979\n","batch 199 | flow #4 | EPE LOSS: 23.049954189074416\n","batch 199 | flow #5 | EPE LOSS: 22.05832309226679\n","batch 199 | flow #6 | EPE LOSS: 21.504192234078\n","batch 199 | flow #7 | EPE LOSS: 20.943592123908335\n","batch 199 | flow #8 | EPE LOSS: 20.64171707376371\n","batch 199 | flow #9 | EPE LOSS: 20.49740160011771\n","batch 199 | flow #10 | EPE LOSS: 20.3998663484027\n","batch 199 | flow #11 | EPE LOSS: 20.369610108921616\n","batch 199 | flow #12 | EPE LOSS: 20.498580917932284\n","batch 199 average EPE LOSS: 20.434813588512423\n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|█████▉    | 200/336 [55:54<32:52, 14.50s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 200 | flow #1 | EPE LOSS: 32.46993793374097\n","batch 200 | flow #2 | EPE LOSS: 32.045797908882854\n","batch 200 | flow #3 | EPE LOSS: 29.9373824259719\n","batch 200 | flow #4 | EPE LOSS: 28.877738702000624\n","batch 200 | flow #5 | EPE LOSS: 28.52595532048932\n","batch 200 | flow #6 | EPE LOSS: 28.46035617305708\n","batch 200 | flow #7 | EPE LOSS: 27.683907320573525\n","batch 200 | flow #8 | EPE LOSS: 26.486353173976013\n","batch 200 | flow #9 | EPE LOSS: 25.290373217962966\n","batch 200 | flow #10 | EPE LOSS: 24.562468645132782\n","batch 200 | flow #11 | EPE LOSS: 24.049305121073317\n","batch 200 | flow #12 | EPE LOSS: 23.509515701934873\n","batch 200 average EPE LOSS: 24.532124458192403\n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|█████▉    | 201/336 [56:09<32:42, 14.54s/it]"]},{"output_type":"stream","name":"stdout","text":["batch 201 | flow #1 | EPE LOSS: 22.862207894889867\n","batch 201 | flow #2 | EPE LOSS: 22.278874702591526\n","batch 201 | flow #3 | EPE LOSS: 20.55949920227105\n","batch 201 | flow #4 | EPE LOSS: 19.79802290071753\n","batch 201 | flow #5 | EPE LOSS: 19.16831918604519\n","batch 201 | flow #6 | EPE LOSS: 18.900073248089463\n","batch 201 | flow #7 | EPE LOSS: 18.780059230533926\n","batch 201 | flow #8 | EPE LOSS: 18.63255986051829\n","batch 201 | flow #9 | EPE LOSS: 18.671264527754584\n","batch 201 | flow #10 | EPE LOSS: 18.71427926141377\n","batch 201 | flow #11 | EPE LOSS: 18.683851988242264\n","batch 201 | flow #12 | EPE LOSS: 18.54519135776189\n","batch 201 average EPE LOSS: 16.840211430995428\n","Model saved to ../../models/model_20240715-041739.pth\n"]},{"output_type":"stream","name":"stderr","text":["Process Process-15:\n","Process Process-65:\n","Process Process-42:\n","Process Process-34:\n","Process Process-18:\n","Process Process-25:\n","Process Process-23:\n","Process Process-48:\n","Process Process-49:\n","Process Process-84:\n","Process Process-53:\n","Process Process-12:\n","Process Process-43:\n","Process Process-31:\n","Process Process-69:\n","Process Process-51:\n","Process Process-29:\n","Process Process-72:\n","Process Process-47:\n","Process Process-58:\n","Process Process-44:\n","Process Process-59:\n","Process Process-27:\n","Process Process-85:\n","Process Process-37:\n","Process Process-74:\n","Process Process-93:\n","Process Process-36:\n","Process Process-60:\n","Process Process-38:\n","Process Process-87:\n","Process Process-24:\n","Process Process-45:\n","Process Process-28:\n","Process Process-46:\n","Process Process-22:\n","Process Process-40:\n","Process Process-20:\n","Process Process-89:\n","Process Process-32:\n","Process Process-39:\n","Process Process-67:\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n","    finalizer()\n","Process Process-33:\n","Process Process-41:\n","Traceback (most recent call last):\n","Process Process-94:\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","Process Process-35:\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","Process Process-86:\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","Process Process-92:\n","Process Process-73:\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n","    util._exit_function()\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n","    util._exit_function()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n","    util._exit_function()\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n","    util._exit_function()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n","    util._exit_function()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n","    util._exit_function()\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n","    util._exit_function()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n","    util._exit_function()\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n","    util._exit_function()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n","    util._exit_function()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n","    util._exit_function()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n","    util._exit_function()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n","    util._exit_function()\n","KeyboardInterrupt\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n","    _run_finalizers()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n","    util._exit_function()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n","    _run_finalizers()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n","    _run_finalizers()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n","    util._exit_function()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n","    util._exit_function()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n","    _run_finalizers()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n","    _run_finalizers()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n","    util._exit_function()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n","    _run_finalizers()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n","    _run_finalizers()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n","    util._exit_function()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n","    util._exit_function()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n","    _run_finalizers()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n","    _run_finalizers()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n","    _run_finalizers()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n","    _run_finalizers()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n","    finalizer()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n","    finalizer()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n","    _run_finalizers()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n","    finalizer()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n","    _run_finalizers()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n","    _run_finalizers()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n","    finalizer()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n","    _run_finalizers()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n","    _run_finalizers()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n","    finalizer()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n","    _run_finalizers()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n","    finalizer()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n","    finalizer()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n","    finalizer()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n","    res = self._callback(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n","    res = self._callback(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n","    finalizer()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n","    finalizer()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n","    finalizer()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n","    res = self._callback(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n","    res = self._callback(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n","    finalizer()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n","    finalizer()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n","    finalizer()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n","    finalizer()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n","    res = self._callback(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n","    res = self._callback(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n","    res = self._callback(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 199, in _finalize_join\n","    thread.join()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n","    res = self._callback(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n","    res = self._callback(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 199, in _finalize_join\n","    thread.join()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n","    res = self._callback(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 199, in _finalize_join\n","    thread.join()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n","    res = self._callback(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n","    res = self._callback(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 199, in _finalize_join\n","    thread.join()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n","    res = self._callback(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n","    res = self._callback(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n","    res = self._callback(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 199, in _finalize_join\n","    thread.join()\n","  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 199, in _finalize_join\n","    thread.join()\n","  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 199, in _finalize_join\n","    thread.join()\n","  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n","    self._wait_for_tstate_lock()\n","  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 199, in _finalize_join\n","    thread.join()\n","  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 199, in _finalize_join\n","    thread.join()\n","  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n","    self._wait_for_tstate_lock()\n","  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 199, in _finalize_join\n","    thread.join()\n","  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n","    self._wait_for_tstate_lock()\n","  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 199, in _finalize_join\n","    thread.join()\n","  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 199, in _finalize_join\n","    thread.join()\n","  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n","    self._wait_for_tstate_lock()\n","  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 199, in _finalize_join\n","    thread.join()\n","  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 199, in _finalize_join\n","    thread.join()\n","  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 199, in _finalize_join\n","    thread.join()\n","  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n","    self._wait_for_tstate_lock()\n","  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n","    self._wait_for_tstate_lock()\n","  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n","    if lock.acquire(block, timeout):\n","  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n","    self._wait_for_tstate_lock()\n","  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n","    self._wait_for_tstate_lock()\n","  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n","    self._wait_for_tstate_lock()\n","  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n","    self._wait_for_tstate_lock()\n","  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n","    if lock.acquire(block, timeout):\n","  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n","    if lock.acquire(block, timeout):\n","  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n","    self._wait_for_tstate_lock()\n","  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n","    if lock.acquire(block, timeout):\n","  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n","    self._wait_for_tstate_lock()\n","  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n","    self._wait_for_tstate_lock()\n","  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n","    self._wait_for_tstate_lock()\n","  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n","    self._wait_for_tstate_lock()\n","  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n","    if lock.acquire(block, timeout):\n","  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n","    if lock.acquire(block, timeout):\n","  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n","    if lock.acquire(block, timeout):\n","KeyboardInterrupt\n","  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n","    if lock.acquire(block, timeout):\n","  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n","    if lock.acquire(block, timeout):\n","  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n","    if lock.acquire(block, timeout):\n","KeyboardInterrupt\n","KeyboardInterrupt\n","  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n","    if lock.acquire(block, timeout):\n","  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n","    if lock.acquire(block, timeout):\n","KeyboardInterrupt\n","  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n","    if lock.acquire(block, timeout):\n","  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n","    if lock.acquire(block, timeout):\n","  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n","    if lock.acquire(block, timeout):\n","KeyboardInterrupt\n","KeyboardInterrupt\n","KeyboardInterrupt\n","KeyboardInterrupt\n","KeyboardInterrupt\n","KeyboardInterrupt\n","KeyboardInterrupt\n","KeyboardInterrupt\n","KeyboardInterrupt\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n","    _run_finalizers()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n","    finalizer()\n","  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n","    res = self._callback(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 199, in _finalize_join\n","    thread.join()\n","  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n","    self._wait_for_tstate_lock()\n","  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n","    if lock.acquire(block, timeout):\n","KeyboardInterrupt\n","Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x79a0f3015750>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n","    if w.is_alive():\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 165, in is_alive\n","    returncode = self._popen.poll()\n","  File \"/usr/lib/python3.10/multiprocessing/popen_fork.py\", line 27, in poll\n","    pid, sts = os.waitpid(self.pid, flag)\n","KeyboardInterrupt: \n"," 60%|█████▉    | 201/336 [57:15<38:27, 17.09s/it]\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"<ipython-input-21-e15654f8958a>\", line 50, in <cell line: 6>\n","    epe_loss.backward() # Change this to which loss function is to be updated\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 525, in backward\n","    torch.autograd.backward(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 267, in backward\n","    _engine_run_backward(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 744, in _engine_run_backward\n","    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-21-e15654f8958a>\", line 64, in <cell line: 6>\n","    raise SystemExit(\"KeyboardInterrupt\")\n","SystemExit: KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","AttributeError: 'tuple' object has no attribute 'tb_frame'\n"]},{"output_type":"error","ename":"TypeError","evalue":"object of type 'NoneType' has no len()","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-e15654f8958a>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mepe_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Change this to which loss function is to be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m<ipython-input-21-e15654f8958a>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mSystemExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"KeyboardInterrupt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mSystemExit\u001b[0m: KeyboardInterrupt","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     stb = ['An exception has occurred, use %tb to see '\n\u001b[1;32m   2091\u001b[0m                            'the full traceback.\\n']\n\u001b[0;32m-> 2092\u001b[0;31m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0m\u001b[1;32m   2093\u001b[0m                                                                      value))\n\u001b[1;32m   2094\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mchained_exceptions_tb_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             out_list = (\n\u001b[0;32m--> 629\u001b[0;31m                 self.structured_traceback(\n\u001b[0m\u001b[1;32m    630\u001b[0m                     \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0metb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchained_exc_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# epe_losses = list(map(lambda x: x.item(), epe_losses[0]))\n","# overall_losses = list(map(lambda x: x.item(), overall_losses[0]))\n","\n","plt.figure(figsize=(16, 9))\n","len_x = min(len(epe_losses), len(overall_losses))\n","\n","plt.plot(epe_losses[:len_x])\n","plt.plot(overall_losses[:len_x])\n","\n","plt.xlabel('Batch Number')\n","plt.ylabel('Loss')\n","\n","plt.grid()\n","plt.legend()\n","\n","plt.show()"],"metadata":{"trusted":true,"id":"lc4Q3acvGb7U","executionInfo":{"status":"ok","timestamp":1721017127840,"user_tz":-540,"elapsed":696,"user":{"displayName":"Ariya Narayanasamy","userId":"11235920312618626542"}},"colab":{"base_uri":"https://localhost:8080/","height":787},"outputId":"1a307687-b056-4e27-9e29-a97c75596818"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1600x900 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["import time\n","# Create the directory if it doesn't exist\n","# if not os.path.exists('checkpoints'):\n","#     os.makedirs('checkpoints')\n","\n","current_time = time.strftime(\"%Y%m%d-%H%M%S\")\n","model_path = f\"models/model_{current_time}_epoch4.pth\"\n","torch.save(model.state_dict(), model_path)\n","print(f\"Model saved to {model_path}\")"],"metadata":{"trusted":true,"id":"FtltB8OsGb7U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.load_state_dict(torch.load('../../models/model_20240715-041739.pth', map_location=device))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C1hufK7WokX6","executionInfo":{"status":"ok","timestamp":1721017674035,"user_tz":-540,"elapsed":1685,"user":{"displayName":"Ariya Narayanasamy","userId":"11235920312618626542"}},"outputId":"45db1267-3b94-4b5d-f08e-1d779ee5a54b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["model.eval()\n","flow: torch.Tensor = torch.tensor([]).to(device)\n","\n","prev_event_volumes = [torch.zeros([args.batch_size, 3, 480, 640])] # Acts as a queue\n","\n","with torch.no_grad():\n","    print(\"start test\")\n","    for batch in tqdm(test_data):\n","        batch: Dict[str, Any]\n","\n","        event_image = batch[\"event_volume\"].to(device)\n","        prev_event_volumes.append(event_image)\n","\n","        flows = model(prev_event_volumes[0],\n","                      prev_event_volumes[1],\n","                      iters=args.iters)\n","\n","        batch_flow = flows[-1] # [B, 3, 480, 640]\n","        flow = torch.cat((flow, batch_flow), dim=0)  # [N, 2, 480, 640]\n","\n","        if len(prev_event_volumes) >= BATCH_CONCAT:\n","            prev_event_volumes.pop(0)\n","\n","    print(\"test done\")\n","# ------------------\n","#  save submission\n","# ------------------\n","current_time = time.strftime(\"%Y%m%d-%H%M%S\")\n","save_optical_flow_to_npy(flow, f'../../submissions/submission_raft_{current_time}')"],"metadata":{"trusted":true,"id":"LdYOQdyRGb7U","executionInfo":{"status":"error","timestamp":1721017777561,"user_tz":-540,"elapsed":16559,"user":{"displayName":"Ariya Narayanasamy","userId":"11235920312618626542"}},"colab":{"base_uri":"https://localhost:8080/","height":742},"outputId":"f7511a86-c8bb-4fea-ce37-c8c89ffe2f96"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["start test\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/97 [00:00<?, ?it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x79a0f870bac0>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n","    self._shutdown_workers()\n","Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x79a0f870bac0>  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n","    \n","Traceback (most recent call last):\n","if w.is_alive():  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n","\n","      File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n","self._shutdown_workers()    \n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n","assert self._parent_pid == os.getpid(), 'can only test a child process'    if w.is_alive():\n","\n","AssertionError  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",":     can only test a child process\n","assert self._parent_pid == os.getpid(), 'can only test a child process'\n","AssertionError: can only test a child process\n","  0%|          | 0/97 [00:16<?, ?it/s]\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"split_with_sizes expects split_sizes to sum exactly to 7 (input tensor's size at dimension 0), but got split_sizes=[6, 6]","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-cf5230992807>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprev_event_volumes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         flows = model(prev_event_volumes[0],\n\u001b[0m\u001b[1;32m     15\u001b[0m                       \u001b[0mprev_event_volumes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                       iters=args.iters)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DataParallel.forward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/.shortcut-targets-by-id/1dfdt85ascc11CuSJ-T02Mu6sKcwvbKdY/event_camera_repo/RAFT/core/raft.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image1, image2, iters, flow_init, upsample, test_mode)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# run the feature network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_precision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mfmap1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmap2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mfmap1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmap1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/.shortcut-targets-by-id/1dfdt85ascc11CuSJ-T02Mu6sKcwvbKdY/event_camera_repo/RAFT/core/extractor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(tensor, split_size_or_sections, dim)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;31m# split_size_or_sections. The branching code is in _tensor.py, which we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;31m# call here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_size_or_sections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, split_size, dim)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_with_sizes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: split_with_sizes expects split_sizes to sum exactly to 7 (input tensor's size at dimension 0), but got split_sizes=[6, 6]"]}]}]}